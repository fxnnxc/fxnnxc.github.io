<!DOCTYPE html>
<!-- _layouts/distill.html -->
<html>
    
<head>    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Bumjini | Does Adaptation of Neural Memory help factual prediction? (Korean)</title>
    <meta name="author" content="Bumjini  " />
    <meta name="description" content="정보 암기를 위한 추가적인 내부 메모리는 모델 예측에 도움이 되는가?" />
    <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website" />


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous" />

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light" />

    <!-- Styles -->
    
    <link rel="shortcut icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>🪴</text></svg>">
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://fxnnxc.github.io/study/240328_fever_neural_memory/">
    
    <!-- Dark Mode -->
    

  <!-- jQuery -->
<script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>
  <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams',
        inlineMath: [['$','$'], ['\\(','\\)']]
      },
      chtml: {
          scale: 1.0,
          minScale: .6,  
          mtextFontInherit: true,
          mtextInheritFont: true,
          merrorInheritFont: true,
        },
        svg: {
          scale: 1.2
        }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

  <!-- Distill js -->
  <script src="/assets/js/distillpub/template.v2.js"></script>
  <script src="/assets/js/distillpub/transforms.v2.js"></script>
  <script src="/assets/js/distillpub/overrides.js"></script>
  <!-- Page/Post style -->
  
</head>

  <d-front-matter>
    <script async type="text/json">{
      "title": "Does Adaptation of Neural Memory help factual prediction? (Korean)",
      "description": "정보 암기를 위한 추가적인 내부 메모리는 모델 예측에 도움이 되는가?",
      "published": "March 28, 2024",
      "authors": [
        {
          "author": "Bumjin Park",
          "authorURL": "",
          "affiliations": [
            {
              "name": "KAIST",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script>
  </d-front-matter>

  <body style="padding-top:0px" class="sticky-bottom-footer"> 
    
    <!-- Header --><header>
      
      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm sticky-top">
        <div class="container">
          <img>
          <!-- <img src="/assets/common/KAIST-hi.gif" width="40px" style="margin-right:0px; padding-bottom: 3px;"> -->
          
          <a class="navbar-brand title font-weight-lighter" href="https://fxnnxc.github.io/" style="margin-left:20px ;">
              <!--Bumjini-->
           <span class="font-weight-bold" style="font-size:larger;font-family:Times New Roman;">Bumjini    </span>
        </a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <!-- <li style="font-size: 17px" class="nav-item ">
                <a class="nav-link" href="/">  About Me</a>
              </li> -->
              
              <!-- Blog -->
              <li style="font-size: 17px" class="nav-item ">
                <a class="nav-link" href="/main_papers/"> Papers</a>
              </li>

              <li style="font-size: 17px" class="nav-item ">
                <a class="nav-link" href="/main_articles/"> Articles</a>
              </li>

              <li style="font-size: 17px" class="nav-item ">
                <a class="nav-link" href="/main_projects/"> Projects</a>
              </li>

              <!-- Other pages -->
              <li style="font-size: 17px" class="nav-item dropdown ">
                <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Others</a>
                <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown">
                  <a class="dropdown-item" href="/side_papers/">🗂️ side-paper</a>
                  <a class="dropdown-item" href="/side_articles/">🥕 side-articles</a>
                  <a class="dropdown-item" href="/book/">📚 book (korean)</a>
                </div>
              </li>
            </ul>
          </div>
        </div>
      </nav>
    </header>

    <!-- Content -->
    <div class="post distill">

      <d-title>
        <h1 style="font-size: 32px;">Does Adaptation of Neural Memory help factual prediction? (Korean)</h1>
        <p>정보 암기를 위한 추가적인 내부 메모리는 모델 예측에 도움이 되는가?</p>
      </d-title>

      <d-byline></d-byline>

      <d-article>
        

        <h2 id="does-adaptation-of-neural-memory-help-factual-prediction">Does Adaptation of Neural Memory help factual prediction?</h2>

<blockquote>
✔️ **This experiment is finished and archived.** 
</blockquote>

<ul>
  <li>🗓️ <strong>Experiment Date</strong> : 24.03.28</li>
  <li>🌸 <strong>Scope</strong> : Few-shot classification for large language models with memorization adaptation. We use FEVER dataset 99 pages and related validation question <br> (#samples: 3045).</li>
  <li>🧑🏻‍💻 <strong>Code</strong>:<a href="https://github.com/fxnnxc/llm/tree/v24.03.29_memorization" target="_blank" rel="noopener noreferrer">llm:v24.03.29_memorization</a>
</li>
  <li>😮 <strong>Status</strong> : Running Experiments
    <ul class="task-list">
      <li class="task-list-item">
<input type="checkbox" class="task-list-item-checkbox" disabled checked>  <em>Documentation of Settings</em>
</li>
      <li class="task-list-item">
<input type="checkbox" class="task-list-item-checkbox" disabled checked>  <em>Running Experiments</em>
</li>
      <li class="task-list-item">
<input type="checkbox" class="task-list-item-checkbox" disabled checked>  <em>Record Results</em>
</li>
      <li class="task-list-item">
<input type="checkbox" class="task-list-item-checkbox" disabled checked>  <em>Analysis Results</em>
</li>
    </ul>
  </li>
</ul>

<hr>

<h2 id="1-memory-adapted-llms">1. Memory Adapted LLMs</h2>

<p>GPT-like LLM은 기본적으로 few-shot learning이 가능하다. 질문에 대한 예시를 바탕으로 비슷하게 예측을 준다<d-footnote>T5 형태의 모델은 질문을 인코더로 입력하고 예측을 디코더에서 할 수 있으므로, few-shot 방식이 필요하지 않다. GPT-like 모델은 문장을 암기하는 형태로 정보를 저장하며, 예측 성능을 높이기 위해서는 few-shot 방식을 사용해야 한다.</d-footnote>. 뉴럴 메모리를 학습하는 방식은 GPT블록 출력에 대해서 residual connection으로 추가적인 표현을 더하는 방식이다. 본 실험에서 비교하는 것은 메모리에 저장하는 정보를 다르게 하는 것이다. 저장하는 정보는 두 가지 종류다.</p>

<ol>
  <li>Document Contents : LLM은 문서에 대한 정보를 추가적으로 외운다. 이 때 전체 파라미터에 저장하기보다 기존 파라미터를 보존할 상태로 추가적인 파라미터로 연산한다.</li>
  <li>Template Memorization : Factual Answer에 대한 template을 학습한다. 이는 LLM이 template 형태의 대답에 대해서 제대로 반응하지 못하는 문제를 해결함과 동시에 질의-대답에 대한 관계를 학습하도록 돕는다.</li>
</ol>

<h2 id="2-데이터-준비-파이프라인">2. 데이터 준비 파이프라인</h2>

<p>Notations: $N_{full}, N_{top}, N_{trunc}$, are the number of all evidence documents in fever validation, number of top $K$ documents, and number of truncated documents by removing invalid documents, respectively.</p>

<ol>
  <li>validation에 Q-A데이터를 각 문서에 대한 Q-A로 정리한다.</li>
  <li>Q-A가 많은 순서대로 정렬하여, top-N개의 문서를 고른다. $$</li>
  <li>문서 길이가 0이거나, 학습 Q-A 데이터에 문서가 존재하지 않는 경우, 해당 문서를 제외한다.</li>
</ol>

<h3 id="few-shot-template">Few-shot Template</h3>

<p>We use the following template for few-shot inference.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Determine the factuality of the following claims 
Answer candidates: <span class="o">(</span>a<span class="o">)</span> supports, <span class="o">(</span>b<span class="o">)</span> refutes, <span class="o">(</span>c<span class="o">)</span> not enough info
Claim: Roman Atwood is a content creator.
A: <span class="o">(</span>a<span class="o">)</span>
Claim: System of a Down briefly disbanded <span class="k">in </span>limbo.
A: <span class="o">(</span>b<span class="o">)</span>
Claim: Adrienne Bailon is an accountant.
A:
</code></pre></div></div>

<h3 id="few-shot-setting">Few-shot setting</h3>

<p>문서 정보 학습에 대한 few-shot setting은 문서에 해당하는 Q-A Pair를 학습하는 방식으로 진행된다. 
만일 Q-A pair에 충분한 데이터가 없을 경우, support, refutes, not enough info에 대해서 추가적인 데이터를 더해준다.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Nikolaj Coster-Waldau worked with the Fox Broadcasting Company.
Roman Atwood is a content creator.
Tetris has sold millions of physical copies.
The Boston Celtics play their home games at TD Garden.
The Ten Commandments is an epic film.

System of a Down briefly disbanded <span class="k">in </span>limbo.
Beautiful reached number two on the Billboard Hot 100 <span class="k">in </span>2003.
Afghanistan is the <span class="nb">source </span>of the Kushan dynasty.
Marilyn Monroe worked with Warner Brothers.
United Nations was relocated to the United States <span class="k">in </span>1945.
Keith Urban is a person <span class="nb">who </span>sings.

Adrienne Bailon is an accountant.
Stranger Things is <span class="nb">set </span><span class="k">in </span>Bloomington, Indiana.
Puerto Rico is not an unincorporated territory of the United States.
Peggy Sue Got Married is a Egyptian film released <span class="k">in </span>1986.
Andy Roddick lost 5 Master Series between 2002 and 2010.
Willie Nelson dropped out of college after three years.
</code></pre></div></div>

<h2 id="3-model-candidates">3. Model Candidates</h2>

<ol>
  <li>
<strong>Pretrained</strong> : 🤗 Huggingface 에서 받은 기본 모델 파라미터 상태</li>
  <li>
<strong>Document Memorization (Doc)</strong> : 추가적인 메모리에 evidence Wiki page를 암기하도록 학습한다.</li>
  <li>
<strong>Template Memorization (Temp)</strong> : 추가적인 메모리에 Wiki page와 관련된 학습 Q-A를 few-shot 형태로 암기한다.</li>
  <li>
<strong>Document + Template Memorization (DocTemp)</strong> : 추가적인 메모리에 두 가지를 모두 학습한다.</li>
</ol>

<p>메모리를 저장하는 방법은 블록의 아웃풋에 대해서 추가적인 표현을 학습하여 residual에 더하는 방식으로 진행된다. 
메모리의 사이즈는 다음과 같다.</p>

<d-code block="" language="python">
class MemMdoule(nn.Module):
    def __init__(self, hidden_dim, model_dim):
        super().__init__()
        self.up = layer_init(nn.Linear(model_dim, hidden_dim))
        self.act = nn.GELU()
        self.down = layer_init(nn.Linear(hidden_dim, model_dim))
        self.ln = nn.LayerNorm(model_dim)
        self.gate = nn.Parameter(torch.tensor([1.0]))
        
    def forward(self, x):
        x= self.up(x)
        x = self.act(x)
        x = self.down(x)
        x = self.ln(x) * self.gate
        return x
</d-code>

<h2 id="4-학습-세팅">4. 학습 세팅</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">optimizer</span><span class="o">=</span><span class="sh">'</span><span class="s">adam</span><span class="sh">'</span>
<span class="n">lr_scheduler</span><span class="o">=</span><span class="sh">'</span><span class="s">cosine</span><span class="sh">'</span>
<span class="n">batch_size</span><span class="o">=</span><span class="mi">4</span>
<span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span>
<span class="n">num_epochs</span><span class="o">=</span><span class="mi">25</span>
<span class="n">num_warmup_steps</span><span class="o">=</span><span class="mi">10</span>
<span class="n">optimizer_weight_decay</span><span class="o">=</span><span class="mf">0.05</span>  <span class="c1"># 메모리가 최대한 적은 영향을 가지도록
</span><span class="n">grad_clip_coeff</span><span class="o">=</span><span class="mf">10.0</span>
<span class="n">hook_memory_dim</span><span class="o">=</span><span class="mi">128</span>
<span class="n">memory_module</span> <span class="n">location</span><span class="o">=</span><span class="mi">70</span><span class="o">~</span><span class="mi">80</span><span class="o">%</span> <span class="n">location</span> <span class="n">of</span> <span class="n">transformer</span> <span class="n">blocks</span> 
</code></pre></div></div>

<h2 id="5-학습-결과">5. 학습 결과</h2>

<p>We answer the following questions.</p>
<ol>
  <li>Does few-shot examples help the factual prediction without memorization fine tuning?</li>
  <li>Does memorization of document pages help the prediction?</li>
  <li>Does memorization of templates help the prediction?</li>
  <li>Does memorization of both show the best performance?</li>
</ol>

<p>Answers</p>
<ol>
  <li>모델별로 차이는 있지만 대부분 1-shot 방식이 높은 성능을 보인다. 이는 문제의 의미를 제대로 파악하는데 많은 샘플이 필요하지 않다는 점을 나타내는 것 같다.</li>
  <li>문서 Memorization 을 진행하는 경우 기존보다 성능향상이 일어났다. 차이가 크진 않지만 정보를 생각하는 게 도움이 되는 것으로 확인된다. (추가적으로 prompt 와 LoRA방식도 잘 작동하는지 확인해야 한다. 다만 지금은 문서 메모리에 초점을 맞추기 때문에 적절한 비교대상은 아닐 수 있다. )</li>
  <li>일부 모델에서는 template 방식이 문서 memorization보다 더 높은 성능을 보였는데, 이는 두 정보 중에 특정 레이어에 더 효율적인 정보가 존재함을 나타내는 것 같다.</li>
  <li>두 정보를 모두 암기하는 것은 높은 성능을 보이지 않았는데, 이는 특정레이어에 두 가지 정보를 모두 넣었기 때문인 것으로 보인다. 추가적인 구조적 개선 (데이터 타입 마다 다른 메모리를 둔다던지..)하는 방식을 사용하지 않으면 두 정보를 모두 하나의 레이어에 넣는 것은 효율적이지 못하다.</li>
</ol>

<h3 id="1-fever--pages-f1-score-pretrained">1. Fever  Pages F1 Score (Pretrained)</h3>

<table>
  <thead>
    <tr>
      <th>Model</th>
      <th>zero-shot</th>
      <th>1-shot</th>
      <th>2-shot</th>
      <th>3-shot</th>
      <th>4-shot</th>
      <th>5-shot</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>llama2 13b</td>
      <td>0.013</td>
      <td>0.460</td>
      <td>0.410</td>
      <td>0.417</td>
      <td>0.472</td>
      <td><strong>0.476</strong></td>
    </tr>
    <tr>
      <td>llama2 7b</td>
      <td>0.018</td>
      <td><strong>0.435</strong></td>
      <td>0.354</td>
      <td>0.368</td>
      <td>0.367</td>
      <td>0.364</td>
    </tr>
    <tr>
      <td>llama2_chat 13b</td>
      <td>0.010</td>
      <td>0.556</td>
      <td>0.389</td>
      <td>0.496</td>
      <td>0.552</td>
      <td><strong>0.578</strong></td>
    </tr>
    <tr>
      <td>llama2_chat 7b</td>
      <td><strong>0.392</strong></td>
      <td><strong>0.636</strong></td>
      <td>0.519</td>
      <td>0.463</td>
      <td>0.408</td>
      <td>0.393</td>
    </tr>
  </tbody>
  <tbody>
    <tr>
      <td>pythia 12b</td>
      <td>0.000</td>
      <td><strong>0.366</strong></td>
      <td>0.118</td>
      <td>0.119</td>
      <td>0.113</td>
      <td>0.083</td>
    </tr>
    <tr>
      <td>pythia 6.9b</td>
      <td>0.021</td>
      <td>0.163</td>
      <td><strong>0.280</strong></td>
      <td>0.175</td>
      <td>0.106</td>
      <td>0.206</td>
    </tr>
    <tr>
      <td>pythia 2.8b</td>
      <td>0.006</td>
      <td>0.026</td>
      <td>0.097</td>
      <td>0.056</td>
      <td>0.108</td>
      <td><strong>0.132</strong></td>
    </tr>
    <tr>
      <td>pythia 1.4b</td>
      <td>0.059</td>
      <td>0.009</td>
      <td>0.139</td>
      <td>0.177</td>
      <td><strong>0.275</strong></td>
      <td>0.176</td>
    </tr>
    <tr>
      <td>pythia 1b</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.001</td>
      <td>0.002</td>
      <td>0.011</td>
      <td>0.013</td>
    </tr>
    <tr>
      <td>pythia 410m</td>
      <td>0.000</td>
      <td>0.010</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
    </tr>
    <tr>
      <td>pythia 160m</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
    </tr>
    <tr>
      <td>pythia 70m</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
    </tr>
  </tbody>
</table>

<h4 id="fever-99-pages-f1-score-template-memo---1-shot-weight-decay-005">Fever 99 Pages F1 Score (template memo - 1 shot, weight decay 0.05)</h4>

<table>
  <thead>
    <tr>
      <th>Model</th>
      <th>1-shot (base)</th>
      <th>Document</th>
      <th>Template (Train Q-A)</th>
      <th>Doc + Template</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>llama2 13b</td>
      <td>0.460</td>
      <td><strong>0.476</strong></td>
      <td>0.465</td>
      <td>0.462</td>
    </tr>
    <tr>
      <td>llama2 7b</td>
      <td>0.435</td>
      <td>0.439</td>
      <td><strong>0.445</strong></td>
      <td>0.438</td>
    </tr>
    <tr>
      <td>llama2_chat 13b</td>
      <td>0.556</td>
      <td>0.561</td>
      <td><strong>0.562</strong></td>
      <td>0.558</td>
    </tr>
    <tr>
      <td>llama2_chat 7b</td>
      <td>0.636</td>
      <td><strong>0.650</strong></td>
      <td>0.647</td>
      <td>0.646</td>
    </tr>
  </tbody>
  <tbody>
    <tr>
      <td>pythia 12b</td>
      <td>0.366</td>
      <td>0.366</td>
      <td>0.366</td>
      <td>0.366</td>
    </tr>
    <tr>
      <td>pythia 6.9b</td>
      <td>0.163</td>
      <td>0.163</td>
      <td>0.163</td>
      <td>0.163</td>
    </tr>
    <tr>
      <td>pythia 2.8b</td>
      <td>0.026</td>
      <td>0.026</td>
      <td>0.026</td>
      <td>0.026</td>
    </tr>
    <tr>
      <td>pythia 1.4b</td>
      <td>0.009</td>
      <td>0.009</td>
      <td>0.009</td>
      <td>0.009</td>
    </tr>
    <tr>
      <td>pythia 1b</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
    </tr>
    <tr>
      <td>pythia 410m</td>
      <td>0.010</td>
      <td>0.010</td>
      <td>0.010</td>
      <td>0.010</td>
    </tr>
    <tr>
      <td>pythia 160m</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
    </tr>
    <tr>
      <td>pythia 70m</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
    </tr>
  </tbody>
</table>

<h4 id="weight-decay-001--005--001">Weight Decay 0.01  (0.05 –&gt; 0.01)</h4>

<table>
  <thead>
    <tr>
      <th>Model</th>
      <th>1-shot (base)</th>
      <th>Best Performance with 0.05 Weight Decay</th>
      <th>Document</th>
      <th>Template (Train Q-A)</th>
      <th>Doc + Template</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>llama2 13b</td>
      <td>0.460</td>
      <td>0.476</td>
      <td><strong>0.488</strong></td>
      <td>0.483</td>
      <td>0.472</td>
    </tr>
    <tr>
      <td>llama2_chat 13b</td>
      <td>0.556</td>
      <td><strong>0.562</strong></td>
      <td>0.557</td>
      <td>0.549</td>
      <td>0.561</td>
    </tr>
    <tr>
      <td>llama2_chat 7b</td>
      <td>0.636</td>
      <td><strong>0.650</strong></td>
      <td>0.639</td>
      <td>0.650</td>
      <td>0.642</td>
    </tr>
  </tbody>
</table>

<h4 id="pythia-full-weight-update-no-memory-module">Pythia Full weight update (no memory module)</h4>

<table>
  <thead>
    <tr>
      <th>model</th>
      <th>size</th>
      <th>1-shot acc</th>
      <th>method</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>pythia</td>
      <td>410m</td>
      <td>0.000</td>
      <td>gpt_template</td>
    </tr>
    <tr>
      <td>pythia</td>
      <td>410m</td>
      <td>0.024</td>
      <td>gpt_wikipage_and_template</td>
    </tr>
    <tr>
      <td>pythia</td>
      <td>70m</td>
      <td>0.000</td>
      <td>gpt_template</td>
    </tr>
    <tr>
      <td>pythia</td>
      <td>70m</td>
      <td>0.000</td>
      <td>gpt_wikipage_and_template</td>
    </tr>
  </tbody>
</table>

<hr>

<h2 id="reproducibility">Reproducibility</h2>

<ul>
  <li>
<strong>Pretrained Models</strong>
    <ul>
      <li><code class="language-plaintext highlighter-rouge">benchmark/fever/llama2_n_100.sh</code></li>
      <li><code class="language-plaintext highlighter-rouge">benchmark/fever/pythia_n_100.sh</code></li>
    </ul>
  </li>
  <li>
<strong>Llama2 Memorization</strong>:<code class="language-plaintext highlighter-rouge">experiments/240329_hook_mem/train_gpt_fever_llama2.sh</code>
</li>
  <li>
<strong>Pythia Memorization</strong>: <code class="language-plaintext highlighter-rouge">experiments/240329_hook_mem/train_gpt_fever_pythia.sh</code>
<br> (we exclude Pythia with documentation memorization only.)</li>
</ul>

<hr>

<h2 id="full-results">Full Results</h2>

<h3 id="pretrained-models">Pretrained Models</h3>

<table>
  <thead>
    <tr>
      <th>model</th>
      <th style="text-align: right">Size</th>
      <th style="text-align: center">Few-shot</th>
      <th style="text-align: center">accuracy</th>
      <th style="text-align: center">f1.micro</th>
      <th style="text-align: center">f1.macro</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>llama2</td>
      <td style="text-align: right">13b</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">0.013</td>
      <td style="text-align: center">0.013</td>
      <td style="text-align: center">0.018</td>
    </tr>
    <tr>
      <td>llama2</td>
      <td style="text-align: right">13b</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">0.460</td>
      <td style="text-align: center">0.460</td>
      <td style="text-align: center">0.319</td>
    </tr>
    <tr>
      <td>llama2</td>
      <td style="text-align: right">13b</td>
      <td style="text-align: center">2</td>
      <td style="text-align: center">0.410</td>
      <td style="text-align: center">0.410</td>
      <td style="text-align: center">0.229</td>
    </tr>
    <tr>
      <td>llama2</td>
      <td style="text-align: right">13b</td>
      <td style="text-align: center">3</td>
      <td style="text-align: center">0.417</td>
      <td style="text-align: center">0.417</td>
      <td style="text-align: center">0.238</td>
    </tr>
    <tr>
      <td>llama2</td>
      <td style="text-align: right">13b</td>
      <td style="text-align: center">4</td>
      <td style="text-align: center">0.472</td>
      <td style="text-align: center">0.472</td>
      <td style="text-align: center">0.268</td>
    </tr>
    <tr>
      <td>llama2</td>
      <td style="text-align: right">13b</td>
      <td style="text-align: center">5</td>
      <td style="text-align: center">0.476</td>
      <td style="text-align: center">0.476</td>
      <td style="text-align: center">0.272</td>
    </tr>
  </tbody>
  <tbody>
    <tr>
      <td>llama2</td>
      <td style="text-align: right">7b</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">0.018</td>
      <td style="text-align: center">0.018</td>
      <td style="text-align: center">0.024</td>
    </tr>
    <tr>
      <td>llama2</td>
      <td style="text-align: right">7b</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">0.435</td>
      <td style="text-align: center">0.435</td>
      <td style="text-align: center">0.241</td>
    </tr>
    <tr>
      <td>llama2</td>
      <td style="text-align: right">7b</td>
      <td style="text-align: center">2</td>
      <td style="text-align: center">0.354</td>
      <td style="text-align: center">0.354</td>
      <td style="text-align: center">0.214</td>
    </tr>
    <tr>
      <td>llama2</td>
      <td style="text-align: right">7b</td>
      <td style="text-align: center">3</td>
      <td style="text-align: center">0.368</td>
      <td style="text-align: center">0.368</td>
      <td style="text-align: center">0.220</td>
    </tr>
    <tr>
      <td>llama2</td>
      <td style="text-align: right">7b</td>
      <td style="text-align: center">4</td>
      <td style="text-align: center">0.367</td>
      <td style="text-align: center">0.367</td>
      <td style="text-align: center">0.219</td>
    </tr>
    <tr>
      <td>llama2</td>
      <td style="text-align: right">7b</td>
      <td style="text-align: center">5</td>
      <td style="text-align: center">0.364</td>
      <td style="text-align: center">0.364</td>
      <td style="text-align: center">0.220</td>
    </tr>
  </tbody>
  <tbody>
    <tr>
      <td>llama2_chat</td>
      <td style="text-align: right">13b</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">0.010</td>
      <td style="text-align: center">0.010</td>
      <td style="text-align: center">0.012</td>
    </tr>
    <tr>
      <td>llama2_chat</td>
      <td style="text-align: right">13b</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">0.556</td>
      <td style="text-align: center">0.556</td>
      <td style="text-align: center">0.381</td>
    </tr>
    <tr>
      <td>llama2_chat</td>
      <td style="text-align: right">13b</td>
      <td style="text-align: center">2</td>
      <td style="text-align: center">0.389</td>
      <td style="text-align: center">0.389</td>
      <td style="text-align: center">0.209</td>
    </tr>
    <tr>
      <td>llama2_chat</td>
      <td style="text-align: right">13b</td>
      <td style="text-align: center">3</td>
      <td style="text-align: center">0.496</td>
      <td style="text-align: center">0.496</td>
      <td style="text-align: center">0.259</td>
    </tr>
    <tr>
      <td>llama2_chat</td>
      <td style="text-align: right">13b</td>
      <td style="text-align: center">4</td>
      <td style="text-align: center">0.552</td>
      <td style="text-align: center">0.552</td>
      <td style="text-align: center">0.284</td>
    </tr>
    <tr>
      <td>llama2_chat</td>
      <td style="text-align: right">13b</td>
      <td style="text-align: center">5</td>
      <td style="text-align: center">0.578</td>
      <td style="text-align: center">0.578</td>
      <td style="text-align: center">0.291</td>
    </tr>
  </tbody>
  <tbody>
    <tr>
      <td>llama2_chat</td>
      <td style="text-align: right">7b</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">0.392</td>
      <td style="text-align: center">0.392</td>
      <td style="text-align: center">0.196</td>
    </tr>
    <tr>
      <td>llama2_chat</td>
      <td style="text-align: right">7b</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">0.636</td>
      <td style="text-align: center">0.636</td>
      <td style="text-align: center">0.457</td>
    </tr>
    <tr>
      <td>llama2_chat</td>
      <td style="text-align: right">7b</td>
      <td style="text-align: center">2</td>
      <td style="text-align: center">0.519</td>
      <td style="text-align: center">0.519</td>
      <td style="text-align: center">0.402</td>
    </tr>
    <tr>
      <td>llama2_chat</td>
      <td style="text-align: right">7b</td>
      <td style="text-align: center">3</td>
      <td style="text-align: center">0.463</td>
      <td style="text-align: center">0.463</td>
      <td style="text-align: center">0.373</td>
    </tr>
    <tr>
      <td>llama2_chat</td>
      <td style="text-align: right">7b</td>
      <td style="text-align: center">4</td>
      <td style="text-align: center">0.408</td>
      <td style="text-align: center">0.408</td>
      <td style="text-align: center">0.341</td>
    </tr>
    <tr>
      <td>llama2_chat</td>
      <td style="text-align: right">7b</td>
      <td style="text-align: center">5</td>
      <td style="text-align: center">0.393</td>
      <td style="text-align: center">0.393</td>
      <td style="text-align: center">0.337</td>
    </tr>
  </tbody>
  <tbody>
    <tr>
      <td>pythia</td>
      <td style="text-align: right">12b</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">0.000</td>
      <td style="text-align: center">0.000</td>
      <td style="text-align: center">0.000</td>
    </tr>
    <tr>
      <td>pythia</td>
      <td style="text-align: right">12b</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">0.366</td>
      <td style="text-align: center">0.366</td>
      <td style="text-align: center">0.197</td>
    </tr>
    <tr>
      <td>pythia</td>
      <td style="text-align: right">12b</td>
      <td style="text-align: center">2</td>
      <td style="text-align: center">0.118</td>
      <td style="text-align: center">0.118</td>
      <td style="text-align: center">0.124</td>
    </tr>
    <tr>
      <td>pythia</td>
      <td style="text-align: right">12b</td>
      <td style="text-align: center">3</td>
      <td style="text-align: center">0.119</td>
      <td style="text-align: center">0.119</td>
      <td style="text-align: center">0.117</td>
    </tr>
    <tr>
      <td>pythia</td>
      <td style="text-align: right">12b</td>
      <td style="text-align: center">4</td>
      <td style="text-align: center">0.113</td>
      <td style="text-align: center">0.113</td>
      <td style="text-align: center">0.115</td>
    </tr>
    <tr>
      <td>pythia</td>
      <td style="text-align: right">12b</td>
      <td style="text-align: center">5</td>
      <td style="text-align: center">0.083</td>
      <td style="text-align: center">0.083</td>
      <td style="text-align: center">0.093</td>
    </tr>
  </tbody>
  <tbody>
    <tr>
      <td>pythia</td>
      <td style="text-align: right">6.9b</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">0.021</td>
      <td style="text-align: center">0.021</td>
      <td style="text-align: center">0.028</td>
    </tr>
    <tr>
      <td>pythia</td>
      <td style="text-align: right">6.9b</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">0.163</td>
      <td style="text-align: center">0.163</td>
      <td style="text-align: center">0.118</td>
    </tr>
    <tr>
      <td>pythia</td>
      <td style="text-align: right">6.9b</td>
      <td style="text-align: center">2</td>
      <td style="text-align: center">0.280</td>
      <td style="text-align: center">0.280</td>
      <td style="text-align: center">0.247</td>
    </tr>
    <tr>
      <td>pythia</td>
      <td style="text-align: right">6.9b</td>
      <td style="text-align: center">3</td>
      <td style="text-align: center">0.175</td>
      <td style="text-align: center">0.175</td>
      <td style="text-align: center">0.164</td>
    </tr>
    <tr>
      <td>pythia</td>
      <td style="text-align: right">6.9b</td>
      <td style="text-align: center">4</td>
      <td style="text-align: center">0.106</td>
      <td style="text-align: center">0.106</td>
      <td style="text-align: center">0.106</td>
    </tr>
    <tr>
      <td>pythia</td>
      <td style="text-align: right">6.9b</td>
      <td style="text-align: center">5</td>
      <td style="text-align: center">0.206</td>
      <td style="text-align: center">0.206</td>
      <td style="text-align: center">0.167</td>
    </tr>
  </tbody>
  <tbody>
    <tr>
      <td>pythia</td>
      <td style="text-align: right">2.8b</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">0.006</td>
      <td style="text-align: center">0.006</td>
      <td style="text-align: center">0.008</td>
    </tr>
    <tr>
      <td>pythia</td>
      <td style="text-align: right">2.8b</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">0.026</td>
      <td style="text-align: center">0.026</td>
      <td style="text-align: center">0.025</td>
    </tr>
    <tr>
      <td>pythia</td>
      <td style="text-align: right">2.8b</td>
      <td style="text-align: center">2</td>
      <td style="text-align: center">0.097</td>
      <td style="text-align: center">0.097</td>
      <td style="text-align: center">0.077</td>
    </tr>
    <tr>
      <td>pythia</td>
      <td style="text-align: right">2.8b</td>
      <td style="text-align: center">3</td>
      <td style="text-align: center">0.056</td>
      <td style="text-align: center">0.056</td>
      <td style="text-align: center">0.066</td>
    </tr>
    <tr>
      <td>pythia</td>
      <td style="text-align: right">2.8b</td>
      <td style="text-align: center">4</td>
      <td style="text-align: center">0.108</td>
      <td style="text-align: center">0.108</td>
      <td style="text-align: center">0.087</td>
    </tr>
    <tr>
      <td>pythia</td>
      <td style="text-align: right">2.8b</td>
      <td style="text-align: center">5</td>
      <td style="text-align: center">0.132</td>
      <td style="text-align: center">0.132</td>
      <td style="text-align: center">0.102</td>
    </tr>
  </tbody>
  <tbody>
    <tr>
      <td>pythia</td>
      <td style="text-align: right">1.4b</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">0.059</td>
      <td style="text-align: center">0.059</td>
      <td style="text-align: center">0.067</td>
    </tr>
    <tr>
      <td>pythia</td>
      <td style="text-align: right">1.4b</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">0.009</td>
      <td style="text-align: center">0.009</td>
      <td style="text-align: center">0.008</td>
    </tr>
    <tr>
      <td>pythia</td>
      <td style="text-align: right">1.4b</td>
      <td style="text-align: center">2</td>
      <td style="text-align: center">0.139</td>
      <td style="text-align: center">0.139</td>
      <td style="text-align: center">0.102</td>
    </tr>
    <tr>
      <td>pythia</td>
      <td style="text-align: right">1.4b</td>
      <td style="text-align: center">3</td>
      <td style="text-align: center">0.177</td>
      <td style="text-align: center">0.177</td>
      <td style="text-align: center">0.133</td>
    </tr>
    <tr>
      <td>pythia</td>
      <td style="text-align: right">1.4b</td>
      <td style="text-align: center">4</td>
      <td style="text-align: center">0.275</td>
      <td style="text-align: center">0.275</td>
      <td style="text-align: center">0.178</td>
    </tr>
    <tr>
      <td>pythia</td>
      <td style="text-align: right">1.4b</td>
      <td style="text-align: center">5</td>
      <td style="text-align: center">0.176</td>
      <td style="text-align: center">0.176</td>
      <td style="text-align: center">0.176</td>
    </tr>
  </tbody>
  <tbody>
    <tr>
      <td>pythia</td>
      <td style="text-align: right">1b</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">0.000</td>
      <td style="text-align: center">0.000</td>
      <td style="text-align: center">0.000</td>
    </tr>
    <tr>
      <td>pythia</td>
      <td style="text-align: right">1b</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">0.000</td>
      <td style="text-align: center">0.000</td>
      <td style="text-align: center">0.000</td>
    </tr>
    <tr>
      <td>pythia</td>
      <td style="text-align: right">1b</td>
      <td style="text-align: center">2</td>
      <td style="text-align: center">0.001</td>
      <td style="text-align: center">0.001</td>
      <td style="text-align: center">0.001</td>
    </tr>
    <tr>
      <td>pythia</td>
      <td style="text-align: right">1b</td>
      <td style="text-align: center">3</td>
      <td style="text-align: center">0.002</td>
      <td style="text-align: center">0.002</td>
      <td style="text-align: center">0.002</td>
    </tr>
    <tr>
      <td>pythia</td>
      <td style="text-align: right">1b</td>
      <td style="text-align: center">4</td>
      <td style="text-align: center">0.011</td>
      <td style="text-align: center">0.011</td>
      <td style="text-align: center">0.011</td>
    </tr>
    <tr>
      <td>pythia</td>
      <td style="text-align: right">1b</td>
      <td style="text-align: center">5</td>
      <td style="text-align: center">0.013</td>
      <td style="text-align: center">0.013</td>
      <td style="text-align: center">0.014</td>
    </tr>
  </tbody>
  <tbody>
    <tr>
      <td>pythia</td>
      <td style="text-align: right">410m</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">0.000</td>
      <td style="text-align: center">0.000</td>
      <td style="text-align: center">0.000</td>
    </tr>
    <tr>
      <td>pythia</td>
      <td style="text-align: right">410m</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">0.010</td>
      <td style="text-align: center">0.010</td>
      <td style="text-align: center">0.009</td>
    </tr>
    <tr>
      <td>pythia</td>
      <td style="text-align: right">410m</td>
      <td style="text-align: center">2</td>
      <td style="text-align: center">0.000</td>
      <td style="text-align: center">0.000</td>
      <td style="text-align: center">0.000</td>
    </tr>
    <tr>
      <td>pythia</td>
      <td style="text-align: right">410m</td>
      <td style="text-align: center">3</td>
      <td style="text-align: center">0.000</td>
      <td style="text-align: center">0.000</td>
      <td style="text-align: center">0.000</td>
    </tr>
    <tr>
      <td>pythia</td>
      <td style="text-align: right">410m</td>
      <td style="text-align: center">4</td>
      <td style="text-align: center">0.000</td>
      <td style="text-align: center">0.000</td>
      <td style="text-align: center">0.000</td>
    </tr>
    <tr>
      <td>pythia</td>
      <td style="text-align: right">410m</td>
      <td style="text-align: center">5</td>
      <td style="text-align: center">0.000</td>
      <td style="text-align: center">0.000</td>
      <td style="text-align: center">0.000</td>
    </tr>
  </tbody>
  <tbody>
    <tr>
      <td>pythia</td>
      <td style="text-align: right">160m</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">0.000</td>
      <td style="text-align: center">0.000</td>
      <td style="text-align: center">0.000</td>
    </tr>
    <tr>
      <td>pythia</td>
      <td style="text-align: right">160m</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">0.000</td>
      <td style="text-align: center">0.000</td>
      <td style="text-align: center">0.000</td>
    </tr>
    <tr>
      <td>pythia</td>
      <td style="text-align: right">160m</td>
      <td style="text-align: center">2</td>
      <td style="text-align: center">0.000</td>
      <td style="text-align: center">0.000</td>
      <td style="text-align: center">0.000</td>
    </tr>
    <tr>
      <td>pythia</td>
      <td style="text-align: right">160m</td>
      <td style="text-align: center">3</td>
      <td style="text-align: center">0.000</td>
      <td style="text-align: center">0.000</td>
      <td style="text-align: center">0.000</td>
    </tr>
    <tr>
      <td>pythia</td>
      <td style="text-align: right">160m</td>
      <td style="text-align: center">4</td>
      <td style="text-align: center">0.000</td>
      <td style="text-align: center">0.000</td>
      <td style="text-align: center">0.000</td>
    </tr>
    <tr>
      <td>pythia</td>
      <td style="text-align: right">160m</td>
      <td style="text-align: center">5</td>
      <td style="text-align: center">0.000</td>
      <td style="text-align: center">0.000</td>
      <td style="text-align: center">0.000</td>
    </tr>
  </tbody>
  <tbody>
    <tr>
      <td>pythia</td>
      <td style="text-align: right">70m</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">0.000</td>
      <td style="text-align: center">0.000</td>
      <td style="text-align: center">0.000</td>
    </tr>
    <tr>
      <td>pythia</td>
      <td style="text-align: right">70m</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">0.000</td>
      <td style="text-align: center">0.000</td>
      <td style="text-align: center">0.000</td>
    </tr>
    <tr>
      <td>pythia</td>
      <td style="text-align: right">70m</td>
      <td style="text-align: center">2</td>
      <td style="text-align: center">0.000</td>
      <td style="text-align: center">0.000</td>
      <td style="text-align: center">0.000</td>
    </tr>
    <tr>
      <td>pythia</td>
      <td style="text-align: right">70m</td>
      <td style="text-align: center">3</td>
      <td style="text-align: center">0.000</td>
      <td style="text-align: center">0.000</td>
      <td style="text-align: center">0.000</td>
    </tr>
    <tr>
      <td>pythia</td>
      <td style="text-align: right">70m</td>
      <td style="text-align: center">4</td>
      <td style="text-align: center">0.000</td>
      <td style="text-align: center">0.000</td>
      <td style="text-align: center">0.000</td>
    </tr>
    <tr>
      <td>pythia</td>
      <td style="text-align: right">70m</td>
      <td style="text-align: center">5</td>
      <td style="text-align: center">0.000</td>
      <td style="text-align: center">0.000</td>
      <td style="text-align: center">0.000</td>
    </tr>
  </tbody>
</table>

<h3 id="memorization">Memorization</h3>

<p>We use one-shot for training and evaluation</p>

<table>
  <tbody>
    <tr>
      <td>llama2_chat</td>
      <td>13b</td>
      <td>0.562</td>
      <td>gpt_template</td>
    </tr>
    <tr>
      <td>llama2_chat</td>
      <td>13b</td>
      <td>0.561</td>
      <td>gpt_wikipage</td>
    </tr>
    <tr>
      <td>llama2_chat</td>
      <td>13b</td>
      <td>0.558</td>
      <td>gpt_wikipage_and_template</td>
    </tr>
    <tr>
      <td>llama2_chat</td>
      <td>7b</td>
      <td>0.647</td>
      <td>gpt_template</td>
    </tr>
    <tr>
      <td>llama2_chat</td>
      <td>7b</td>
      <td>0.650</td>
      <td>gpt_wikipage</td>
    </tr>
    <tr>
      <td>llama2_chat</td>
      <td>7b</td>
      <td>0.646</td>
      <td>gpt_wikipage_and_template</td>
    </tr>
    <tr>
      <td>llama2</td>
      <td>13b</td>
      <td>0.465</td>
      <td>gpt_template</td>
    </tr>
    <tr>
      <td>llama2</td>
      <td>13b</td>
      <td>0.476</td>
      <td>gpt_wikipage</td>
    </tr>
    <tr>
      <td>llama2</td>
      <td>13b</td>
      <td>0.462</td>
      <td>gpt_wikipage_and_template</td>
    </tr>
    <tr>
      <td>llama2</td>
      <td>7b</td>
      <td>0.445</td>
      <td>gpt_template</td>
    </tr>
    <tr>
      <td>llama2</td>
      <td>7b</td>
      <td>0.439</td>
      <td>gpt_wikipage</td>
    </tr>
    <tr>
      <td>llama2</td>
      <td>7b</td>
      <td>0.438</td>
      <td>gpt_wikipage_and_template</td>
    </tr>
    <tr>
      <td>pythia</td>
      <td>12b</td>
      <td>0.366</td>
      <td>gpt_template</td>
    </tr>
    <tr>
      <td>pythia</td>
      <td>12b</td>
      <td>0.366</td>
      <td>gpt_wikipage</td>
    </tr>
    <tr>
      <td>pythia</td>
      <td>12b</td>
      <td>0.366</td>
      <td>gpt_wikipage_and_template</td>
    </tr>
    <tr>
      <td>pythia</td>
      <td>6.9b</td>
      <td>0.163</td>
      <td>gpt_template</td>
    </tr>
    <tr>
      <td>pythia</td>
      <td>6.9b</td>
      <td>0.163</td>
      <td>gpt_wikipage</td>
    </tr>
    <tr>
      <td>pythia</td>
      <td>6.9b</td>
      <td>0.163</td>
      <td>gpt_wikipage_and_template</td>
    </tr>
    <tr>
      <td>pythia</td>
      <td>2.8b</td>
      <td>0.026</td>
      <td>gpt_template</td>
    </tr>
    <tr>
      <td>pythia</td>
      <td>2.8b</td>
      <td>0.026</td>
      <td>gpt_wikipage</td>
    </tr>
    <tr>
      <td>pythia</td>
      <td>2.8b</td>
      <td>0.026</td>
      <td>gpt_wikipage_and_template</td>
    </tr>
    <tr>
      <td>pythia</td>
      <td>1.4b</td>
      <td>0.009</td>
      <td>gpt_template</td>
    </tr>
    <tr>
      <td>pythia</td>
      <td>1.4b</td>
      <td>0.009</td>
      <td>gpt_wikipage</td>
    </tr>
    <tr>
      <td>pythia</td>
      <td>1.4b</td>
      <td>0.009</td>
      <td>gpt_wikipage_and_template</td>
    </tr>
    <tr>
      <td>pythia</td>
      <td>1b</td>
      <td>0.000</td>
      <td>gpt_template</td>
    </tr>
    <tr>
      <td>pythia</td>
      <td>1b</td>
      <td>0.000</td>
      <td>gpt_wikipage</td>
    </tr>
    <tr>
      <td>pythia</td>
      <td>1b</td>
      <td>0.000</td>
      <td>gpt_wikipage_and_template</td>
    </tr>
    <tr>
      <td>pythia</td>
      <td>160m</td>
      <td>0.000</td>
      <td>gpt_template</td>
    </tr>
    <tr>
      <td>pythia</td>
      <td>160m</td>
      <td>0.000</td>
      <td>gpt_wikipage</td>
    </tr>
    <tr>
      <td>pythia</td>
      <td>160m</td>
      <td>0.000</td>
      <td>gpt_wikipage_and_template</td>
    </tr>
    <tr>
      <td>pythia</td>
      <td>410m</td>
      <td>0.010</td>
      <td>gpt_template</td>
    </tr>
    <tr>
      <td>pythia</td>
      <td>410m</td>
      <td>0.010</td>
      <td>gpt_wikipage</td>
    </tr>
    <tr>
      <td>pythia</td>
      <td>410m</td>
      <td>0.010</td>
      <td>gpt_wikipage_and_template</td>
    </tr>
    <tr>
      <td>pythia</td>
      <td>70m</td>
      <td>0.000</td>
      <td>gpt_template</td>
    </tr>
    <tr>
      <td>pythia</td>
      <td>70m</td>
      <td>0.000</td>
      <td>gpt_wikipage</td>
    </tr>
    <tr>
      <td>pythia</td>
      <td>70m</td>
      <td>0.000</td>
      <td>gpt_wikipage_and_template</td>
    </tr>
  </tbody>
</table>

      </d-article>

      <d-appendix>
        <d-footnote-list></d-footnote-list>
        <d-citation-list></d-citation-list>
      </d-appendix>

<!--
    </div>
 -->
      <hr>
<!-- Footer -->    <footer class="sticky-bottom mt-5" style="border: none;border-top:0px">
      <div class="container" style="text-align: center; ">
        © Copyright 2024 Bumjini  . Powered by Jekyll with al-folio theme. Hosted by GitHub Pages.

      </div>
    </footer>
    
    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.2/dist/umd/popper.min.js" integrity="sha256-l/1pMF/+J4TThfgARS6KwWrk/egwuVvhRzfLAMQ6Ds4=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.min.js" integrity="sha256-SyTu6CwrfOhaznYZPoolVw2rxoY7lKYKQvqbtqN93HI=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    
    

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
  /*
   * This JavaScript code has been adapted from the article 
   * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar, 
   * published on the website https://css-tricks.com on the 7th of May, 2014.
   * Couple of changes were made to the original code to make it compatible 
   * with the `al-foio` theme.
   */
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    let navbarHeight = $("#navbar").outerHeight(true);
    $("body").css({ "padding-top": 0 });
    $("progress-container").css({ "padding-top": 0 });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  /*
   * The offset between the html document height and the browser viewport
   * height will be greater than zero if vertical scroll is possible.
   * This is the distance the user can scroll
   */
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>
  </div></body>
  
  <d-bibliography src="/assets/bibliography/all.bib">
  </d-bibliography>
  <script src="/assets/js/distillpub/overrides.js"></script>

  <center>
    <a  href="">
    <img src="/assets/common/KAIST-hi.gif" width="40px" style="margin-right:0px; padding-bottom: 3px;">
    </a>
  </center>
  <hr> 

  </html>
