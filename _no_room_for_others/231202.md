---
layout: distill
title:   ' 231202 ğŸš€' 
date: 2023-12-02
giscus_comments : true
description: 
bibliography: all.bib
authors: 
    - name: Bumjin Park
      affiliations:ë¶„
        name: KAIST

---


ì•”ê¸°í•˜ëŠ” ê²ƒì€ privacy ë¥¼ ì¹¨í•´í•˜ë©° ë¬¸ì¥ì˜ í€„ë¦¬í‹°ë¥¼ ë‚®ì¶”ë©°, ê³µì •ì„±ì„ í•´ì¹œë‹¤. 
ê·¸ëŸ¬ë‹ˆê¹Œ ëª¨ë¸ì´ ë°ì´í„°ë¥¼ ì•”ê¸°í•˜ëŠ”ì§€ ì¸¡ì •í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•˜ë‹¤. [Q] ì—°êµ¬ì—ì„œ ë°ì´í„°ì— ëŒ€í•´ì„œ ì•”ê¸°ì™€ ê´€ë ¨ëœ ì„¸ ê°€ì§€ ì„ í˜• ê´€ê³„ë“¤ì„ ì†Œê°œí•œë‹¤. 

1. ì•”ê¸°ëŠ” ëª¨ë¸ ì‚¬ì´ì¦ˆê°€ ì¦ê°€í• ìˆ˜ë¡ ì¦ê°€í•œë‹¤. 
2. ì¤‘ë³µë ìˆ˜ë¡ ì¦ê°€í•œë‹¤. 
3. prompt ë¡œ ì‚¬ìš©ëœ í† í° ë‹¨ì–´ ìˆ˜ì— ë”°ë¼ ì¦ê°€í•œë‹¤. 


*training data extraction attacks* (Carlini et al., 2020);
1. by querying the GPT-2 language model, Carlini et al. (2020) (manually) identified just 600 memorized training examples out of a 40GB training dataset.
2. In contrast, we are able to show that the 6 billion parameter GPT-J model (Black et al., 2021; Wang and Komatsuzaki, 2021) memorizes at least 1% of its training dataset: The Pile (Gao et al., 2020).
3. While McCoy et al. (2021) broadly study the extent to which language models memorize, their focus is on how to avoid the problem and ensure novelty of model outputs, rather than on studying model risk through identifying the maximal amount of data memorization.

[Q] ë…¼ë¬¸ ì´ì „ ì—°êµ¬ë“¤ì€ ì •ë³´ ì¶”ì¶œì´ ê°€ëŠ¥í•˜ë‹¤ëŠ” ì  (URLs, phone numbers, and other personal information) ì„ ë³´ì˜€ê³ , ì´ ë¶€ë¶„ì— ëŒ€í•´ì„œ ëª…ì‹œì ìœ¼ë¡œ ì •ëŸ‰í™”í•œ ì—°êµ¬ê°€ ë¶€ì¡±í–ˆë‹¤. 


* differential privacy : removing the personal information from the training data would not hurt the model performance. (Dwork 2006)
* ì¶”ê°€ì ìœ¼ë¡œ Kandpal 2022 ì—°êµ¬ëŠ” í•™ìŠµ ë°ì´í„°ê°€ ì¤‘ë³µë¨ì— ë”°ë¼ì„œ ëª¨ë¸ì´ ì–¼ë§ˆë‚˜ ë°ì´í„°ë¥¼ ì•”ê¸°í•œ ë°ì´í„°ë¥¼ ë‚´ë³´ë‚´ëŠ”ì§€ í™•ì¸í•˜ì˜€ë‹¤. 
* 

> We first construct a set of prompts from the modelâ€™s training set. By feeding prefixes of these prompts into the trained model, we check whether the model has the ability to complete the rest of the example verbatim. (í•™ìŠµë°ì´í„°ì˜ ì•ë¶€ë¶„ì„ ê·¸ëŒ€ë¡œ ì§‘ì–´ë„£ê³ , ëª¨ë¸ì´ í•´ë‹¹ ë¶€ë¶„ì„ ê¸°ì–µí•˜ëŠ”ì§€ í™•ì¸. )

ëª‡ ê°€ì§€ ì—°ê²°ëœ ì„ í–‰ ì—°êµ¬ë“¤ 

* our analysis on model scale is informed by preliminary experiments in (Zhang et al., 2017; Carlini et al., ã„´2020)
* data duplication experiments follow in the line of Lee et al. (2021),
* context length experiments build on hypotheses by Carlini et al. (2020); Ziegler (2021).


**our choice of decoding strategy does not significantly impact our results.**


In this paper we randomly choose subsets of roughly 50,000 sequences, allowing us to efficiently run inference in just a few hours.
Therefore, our second subset is a random sample normalized by both sequence lengths and duplication counts, which allows us to accurately measure memorization of large language models in the worst-case, on highly duplicated data with long prompts.

For each length from 50 to 500 tokens, we collect 50,000 examples duplicated varying numbers of times, totaling roughly 500,000 sequences. ` L âˆ’ 50 tokens and report the sequence as â€œextractableâ€ if the model exactly emits the next 50
token suffix of this sequence.


ë” í° ëª¨ë¸ì´ ë”ìš± ë©”ëª¨ë¼ì´ì œì´ì…˜ì„ ì˜í•œë‹¤. ì´ëŠ” ì˜ˆì¸¡ ì„±ëŠ¥ìœ¼ë¡œ ì¸í•´ì„œ ë°œìƒë˜ëŠ” í˜„ìƒì€ ì•„ë‹ˆë‹¤.
ë³¸ ì—°êµ¬ì—ì„œëŠ” discoverability phenomenon ì´ë¼ëŠ” í˜„ìƒì„ ì œì•ˆí•œë‹¤. í•´ë‹¹ í˜„ìƒì€ ëª¨ë¸ì—ê²Œ ë” ë§ì€ ì»¨í…ìŠ¤íŠ¸ ì •ë³´ë¥¼ ì£¼ë©´ memorization fractionì˜ ë¹„ìœ¨ì´ ì¦ê°€í•˜ëŠ” í˜„ìƒì„ ë‚˜íƒ€ë‚¸ë‹¤. ë”°ë¼ì„œ, prompt ì˜ ê¸¸ì´ë¥¼ ì¤„ì—¬ì„œ í•´ë‹¹ í˜„ìƒì„ ë§‰ì•„ì•¼ í•œë‹¤ê³  ë§í•œë‹¤. 

---

OpenAI ì—ì„œ Embedding ì„ ê°€ì ¸ì˜¬ ìˆ˜ ìˆë‹¤. ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ëª¨ë¸ì— ì¶”ê°€ì ì¸ í•™ìŠµì„ ì§„í–‰í•˜ê³ , ì´ ë•Œ, í•™ìŠµì„ ë°°í¬í•  ë•Œ, watermark ë¥¼ í™œìš©í•˜ì—¬, í•´ë‹¹ ì„ë² ë”©ìœ¼ë¡œë¶€í„° ë°œìƒëœ ì»¨í…ì¸ ë¼ëŠ” ì ì„ ëª…ì‹œí•œë‹¤. 


[Q] Quantifying memorization across neural language models 
[H] HARE: Explainable Hate Speech Detection with Step-by-Step Reasoning
[Mc] R. Thomas McCoy, Paul Smolensky, Tal Linzen, Jianfeng Gao, and Asli Celikyilmaz. How much do language models copy from their training data? Evaluating linguistic novelty in text generation using RAVEN. CoRR, abs/2111.09509, 2021.
[Carlini] The secret sharer: Evaluating and testing unintended memorization in neural networks.
[EaaS] Are you copying my model? Protecting the copyright of large language models for EaaS via Backdoor Watermark 