---
layout: distill
authors: 
    - name: Bumjin Park
      affiliations:
        name: KAIST
bibliography: all.bib
giscus_comments: true
disqus_comments: true
date: 2023-07-10
featured: true
toc:
  - name: Problems
title: 'Source Identification for Generative Models'
description: 'Can we provide the source of generation?'
img: /assets/side_articles/deconvolution/checkboard_example.png
importance: 2 
---

> This is a problem formulation for finding the source of generation. 

## 1. Introduction 


In the training of a deep neural network, we collect a large amount of data and process the data only for training purposes. 
The pre-process mostly decreases the amount of information to abandon irrelevant information in data and provides compact representations for the training data by discarding information which is not helpful for the training. The process of filtering information is essential to properly match the training objective efficiently and robustly <d-footnote> For example, in the classification model, we use correlative features. In the field of NLP, we use natural sentences without meta data. </d-footnote>. Although such filtering can benefit the training, we discard even necessary meta data which helps to identify the information such as **the source of data**. One example would be the copyright of lyrics or contents in a book. For example, can you identify the source of the sentence below? That is, who said the sentence.
> Creating safe AGI that benefits all of humanity  <br>   - From : ???

The source of the sentence is [website of OpenAI ©](https://openai.com/). As the source is entangled with the sentence, we get additional information such as
* The underlying context of the sentence
* The factuality of the sentence 
* The meaning of AGI (if you are in the AI field.)

Even though there are benefits of knowing the source location of the data, the source link of data may not required to train neural networks <d-footnote> For example, the causal language modeling of GPT<d-cite key="brown2020language"/> and the masked language modeling of BERT do not require the source link to maximize the training objective. </d-footnote>. However, the source information is highly important when our objective is not performance but tracing the source of the information that the generative model provides. In short, 

> <text style="color:red"> Only for the training purpose, discarding irrelevant information is beneficial. But, we don't know made the information.  </text>

Within the progress of generative models, another training mechanism is required to ensure the safety issues for both models and data. 



---

## 2. Problem Definition  

The fundamental problem is the identification of the source of a corpus called as source identification problem (SIP).  Given a corpus, scrapped from a website or generated by chatGPT, we identify the original source of the corpus. The casual link of source $S$, text $x$, generator $G$, and the generated text $\hat{x}$ is as follows: 

$$
\begin{equation}
s \rightarrow x \rightarrow  G \rightarrow  \hat{x}
\end{equation}
$$

The SIP is a identification problem of the source of the text $x$ or the generated text.

$$
\begin{gather}
x \rightarrow s \\ 
\hat{x} \leftarrow s
\end{gather}
$$

However, it is a ill-posed problem as multiple sources are possible. 
As finding the source from corpus is **neither injective nor surjective**. 
For example, the corpus *"apple is delicious"* is a quite common sentence and lots of websites can include the corpus (non-injective) and the corpus *"fine apple is pricked by pineapple."* may not have the source, but generated by a generative model (non-surjective). In addition if the text is slightly changed by adding *"The"*, the corpora have almost same meaning and the sources must be equal.

$$
\begin{equation}
\hat{x} \rightarrow \{s_1, s_2, \cdots\}
\end{equation}
$$

As such, the source verification problem is a problem of generating possible multiple sources of a given corpus. Although, functionally learning the Equation (4) can solve the source identification problem, we don't know the actual meaning of mapping corpora to sources. One reason is that a corpus has semantic and lexical information and two different corpora can have exactly same information.


---
## 3. Building Blocks of SIP

Before proceeding on the detailed discussion on SIP, we discuss on the proper definitions for sources and corpora. **C-corpora (copyrighted-corpora)** are semantic or lexical contents in the source.   **Source** is the genuine location of c-corpora.

### 3.1 Copyrighted-Corpora

#### Which information

The copyrighted-corpora is  copyrighted a corpora in two perspectives, semantic anc lexical.

> Ex) Creating safe AGI that benefits all of humanity 
 * Semantic : AI system benefits all people 
 * Lexical : Creating, safe, AGI, benefit, humanity

As such, we have the following properties for copyrighted-corpora 
* `Semantic Source`   : the semantic meaning of a copyrighted-corpus comes from the source
* `Lexical Source`  : the lexical information of a copyrighted-corpus comes from the sentence

#### Which Sources

One additional property is that a corpus can be formed by multiple sources of corpora. For example, we can combine two corpus from OpenAI and Meta each to make the following sentence.  [© from [Meta's Action](https://about.meta.com/actions/)]
> Ex) Creating safe AGI that benefits all of humanity **by keeping people safe and making a positive impact.** 

#### Definition of Copyrighted-Corpora

> <strong style="font-style:normal">Definition [Copyrighted-Corpora]</strong> <br> <text style="font-style:normal"> Copyrighted-corpora is a corpora whose lexical or semantic meaning is similar with corpus from a source or combined by multiple corpus originated from possibly multiple corpus.  </text>

This definition is similar to how we think about copyright.  Even though a sentence is not totally matched with any sentence in training dataset, 
Multiple sentences from other sources can be used to form the sentence by extracting (1) semantic information and (2) lexical information. 

### 3.2 Source 

Source is the origin of sentences. The origin could be companies, persons, websites which have the copyright of the sentences. We list some possible sources. 

* `Company` : The company who has the copyright of overall contents (OpenAI, Meta, ...) 
* `Person`  : The labeled person who talked or wrote sentences (Martin Ruther King, ...)
* `Website` : The well known website (Wiki, Reddit, ... )
* `Hyperlinks` : Specific link of website  (https://distill.pub/2020/circuits/)    




## Related Work 

There are several ways to provide the source link of generated sentences. 

1. Generation : directly generate the sources with labels.
2. Watermark: inject identifiable keys to sentences which is visible in the inference steps 
3. Web search engine : search websites with a GPT and provide links
4. Prompt tuning : ask GPT to provide the source 

| Methods | Procedure  |   Pros |   Cons  | 
| ------  | -------- | ----------- | ------  | 
| **Generation** | $G \rightarrow \hat{x}  \rightarrow \hat{G} \rightarrow s$   | Easily learnable  | in-scalable, hard to believe |   
| **Watermark**  | $G \rightarrow (s), h \rightarrow \hat{x}$       | Only requires inference steps  | Unclear how to apply it to NLP.(Fourier Frequency) | 
| **Web Search** | $G \rightarrow [s_1,s_2,\cdots ] \rightarrow s_k \rightarrow \hat{x}$    | Direct identification | does not provide model's knowledge in inference | 
| **Prompt Tuning**  | $G \rightarrow \hat{x}, S$  | The most easiest way  | hard to believe |   


### Identification via Generation

The most simple way is to train another GPT model to recover the source link of the sentences. 
When a GPT model $G$ generates sentences $\hat{x}$, another GPT-like model generates the description of sources $S$.  

$$ 
G \rightarrow \hat{x}  \rightarrow \hat{G} \rightarrow s
$$ 



### Watermark
Previous work is done in vision domain which has continuous representations.
Watermark is a simple way to encode information in the representation and previously explored in diffusion models <d-cite key="fernandez2023stable"/>,<d-cite key="wen2023tree"/>. Although it is compelling way of encoding a secret key in the generated stuffs. It is only available for syntactic representation and unclear for the semantic meaning itself. When a GPT model $G$ generates sentences $\hat{x}$, the inner representation $h$ provokes the source of sentences. 

$$ 
G \rightarrow (s), h \rightarrow \hat{x} 
$$ 



### WebGPT and Bing 

One way to prevent the copyright issues is to make a model directly uses the corpus in a source in the inference procedure. This method  can  provide the source link to the end users. Recently WebGPT uses the Bing search engine to improve the factuality and to provide the external links <d-cite key="nakano2021webgpt"/>, [[OpenAI's blog](https://openai.com/research/webgpt)]. 

$$ 
G \rightarrow [s_1,s_2,s_3,\cdots ] \rightarrow s_k \rightarrow \hat{x} 
$$ 


### GPT Prompt tuning 

One direct way is to ask the model to provide the source link [[related post](https://www.zdnet.com/article/how-to-make-chatgpt-provide-sources-and-citations/)].

> Please provide sources for the previous answer <br> Please provide URL sources <br> Please provide 10 URL sources

$$ 
G \rightarrow \hat{x}, S 
$$ 



# About Training 



## Related Work 



### Parameter Efficient Fine-tuning 




---