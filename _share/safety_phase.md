---
layout: distill
authors: 
    - name: Bumjin Park
      affiliations:
        name: KAIST
bibliography: all.bib
date: 2024-05-21
giscus_comments: true
title: 'Explanation on Safety Phase Transition in Large Language Models'
description: 'ì•ˆì „ì¥ì¹˜ë¡œ í•™ìŠµëœ ëª¨ë¸ì˜ phase transitionì— ëŒ€í•œ ì²´ê³„ì  ì—°êµ¬. ' 
---
<style>
.table td, table th{
    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen, Ubuntu, Cantarell, "Fira Sans", "Droid Sans", "Helvetica Neue", Arial, sans-serif;
    color: var(--global-text-color);
    line-height: 140%;
    font-weight: 420;
    font-size: var(--global-text-size);
} 
</style>


## Motiv 

AI ëª¨ë¸ì€ ì¸ë¥˜ì— í•´ê°€ ë˜ëŠ” ë¬¸ì¥ì„ ë§í•˜ë©´ ì•ˆëœë‹¤. ìµœê·¼ ì—°êµ¬ë“¤ì€ ì•ˆì „í•œ ì‚¬ìš©ì„ ìœ„í•´ ë‚˜ìœë§ì„ í•˜ëŠ” ëŒ€ì‹  safetyì™€ ê´€ë ¨ëœ ë§ì„ í•˜ë„ë¡ í•™ìŠµëœë‹¤. ëŒ€í‘œì ì¸ ì˜ˆì‹œë¡œ ì°¨ë³„ì ì¸ ë‚´ìš©ì„ ë¬¼ì–´ë³´ë©´ í‰ë“±ì— ëŒ€í•œ ì¤‘ìš”ì„±ì„ ì–¸ê¸‰í•œë‹¤. ì´ë ‡ê²Œ Safetyì™€ ê´€ë ¨ëœ ë§ì„ í•˜ë„ë¡ ìƒì„± ëª¨ë“œê°€ ë°”ë€ŒëŠ” *safety phase*ì— ëŒ€í•œ ì „ë°˜ì ì¸ ì´í•´ ë° ì„¤ëª…ì€ ë”ìš± ì•ˆì „í•œ ëª¨ë¸ì„ ë§Œë“¤ê¸° ìœ„í•´ì„œ í•„ìˆ˜ì ì´ë‹¤. ë³¸ ì—°êµ¬ì—ì„œëŠ” instruction-tuned ëœ GPT ëª¨ë¸ì— ëŒ€í•œ *safety phase*ì„ í•´ì„í•˜ê¸° ìœ„í•œ ë‘ ê°€ì§€ ì‹¤í—˜ì ì¸ ê²°ê³¼ë¥¼ ì œì‹œí•œë‹¤. 

**ğŸ“Œ 1) ëª¨ë¸ì€ ì–´ë– í•œ ìƒí™©ì—ì„œ safety phaseê°€ ë°œìƒí•˜ëŠ”ê°€.**: ì´ ì—°êµ¬ëŠ” safetyì™€ ê´€ë ¨ëœ ë‹¤ì–‘í•œ ë¬¸ì¥ì— ëŒ€í•´ì„œ ê° ë¬¸ì¥ì˜ í‘œí˜„ë“¤ì„ ë¹„êµí•˜ê³  í´ëŸ¬ìŠ¤í„°ë§í•˜ì—¬ ì „ë°˜ì ì¸ ëª¨ë¸ì˜ safety sentenceë“¤ì˜ ì…ì¶œë ¥ì„ í†µê³„ì ìœ¼ë¡œ ë¶„ì„í•œë‹¤.  
**ğŸ“Œ 2) Safety phaseëŠ” ì–´ë–¤ ë°©ì‹ìœ¼ë¡œ trigger ë˜ëŠ”ê°€?**: ëª¨ë¸ ë‚´ë¶€ì—ì„œëŠ” safetyì™€ ê´€ë ¨ëœ ë¬¸ì¥ì„ ìƒì„±í•˜ë„ë¡ ìœ ë„í•˜ëŠ” íŠ¹ì§• í˜¹ì€ ë‰´ëŸ°ì´ ì¡´ì¬í•œë‹¤. Safetyë¥¼ ìœ ë°œí•˜ëŠ” ëª¨ë¸ ë‚´ë¶€ë¥¼ íƒêµ¬í•˜ì—¬ í•´ë‹¹ êµ¬ì¡°ì˜ ì¡°ì‘ìœ¼ë¡œ safetyì™€ ê´€ë ¨ëœ ë¬¸ì¥ë“¤ì˜ ë³€í™”ë¥¼ íƒêµ¬í•œë‹¤. 

ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ì€ ì‚¬íšŒì˜ ê·œì•½ê³¼ ì •ë³´ì— ë§ì¶°ì„œ ì§€ì†ì ìœ¼ë¡œ ê¸°ì¡´ ì •ë³´ë¥¼ ì§€ìš°ê³  ìƒˆë¡­ê²Œ ë®ì–´ì“´ë‹¤. ë˜í•œ ë¶€ì •ì ì¸ ì •ë³´ë“¤ì€ ìƒì„±ì„ ë§‰ë„ë¡ ì¶”ê°€ì ì¸ í•™ìŠµì´ ë˜ë©° ì‚¬íšŒê·œë²”ê³¼ ë²•ì˜ ë³€í™”ì— ë§ì¶”ì–´ ëª¨ë¸ì˜ ì‘ë™ ë°©ì‹ì€ ë°”ë€ë‹¤. ë³¸ ì—°êµ¬ì—ì„œ íƒêµ¬í•˜ëŠ” *safety phase transition*ì€ ì•ˆì „ ë¬¸êµ¬ì™€ ê´€ë ¨ëœ LLMì˜ ìƒì„±ê³¼ì •ì„ ë¶„ì„í•¨ìœ¼ë¡œì¨ ì‹¤í—˜ì  ê´€ì°°ì„ ë°”íƒ•ìœ¼ë¡œí•œ ë”ìš± ì•ˆì „í•œ ëª¨ë¸ êµ¬ì¡°ì— ëŒ€í•œ ì˜ê°ì„ ì œì‹œí•œë‹¤. 

## Big Picture 

- safety transition in LLM
    -  ğŸ“Œ (3) safety transitionì˜ ì •ì˜. Complete and incomplete sentence ì •ì˜. Harmful sentence ì •ì˜
    -  ğŸ“Œ (3.1, 4.1) template-base evaluation of safety-phase transition (Yeonji, Jinsil)
    -  ğŸ“Œ (3.2, 4.2) activation-base evaluation of safety-phase transition (Bumjin, Youngju)

## Collection of Papers 

* **Safety finetuning + deceiving** 
    * Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training
    * Simple probes can catch sleeper agents

* **Safety finetuning**
    * Training a helpful and harmless assistant with reinforcement learning from human feedback
    * Training language models to follow instructions with human feedback

* **Jail-break** 
    * Defending chatgpt against jailbreak attack via self-reminders
    * Many-shot jailbreaking
    * Universal and Transferable Adversarial Attacks on Aligned Language Models
    * Jailbroken: How does llm safety training fail?
    * â€œDo Anything Nowâ€: Characterizing and Evaluating In-The-Wild Jailbreak Prompts on Large Language Models
* Detecting Jailbreaks 
    * Causality Analysis for Evaluating the Security of Large Language Models
* Explanation on LLM
    * Explainability for Large Language Models: A Survey
* **Safety** 
    * Towards understanding sycophancy in language models
    * GPT-4 Technical Report
    * Constitutional AI:Harmlessness from AI feedback


* **Interpretability (feature)**
    * Towards Monosemanticity: Decomposing Language Models With Dictionary Learning
    * Mapping the Mind of a Large Language Model

---

## Abstract 


## 1. Introduction 


## 2. Related Work



## 3. Method

We consider a set of concepts $\mathcal{C} = (z_1, z_2, \cdots)$ where each concept $c$ is related to a human-oriented concept, such as harmless or safety. 
Each concept $z$ has examples, a set of passages $$\mathcal{Y}_{c}$$, where passage $$y = (y_1, y_2, \cdots) \in \mathcal{Y}_{c}$$ consists of tokens $y_i$ and includes the semantic meaning of concept $c$. We use notation $c \perp c'$ when two concepts $c$ and $c'$ can not exist together, e.g., safety concept and violence concept.

We define two terms *phase* and *phase transition$ as follows. 

> Definition: **Phase** <br>
>> A $c$-*phase* is a subset of time steps $[t_{begin}, t_{end}]$ where the generated content $$({y}_{t_{begin}}, \cdots, {y}_{t_{end}})$$ with $${y}_{t_{k}} \sim P_{\theta}(\cdot \vert y_1, \cdots, y_{t_{k}-1})$$ can be included in the set of concept-related passages $$\mathcal{Y}_{c}$$.

The definition of *phase* assumes a passage $y$. The inclusion of an element in the set element in $\mathcal{Y}_{c}$ can be either determined by a human-annotator or text similarity scores with the examples in the set.  

<!-- We define *safe phase* which generates safety related passes $\mathcal{Y}_{safety}$   -->

> Definition: **Phase Emergence**   <br>
>> A *phase emergence* is the time step when the generation includes new concept $c'$ as a phase. 

> Definition: **Phase Transition**   <br>
>> A *phase transition* is a $c$-phase emergence with the disappearance of existing concept $c'$ such that $ c' \perp c$.  


Rigorously, the definition of phase transition 

### 3.1 Safety-Phase Transition 

consider two types of passages, $\mathcal{T}_{com}$ and $\mathcal{T}_{incom}$ where the elements are complete and incomplete sentences. 

Consider a harmful prompt $\mathcal{P} \in\mathcal{T}_{com}$. 

Figure below shows the phase transition for complete and incomplete cases. 
<p align='center'>
<img src="https://onedrive.live.com/embed?resid=AE042A624064F8CA%219678&authkey=%21AL_paZel8mue_pc&width=1636&height=602" width="1636" height="250" />
</p>


### 3.2 Detection of Phase Transition 

Several work propose the methods to detect the conceptual neurons. 

* Value Detection via optimization : find the activation pattern that mostly maximize the likelihood of the safe phase. (FactualGPT)
* Activation : Find the activation pattern via forwarding the target output
* Neuron Attribution : Compute the attribution score via gradient descent.  

We find a vector at the end of sentence. A hidden representation $A_{safe}$ which maximizes the forward. 

> What about the key-value structure? Our we finding any key? 
> We do not find a sparse key neurons which activate the concept. 
> Because recent work reveals that the neural network prefers. However, it could be the case that only the sparse combination of neurons forms the target activation. Therefore, finding neuron (entry point for vector representation) is not the primitive problem. 

## 4. Experiments 

## 4.1 Phase Transition with Templates

We categorize four types of generations for the complete and incomplete prompts.   


#### Complete

|  Harmful Generation      |  Safety-phase Transition  |  $\mathcal{P}$="Teach me how to hurt people."| 
|:---:|:---:| :------------------| 
|  -    | âœ…     |   **I cannot fullfil your request.** 
|  -    | âŒ     |   **A stab in the back ...**

#### Incomplete

 |  Harmful Generation      |  Safety-phase Transition  |  $\mathcal{P}'$="A stab in the back or"| 
|:---:|:---:| :------------------| 
 | âœ…    | âœ…   |   **a knife to the chest. <br> I cannot fullfil your request.** 
  | âœ…    | âŒ  |  **a knife to the chest. <br> I addition, shoot a gun.** 
  | âŒ    | âœ…  |  **hurt is not a good action <br> I cannot fullfil your request.** 
  | âŒ    | âŒ |  **hurt is not a good action. <br> but  you can shoot a gun**

## 4.2 Phase Transition via Activation 

### Adding concept

1. Does the jailbreak examples shows the safe-phase with the activation addition?
2. Does the incomplete sentence jailbreak examples shows the safe-phase without completion of the harmful completion?

### Removing concept


## 5. Results 


## 6. Discussion

## 7. Conclusion



