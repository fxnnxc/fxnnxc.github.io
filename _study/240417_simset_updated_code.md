---
layout: distill
authors: 
    - name: Bumjin Park
      affiliations:
        name: KAIST
bibliography: all.bib
disqus_comments: false
giscus_comments: false
date: 2024-04-16
featured: true
title: 'Simset + inductive bias regularization (Korean)'
description: 'Simsetì— ëŒ€í•´ì„œ ì œëŒ€ë¡œ ì—°êµ¬í•˜ê¸° ìœ„í•´ì„œ inductive biasë¥¼ ìœ„í•œ ì‹¤í—˜ ì„¤ê³„ ë° ì§„í–‰.'
---
## Does Structural Document Memory Help Factual Recall? 

* ğŸ—“ï¸ **Experiment Date** : 24.04.17
* ğŸŒ¸ **Scope** : FEVER dataset and counterfactual dataset construction and experiments. 
* ğŸ§‘ğŸ»â€ğŸ’» **Code**:[Simset]()
* ğŸ˜® **Status** : Running Experiments 
  - [ ] &nbsp; *Documentation of Settings*
  - [ ] &nbsp; *Implementation of Codes*
  - [ ] &nbsp; *Running Experiments*
  - [ ] &nbsp; *Record Results*
  - [ ] &nbsp; *Analysis Results* 


## Questions 

1. Inductive Structure ë©”ëª¨ë¦¬ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì€ ì•”ê¸° ì†ë„ë¥¼ ì˜¬ë¦¬ëŠ”ê°€? 
2. Inductive Structure ë©”ëª¨ë¦¬ë¡œ í•™ìŠµëœ ì •ë³´ëŠ” ë¬¸ì„œ ê´€ë ¨ëœ QAì— ëŒ€í•œ ì •í™•ë„ë¥¼ ë†’ì´ëŠ”ê°€? 


## Data Processing Pipeline 

We use two datasets, CounterFact and FEVER datasets.
For the related documents of CounterFact, we find wiki pages related to the subject word in Counterfact including single topic which has 1285 documents.  For FEVER data, the evidence page for paper_dev dataset which includes 449 documents. For additional data processing, we provide details in the Appendix. 

### FEVER Documents


### CounterFact Documents


## Baselines and Proposed Methods

For all cases, we use the memory size of 1024, 4096, 12288. 
For all cases, we use four types of model architectures: Llama2 7B, Llama2 13B, Pythia 6.9B, Pythia 1B

* Baseline 1: no inductive entry bias
* Baseline 2: random entry bias
* Baseline 3: fixed partitioned entry bias
* Baseline 4: MemTRN (for future work)
* Proposed 1: Simset-1
* Proposed 2: Simset-2




----

## Experiment 1


### Training 1 (FEVER)

* Dataset: FEVER 

Document training for fixed epochs and optimizers

#### Evaluation 1 (FEVER) 

* Measure: negative log likelihood in training
* Models: Pythia, Llama

#### Evaluation 2 (FEVER)

* Measure: F1 score (a,b,c) The first character.  ` (a) ` 
* Models: Pythia, Llama

Document training for fixed epochs and optimizers

----

### Training 2 (Counterfact)

* Dataset: CounterFact

Document training for fixed epochs and optimizers

#### Evaluation 1 (Counterfact)

* Measure: negative log likelihood in training
* Models: Pythia, Llama



#### Evaluation 2 (Counterfact)

* Measure: F1 score for the target true (sampling five answer)
* Models: Pythia, Llama

Document training for fixed epochs and optimizers

----

## Experiment 2: Clustering Documents

ìœ„ ì‹¤í—˜ ì¤‘ì— ì œì¼ ì í•©í•œ ë°ì´í„°-ëª¨ë¸ì— ëŒ€í•´ì„œ ë¬¸ì„œ í´ëŸ¬ìŠ¤í„°ë§ì„ í–ˆì„ ë•Œ íš¨ê³¼. 
ì¤‘ì : í´ëŸ¬ìŠ¤í„°ë¥¼ í˜•ì„±í•˜ë©´ ì„±ëŠ¥ì´ ì €í•˜ë˜ëŠ”ê°€? í–¥ìƒë˜ëŠ”ê°€? 


## Experiment 3: Regularization Alphaì— ëŒ€í•œ ì •ë„ 

* 0ì— ê°€ê¹Œìš¸ìˆ˜ë¡ no inductive bias 
* í´ìˆ˜ë¡ inductive biasê°€ ë” ê°•í•´ì§„ë‹¤. 



## Experiment 4: Running on different Layer

For all cases, we evaluate validate the 10 internal layers. 
