---
layout: distill
authors: 
    - name: Bumjin Park
      affiliations:
        name: KAIST
bibliography: all.bib
disqus_comments: false
giscus_comments: false
date: 2024-04-07
featured: true
title: 'Simset ë°©ì‹ì€ document memorizationìœ¼ë¡œ Factualityì— ëŒ€í•´ì„œ ë†’ì€ ì„±ëŠ¥ì„ ë‚¼ ìˆ˜ ìˆëŠ”ê°€? (Korean)'
description: 'ë¬¸ì„œë¥¼ ì•”ê¸°í•˜ë„ë¡ ì €ì¥í•˜ëŠ” ê²ƒì€ ë„ì›€ì´ ëœë‹¤. ì¶”ê°€ì ì¸ ê¶ê¸ˆì¦ì€ êµ¬ì¡°ì ì¸ ë©”ëª¨ë¦¬ë¡œ ë¬¸ì„œë¥¼ ì•”ê¸°í•˜ëŠ” ê²ƒì´ ë„ì›€ì´ ë˜ëŠ”ì§€ ì—¬ë¶€ì´ë‹¤.'
---
## Does Structural Document Memory Help Factual Recall? 

* ğŸ—“ï¸ **Experiment Date** : 24.04.08 
* ğŸŒ¸ **Scope** : Few-shot classification for large language models with memorization adaptation. We use FEVER dataset 449 pages and related validation question <br> (#samples: valid 18999 train 311431). 
* ğŸ§‘ğŸ»â€ğŸ’» **Code**:[TBW]()
* ğŸ˜® **Status** : Running Experiments 
  - [ ] &nbsp; *Documentation of Settings*
  - [ ] &nbsp; *Implementation of Codes*
  - [ ] &nbsp; *Running Experiments*
  - [ ] &nbsp; *Record Results*
  - [ ] &nbsp; *Analysis Results* 

## Document-wise Memories 



## Experimental Settings 

We use the same setting in [the previous work](https://fxnnxc.github.io/study/240328_fever_neural_memory/) except the number of documents to 449 pages. 
We do not train the template for fever training dataset and directly evaluate the validation question and answer pairs by memorizing the evidence Wiki pages. 
We use Llama2 7B and  Llama2-Chat 7B for the memorization of documents. All the predictions are conducted with 1-shot setting where the few-shot examples are obtained from the training QA pairs of the same document. 


### Baselines and SimSet 

We use the following baselines for the experiment and evaluate the performance of SimSet structural memories for documents. 

1. Pretrained Status
2. Unstructured Neural Memory 
3. Structured with Disjoint Length Proportional 

We use the following Simset settings 
1. 1-shingling 
2. 2-shingling 
3. 3-shingling 

|Model            |  Pretrain |  Full  |  DLP |         |  1-shin  |    2-shin   |   3-shin   |    
|-----------------|-----------|--------|------|---------|----------|-------------|------------|
llama2 7b         |  0.435    |
llama2_chat 7b    |  0.556    |





