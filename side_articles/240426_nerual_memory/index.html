<!DOCTYPE html>
<!-- _layouts/distill.html -->
<html>
    
<head>    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Bumjini | Literature Reviews: Neural Memory, kNN augmentation and some other recent trends.</title>
    <meta name="author" content="Bumjini  " />
    <meta name="description" content="This article is a review of several papers related to the neural memory and utilization of data embeddings (12 papers)" />
    <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website" />


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous" />

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light" />

    <!-- Styles -->
    
    <link rel="shortcut icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ğŸª´</text></svg>">
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://fxnnxc.github.io/side_articles/240426_nerual_memory/">
    
    <!-- Dark Mode -->
    

  <!-- jQuery -->
<script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>
  <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams',
        inlineMath: [['$','$'], ['\\(','\\)']]
      },
      chtml: {
          scale: 1.0,
          minScale: .6,  
          mtextFontInherit: true,
          mtextInheritFont: true,
          merrorInheritFont: true,
        },
        svg: {
          scale: 1.2
        }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

  <!-- Distill js -->
  <script src="/assets/js/distillpub/template.v2.js"></script>
  <script src="/assets/js/distillpub/transforms.v2.js"></script>
  <script src="/assets/js/distillpub/overrides.js"></script>
  <!-- Page/Post style -->
  
</head>

  <d-front-matter>
    <script async type="text/json">{
      "title": "Literature Reviews: Neural Memory, kNN augmentation and some other recent trends.",
      "description": "This article is a review of several papers related to the neural memory and utilization of data embeddings (12 papers)",
      "published": "April 30, 2024",
      "authors": [
        {
          "author": "Bumjin Park",
          "authorURL": "",
          "affiliations": [
            {
              "name": "KAIST",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script>
  </d-front-matter>

  <body style="padding-top:0px" class="sticky-bottom-footer"> 
    
    <!-- Header --><header>
      
      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm sticky-top">
        <div class="container">
          <img>
          <!-- <img src="/assets/common/KAIST-hi.gif" width="40px" style="margin-right:0px; padding-bottom: 3px;"> -->
          
          <a class="navbar-brand title font-weight-lighter" href="https://fxnnxc.github.io/" style="margin-left:20px ;">
              <!--Bumjini-->
           <span class="font-weight-bold" style="font-size:larger;font-family:Times New Roman;">Bumjini    </span>
        </a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <!-- <li style="font-size: 17px" class="nav-item ">
                <a class="nav-link" href="/">  About Me</a>
              </li> -->
              
              <!-- Blog -->
              <li style="font-size: 17px" class="nav-item ">
                <a class="nav-link" href="/main_papers/"> Papers</a>
              </li>

              <li style="font-size: 17px" class="nav-item ">
                <a class="nav-link" href="/main_articles/"> Articles</a>
              </li>

              <li style="font-size: 17px" class="nav-item ">
                <a class="nav-link" href="/main_projects/"> Projects</a>
              </li>

              <!-- Other pages -->
              <li style="font-size: 17px" class="nav-item dropdown ">
                <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Others</a>
                <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown">
                  <a class="dropdown-item" href="/side_papers/">ğŸ—‚ï¸ side-paper</a>
                  <a class="dropdown-item" href="/side_articles/">ğŸ¥• side-articles</a>
                  <a class="dropdown-item" href="/book/">ğŸ“š book (korean)</a>
                </div>
              </li>
            </ul>
          </div>
        </div>
      </nav>
    </header>

    <!-- Content -->
    <div class="post distill">

      <d-title>
        <h1 style="font-size: 32px;">Literature Reviews: Neural Memory, kNN augmentation and some other recent trends.</h1>
        <p>This article is a review of several papers related to the neural memory and utilization of data embeddings (12 papers)</p>
      </d-title>

      <d-byline></d-byline>

      <d-article>
        

        <h2 id="reading-list">Reading List</h2>

<table>
  <thead>
    <tr>
      <th>Date</th>
      <th>Venue</th>
      <th>Title</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>2023</td>
      <td>NeurIPS</td>
      <td>Neural Priming for Sample-Efficient Adaptation</td>
    </tr>
    <tr>
      <td>2023</td>
      <td>NeurIPS</td>
      <td>Monitor-Guided Decoding of Code LMs with Static Analysis of Repository Context</td>
    </tr>
    <tr>
      <td>2023</td>
      <td>NeurIPS</td>
      <td>ResMem: Learn what you can and memorize the rest</td>
    </tr>
    <tr>
      <td>2023</td>
      <td>NeurIPS</td>
      <td>Accessing Higher Dimensions for UnsupervisedWord Translation</td>
    </tr>
    <tr>
      <td>2023</td>
      <td>NeurIPS</td>
      <td>Lift Yourself Up: Retrieval-augmented Text Generation with Self-Memory</td>
    </tr>
    <tr>
      <td>2023</td>
      <td>NeurIPS</td>
      <td>Exposing Attention Glitches with Flip-Flop Language Modeling</td>
    </tr>
    <tr>
      <td>2024</td>
      <td>Arxiv</td>
      <td>TransformerFAM: Feedback attention is working memory</td>
    </tr>
    <tr>
      <td>2024</td>
      <td>Arxiv</td>
      <td>Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention</td>
    </tr>
    <tr>
      <td>2017</td>
      <td>ICLR</td>
      <td>Learning to Remember Rare Events</td>
    </tr>
    <tr>
      <td>2019</td>
      <td>ICCV</td>
      <td>Memorizing Normality to Detect Anomaly: Memory-augmented Deep Autoencoder (MemAE) for Unsupervised Anomaly Detection</td>
    </tr>
    <tr>
      <td>2020</td>
      <td>ICLR</td>
      <td>Generalization through memorization: Nearest neighbor language models</td>
    </tr>
    <tr>
      <td>2023</td>
      <td>AAAI</td>
      <td>Memory-Augmented Theory of Mind Network</td>
    </tr>
  </tbody>
</table>

<h2 id="ìœ„-ë…¼ë¬¸ë“¤ì—-ëŒ€í•œ-ì „ë°˜ì ì¸-ìƒê°-ë°-ì•ìœ¼ë¡œ-ë°©í–¥ì„±">ìœ„ ë…¼ë¬¸ë“¤ì— ëŒ€í•œ ì „ë°˜ì ì¸ ìƒê° ë° ì•ìœ¼ë¡œ ë°©í–¥ì„±</h2>

<ol>
  <li>ëª¨ë¸ì— ë°ì´í„°ì˜ í‘œí˜„ì„ ëª¨ë‘ ì €ì¥í•˜ëŠ” ê²ƒë³´ë‹¤ ë°ì´í„°ì— í‘œí˜„ì„ ì™¸ë¶€ì— ë‘ê³  í™œìš©í•˜ëŠ” ë°©ì‹ì´ ëŒ€ë‘ë˜ê³  ìˆìŒ. <br> (ResMem, MemAE, Generalization through memorization, Memory-Augmented Theory of Mind Network )</li>
  <li>Transformerì— augmentationí•˜ëŠ” ë°©ì‹ì€ ë…ë¦½ì ì¸ logitì— ê²°í•©í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ì§„í–‰ë¨ (Monitor, kNN ê²°í•©ë“±)</li>
  <li>íŠ¸ëœìŠ¤í¬ë¨¸ ê³„ì—´ì—ì„œ Attentionì˜ ë¯¿ìŒì´ ì•½í•´ì§€ê³  RNN ê³„ì—´ì´ ê°•í•´ì§€ê³  ìˆìŒ. (Attention Glitch, TransformerFAM, Infini-attention)</li>
</ol>

<ul>
  <li>ë‚´ê°€ ì—°êµ¬í•œ ê²ƒë„ ë¬¸ì„œì— ëŒ€í•œ ì •ë³´ë¥¼ ì €ì¥í•˜ê¸° ìœ„í•´ì„œ ë§ˆì§€ë§‰ ë ˆì´ì–´ë¥¼ êµì²´ í•˜ì˜€ëŠ”ë°, ì´ëŠ” ìµœê·¼ ì—°êµ¬ë˜ëŠ” ë§ˆì§€ë§‰ ë ˆì´ì–´ì˜ í‘œí˜„ì„ ë³€ê²½í•˜ëŠ” ê²ƒê³¼ ìœ ì‚¬í•˜ë‹¤.</li>
  <li>ë§ˆì§€ë§‰ ë ˆì´ì–´ì—ë§Œ í‘œí˜„ì„ augmentation í•˜ëŠ” ë¶€ë¶„ì€ logitì— ê°€ê¹Œìš´ ë¶€ë¶„ë§Œ ìˆ˜ì •í•˜ê¸° ë•Œë¬¸ì— ì¤‘ê°„ ë ˆì´ì–´ë¥¼ ë°”ê¿”ì•¼ ì¢€ë” ë§ì€ ê²ƒë“¤ì„ ë°”ê¿€ ìˆ˜ ìˆë‹¤. ê·¸ëŸ¬ë‚˜ ì œëŒ€ë¡œ ì•Œì§€ë„ ëª»í•˜ëŠ” ì¤‘ê°„ ë ˆì´ì–´ ê°’ì„ ìˆ˜ì •í•˜ëŠ” ê²ƒì€ ì„¤ëª…/í•´ì„ì˜ ê²°í•¨ì„ ì§€ë‹ ê°€ëŠ¥ì„±ì´ ë†’ë‹¤.</li>
  <li>íŠ¸ëœìŠ¤í¬ë¨¸ì—ì„œ RNN ê³„ì—´ì„ ë„ì…í•˜ëŠ” ê²½ìš°, ë”ìš± íš¨ìœ¨ì ìœ¼ë¡œ long-term ì˜ˆì¸¡ì„ ì§„í–‰í•  ìˆ˜ ìˆë‹¤. ê²°êµ­ â€œattention is all you needâ€ë¡œ ë§ì€ ë¶€ë¶„ì´ ë°”ê¼ˆì§€ë§Œ, ìŠ¬ìŠ¬ í•œê³„ë¥¼ ë³´ì´ê³  ìˆìœ¼ë©°, convolution (Mamba) í˜¹ì€ RNN (TransformerFAM)ê³¼ ê°™ì€ ì—°êµ¬ë“¤ì´ ì§„í–‰ë˜ëŠ” ê²ƒì´ë‹¤. ì´ëŸ¬í•œ ëª¨ë¸ë§ì„ ê²°í•©í•˜ë©´ ê²°í•¨ì´ ì—†ëŠ” ëª¨ë¸ë§ì´ ê°€ëŠ¥í•  ê²ƒìœ¼ë¡œ ë³´ì¸ë‹¤. Memorizing Transformer, ReadAgent ì™€ ê°™ì€ ì—°êµ¬ë“¤ë„ long-contextë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ì„œ ì—°êµ¬ë˜ì—ˆë‹¤. ë§ˆì°¬ê°€ì§€ë¡œ ìœ„ ì—°êµ¬ë“¤ë„ soft-attentionì´ ì§€ë‹ˆëŠ” ë„ˆë¬´ softí•¨ì„ í•´ê²°í•˜ë ¤ê³  ë…¸ë ¥í•˜ëŠ” ê²ƒì´ë‹¤. ë‹¤ìŒ Transformer êµ¬ì¡°ëŠ” RNNì„ ê²°í•©í•œ ëª¨ë¸ë§ì´ ë  ê²ƒì´ë¼ê³  ë¯¿ì–´ ì˜ì‹¬ì¹˜ ì•ŠëŠ”ë‹¤.</li>
</ul>

<hr>

<h2 id="1-neural-priming-for-sample-efficient-adaptation">1. Neural Priming for Sample-Efficient Adaptation</h2>

<p>ì£¼ìš” ë©”ì†Œë“œ</p>
<ol>
  <li>íŠ¹ì • ë ˆì´ë¸”ì„ ê°€ì§€ê³  ìˆëŠ” ë°ì´í„°ë¥¼ ë¬¶ì–´ì„œ í´ëŸ¬ìŠ¤í„°ë¥¼ í˜•ì„±í•œë‹¤.</li>
  <li>ì˜ˆì¸¡í•  ë•Œ, í•´ë‹¹ í´ëŸ¬ìŠ¤í„°ë¡œ í˜•ì„±ëœ ì„ë² ë”© ë²¡í„°ë¥¼ í™œìš©í•œë‹¤.</li>
</ol>

<p>ì´ ì—°êµ¬ëŠ” Pretraining ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ ìœ ì‚¬í•œ ì´ë¯¸ì§€ ì„¤ëª…ì„ ê°€ì ¸ì™€ ì˜ˆì¸¡ì— í™œìš©í•˜ëŠ” Neural Priming Poolì„ ì œì•ˆí•˜ì˜€ë‹¤. 
Pretraining ì‹œ ì‚¬ìš©ëœ í•™ìŠµ ë°ì´í„°ëŠ” í›„ì† ì˜ˆì¸¡ì— ì„ë² ë”©ì„ ì œê³µí•˜ëŠ” ë° ì‚¬ìš©í•œë‹¤. 
ì´ëŸ¬í•œ ë°©ë²•ì€ ê¸°ê³„ ë²ˆì—­ì—ì„œ ì‚¬ìš©ë˜ëŠ” kNN ê²€ìƒ‰ ê¸°ë°˜ ì˜ˆì¸¡ì— Pretraining ë°ì´í„°ë¥¼ í™œìš©í•˜ëŠ” ê²ƒì´ë‹¤.</p>

<p>ì´ëŠ” ëª¨ë¸ì´ ë°ì´í„°ì— ëŒ€í•œ í•™ìŠµìœ¼ë¡œ ì¸í•œ í¸í–¥ì„ í™œìš©í•˜ëŠ” ë° ê´€ë ¨ì´ ìˆì–´ë³´ì¸ë‹¤.</p>

<p>(ì´ì „ì— ê¸°ì˜ë‹˜ì´ ëª¨ë¸ì´ ê°€ì •í•œ ë‚´ìš©ê³¼ ì‹¤ì œ ë ˆì´ë¸” ê°„ì˜ ì…ë ¥ ê¸°ì—¬ê°€ ë‹¤ë¥´ë‹¤ëŠ” ê²ƒì²˜ëŸ¼, Pretraining ë°ì´í„°ì˜ ì„ë² ë”© í™œìš©ê³¼ ê´€ë ¨ì´ ìˆìŒ.)</p>

<h2 id="2-monitor-guided-decoding-of-code-lms-with-static-analysis-of-repository-context">2. Monitor-Guided Decoding of Code LMs with Static Analysis of Repository Context</h2>

<p>í•´ë‹¹ ë°©ì‹ì€ ì½”ë“œ ìƒì„± ì–¸ì–´ ëª¨ë¸(Code generation LLM)ì—ì„œ ë§ˆì§€ë§‰ ë¡œì§“ì„ ê°€ë¦¬ëŠ” ë§ˆìŠ¤í¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìƒì„±ì„ ì•ˆë‚´í•˜ëŠ” ë°©ì‹ì´ë‹¤. 
ì´ëŠ” ì›Œí„°ë§ˆí¬ì—ì„œ ë‹¤ìŒ ë‹¨ì–´ì— ëŒ€í•œ ë¶„í¬ë¥¼ ìˆ˜ì •í•˜ì—¬ ì¶”í›„ ì˜ˆì¸¡ì— ì‚¬ìš©í•œ ê²ƒì²˜ëŸ¼ í•´ë‹¹ ì—°êµ¬ë„ ìœ íš¨í•œ ì½”ë“œë¥¼ ìƒì„±í•˜ê¸° ìœ„í•´ ìƒ˜í”Œë§í•˜ëŠ” ë‹¨ì–´ë¥¼ ì œì•½í•˜ëŠ” ê²ƒì´ë‹¤.</p>

<h2 id="3-resmem-learn-what-you-can-and-memorize-the-rest">3. ResMem: Learn what you can and memorize the rest</h2>

<ul>
  <li>í•´ë‹¹ ë°©ì‹ì€ LLMì—ì„œ KNNìœ¼ë¡œ ì„±ëŠ¥ì„ ì˜¬ë¦¬ëŠ” ë¶€ë¶„ì´ ë¹„ìŠ·í•˜ë‹¤.</li>
  <li>ì°¨ì´ì ì€ prediction taskì˜ ë‹¨ì¼ ëª¨ë¸ì— ëŒ€í•´ì„œ residualì„ ëª¨ë‘ ì•”ê¸°í•´ë†¨ë‹¤ëŠ” ì ì´ë‹¤. ë˜í•œ ì´ë¡ ì ìœ¼ë¡œ riskì— ëŒ€í•œ boundë¥¼ ê³„ì‚°í•  ìˆ˜ ìˆë‹¤.</li>
</ul>

<p>Base ëª¨ë¸ì„ í•™ìŠµí•œ í›„ì—ëŠ” ì¶”ê°€ì ì¸ ëª¨ë“ˆì„ Residualì— ì•”ê¸°ì‹œí‚¨ë‹¤. 
ê·¸ëŸ° ë‹¤ìŒ ì˜ˆì¸¡í•  ë•Œ kNNì„ ì‚¬ìš©í•˜ì—¬ ì¶”ê°€ì ì¸ ìƒ˜í”Œì„ ì°¾ì•„ Residual ë¶€ë¶„ì„ ë³´ì™„í•œë‹¤.</p>

<h2 id="4-accessing-higher-dimensions-for-unsupervisedword-translation">4. Accessing Higher Dimensions for UnsupervisedWord Translation</h2>

<p>Cooccurrenceë¡œ ë²¡í„°ë¥¼ ë§Œë“¤ì–´ì„œ íš¨ìœ¨ì ì¸ translationì„ ê°€ëŠ¥í•˜ê²Œ ë§Œë“¤ì—ˆë‹¤. (Coocmap)</p>

<h2 id="5-lift-yourself-up-retrieval-augmented-text-generation-with-self-memory">5. Lift Yourself Up: Retrieval-augmented Text Generation with Self-Memory</h2>

<p>Interactiveí•œ self-play ë°©ì‹ì˜ ë©”ëª¨ë¦¬ ì‚¬ìš©ì„ ì œì•ˆí•¨.</p>

<ul>
  <li>primal problem: better memory prompts better generation (ë©”ëª¨ë¦¬ ìœ ì‚¬ë„ê°€ ë†’ì„ìˆ˜ë¡ BLEU scoreê°€ ë†’ì•˜ìŒ)</li>
  <li>dual problem: better generation also prompts better memory (fixed memoryëŒ€ì‹ ì— í•„ìš”í•œ ë©”ëª¨ë¦¬ë¥¼ ìƒì„±í•˜ë©´ ë” ì¢‹ì€ ë©”ëª¨ë¦¬ë¥¼ ë§Œë“¤ ìˆ˜ ìˆìŒ.)</li>
</ul>

<p>ë” ì¢‹ì€ ë©”ëª¨ë¦¬ë¥¼ ë½‘ë„ë¡ í•™ìŠµí•œë‹¤. (Target distributionì— ëŒ€í•´ì„œ KL divergenceë¥¼ ìµœì†Œí™”)
ë” ì¢‹ì€ ë©”ëª¨ë¦¬ë¥¼ ìƒì„±í•˜ë„ë¡ í•™ìŠµí•œë‹¤.</p>

<h2 id="6-exposing-attention-glitches-with-flip-flop-language-modeling">6. Exposing Attention Glitches with Flip-Flop Language Modeling</h2>

<p>ë¬¸ì œì— ëŒ€í•œ ì´í•´ê°€ ë¶€ì¡±í•œ ê²ƒ ê°™ë‹¤. 
ì†ŒìŠ¤ì™€ íƒ€ê²Ÿì´ ì£¼ì–´ì¡Œì„ ë•Œ ê° ë‹¨ì–´ì˜ ê³µìƒ(cooccurrence)ì„ ì¸¡ì •í•˜ëŠ” ê²ƒì€ ì´í•´ë©ë‹ˆë‹¤. ì„ íƒëœ ë‹¨ì–´ë“¤ ê°„ì˜ ìµœëŒ€í•œ ì¼ì¹˜(matching)ë¥¼ í•˜ëŠ” ê²ƒìœ¼ë¡œ ë³´ì…ë‹ˆë‹¤.</p>
<ul>
  <li>Vecmapê³¼ ì°¨ì´: coocurrenceëŠ” ë°ì´í„°ì˜ í‘œí˜„ì´ ë‹¤ë¥¸ ë‹¨ì–´ë“¤ë¡œ ì´ë£¨ì–´ì§„ë‹¤. ë‹¨ì–´ì™€ ê°™ì´ ë‚˜ì˜¤ëŠ” ê²ƒìœ¼ë¡œ í‘œí˜„í•˜ì˜€ìŒ.</li>
</ul>

<h2 id="7-transformerfam-feedback-attention-is-working-memory">7. TransformerFAM: Feedback attention is working memory</h2>

<p>ë©”ëª¨ë¼ì´ì œì´ì…˜ì„ ìœ„í•œ í† í°ì„ ì‚¬ìš©í•˜ì—¬ long-range token processingì„ ê°€ëŠ¥í•˜ê²Œ ë§Œë“¤ì—ˆë‹¤. 
ê°’ì„ ëª…ì‹œì ìœ¼ë¡œ ì „ë‹¬í•´ì£¼ëŠ” ê²ƒ ë¿ë§Œì•„ë‹ˆë¼ ã…ˆë†’ì€ ë ˆì´ì–´ì— ìˆëŠ” ê°’ì„ ì•„ë˜ë¡œ ë‚´ë¦´ ìˆ˜ ìˆê¸° ë•Œë¬¸ì— 
Transformerê¸°ë°˜ ëª¨ë¸ì´ íŠ¹ì • ëŒ€ìƒì„ ë”ìš± ì˜¤ë«ë™ì•ˆ ìƒê°í•˜ë„ë¡ ë§Œë“œëŠ” ê²ƒ ê°™ë‹¤.</p>

<h2 id="8-leave-no-context-behind-efficient-infinite-context-transformers-with-infini-attention">8. Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention</h2>

<p>ì¤‘ê°„ì— RNNê¸°ë°˜ì˜ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ í‘œí˜„ì´ ì „ë‹¬ë˜ë„ë¡ ë§Œë“¤ì—ˆë‹¤. 
ìš”ì¦˜ íŠ¸ë Œë“œëŠ” íŠ¸ëœìŠ¤í¬ë¨¸ ê¸°ë°˜ ëª¨ë¸ì´ recurrentí•œ íŠ¹ì„±ì„ ì§€ë‹ˆëŠ” hidden stateë¥¼ ë§Œë“¤ë„ë¡ ì—°êµ¬ë˜ëŠ” ê²ƒ ê°™ë‹¤.</p>

<h2 id="9-learning-to-remember-rare-events">9. Learning to Remember Rare Events</h2>

<p>Rare eventë¥¼ ì‚¬ìš©í•˜ê³  í™•ì¸í•˜ëŠ” ê²ƒì€ ì‰½ì§€ ì•Šë‹¤. íŠ¹íˆë‚˜ neural networkëŠ” long-rangeë‚˜ ê¸´ ë°ì´í„°ì— ëŒ€í•´ì„œ ì‚¬ìš©í•˜ê¸° ì‰½ì§€ ì•Šë‹¤. ë©”ëª¨ë¦¬ë¥¼ ì—…ë°ì´íŠ¸ í•˜ëŠ” ë¶€ë¶„ì— ëŒ€í•´ì„œ ì¶”ê°€ì ìœ¼ë¡œ ê³µë¶€í•´ì•¼ í•œë‹¤.</p>

<h2 id="10--memorizing-normality-to-detect-anomaly-memory-augmented-deep-autoencoder-memae-for-unsupervised-anomaly-detection">10.  Memorizing Normality to Detect Anomaly: Memory-augmented Deep Autoencoder (MemAE) for Unsupervised Anomaly Detection</h2>

<p>í•´ë‹¹ ë°©ì‹ì€ Code generation LLMì— ëŒ€í•´ì„œ ë§ˆì§€ë§‰ Logitì„ ê°€ë¦´ maskë¥¼ ê¸°ë°˜ìœ¼ë¡œ generationì„ guide í•˜ëŠ” ë°©ì‹ì´ë‹¤ watermarkì—ì„œ ë‹¤ìŒ ë‹¨ì–´ì— ëŒ€í•œ ë¶„í¬ë¥¼ ìˆ˜ì •í•˜ì—¬ ì¶”í›„ ì˜ˆì¸¡ì„ ìœ„í•´ì„œ ì‚¬ìš©í–ˆë˜ ê²ƒì²˜ëŸ¼ í•´ë‹¹ ì—°êµ¬ë„ Valid codeë¥¼ ìƒì„±í•˜ê¸° ìœ„í•´ì„œ ìƒ˜í”Œí•˜ëŠ” ë‹¨ì–´ë¥¼ ì œì•½í•˜ëŠ” ê²ƒì´ë‹¤.</p>

<h2 id="neural-priming-for-sample-efficient-adaptation">Neural Priming for Sample-Efficient Adaptation</h2>

<p>Pretraining ë°ì´í„°ì— ëŒ€í•´ì„œ ìœ ì‚¬í•œ ì´ë¯¸ì§€ descriptionì„ ê°€ì ¸ì™€ì„œ ì˜ˆì¸¡ì— í™œìš©í•˜ëŠ” neural priming poolì„ ì œì•ˆí•˜ì˜€ë‹¤. 
pretraining ë‹¹ì‹œ ì‚¬ìš©í–ˆë˜ í•™ìŠµ ë°ì´í„°ëŠ” ì´í›„ ì˜ˆì¸¡ì„ í•˜ëŠ”ë° ìˆì–´ì„œ ì„ë² ë”©ì„ í™œìš©í•œë‹¤. ì´ëŸ¬í•œ ë°©ì‹ì€ MTì—ì„œ í™œìš©í•˜ëŠ” kNN searchê¸°ë°˜ ì˜ˆì¸¡ì— ëŒ€í•´ì„œ 
pretraining ë°ì´í„°ë¥¼ í™œìš©í•˜ëŠ” ë°©ì‹ì´ë‹¤. ì´ëŠ” ëª¨ë¸ì´ ë°ì´í„°ì— ëŒ€í•´ì„œ í•™ìŠµí•˜ë©° ìƒê¸´ biasë¥¼ í™œìš©í•˜ëŠ” ê²ƒê³¼ë„ ì—°ê´€ë˜ì–´ ìˆë‹¤. (ì´ì „ì— ê¸°ì˜ë‹˜ì´ ëª¨ë¸ì´ ìƒê°í•˜ëŠ” ê²ƒê³¼ ì‹¤ì œ ë ˆì´ë¸”ì—  ëŒ€í•´ì„œ input attributionì´ ë‹¤ë¥´ë‹¤ëŠ” ê²ƒì²˜ëŸ¼, pretraining ë°ì´í„°ì— ëŒ€í•œ ì„ë² ë”© í™œìš©)</p>

<ol>
  <li>íŠ¹ì • ë ˆì´ë¸”ì„ ê°€ì§€ê³  ìˆëŠ” ë°ì´í„°ë¥¼ ë¬¶ì–´ì„œ í´ëŸ¬ìŠ¤í„°ë¥¼ í˜•ì„±í•œë‹¤.</li>
  <li>ì˜ˆì¸¡í•  ë•Œ, í•´ë‹¹ í´ëŸ¬ìŠ¤í„°ë¡œ í˜•ì„±ëœ ì„ë² ë”© ë²¡í„°ë¥¼ í™œìš©í•œë‹¤.</li>
</ol>

<p>Autoencoderì— ëŒ€í•´ì„œ ì„ë² ë”©ì„ ìƒì„±í•˜ê³  key-valueë¡œë¶€í„° ê°’ì„ ì„ íƒí•˜ì—¬ reconstructioní•˜ëŠ” ë¶€ë¶„ì„ ì œì•ˆí•˜ì˜€ë‹¤. 
ì œì•ˆí•œ ë°©ì‹ì€ keyì— ëŒ€í•œ weightì„ ì„¤ì •í•˜ëŠ” ë¶€ë¶„ì—ì„œ hard shrinking ì„ ì‚¬ìš©í•˜ì—¬ sparsityë¥¼ ê°•ì œí•˜ì˜€ë‹¤.</p>

\[\hat{w} = h(w_i;\lambda) = \begin{cases} w_i &amp; \text{if} ~ w_i &gt; \lambda \\ 0 &amp; \text{othderwise} \end{cases}\]

<p>ì´ ì‹ì€ ReLUì— ì˜í•´ì„œ ë‹¤ì‹œ ì“°ì—¬ì§ˆ ìˆ˜ ìˆë‹¤. 
\(\hat{w} = \frac{\text{max}(w_i - \lambda, 0 ) * w_i}{|w_i - \lambda| + \epsilon}\)</p>

<p>ì´ ë°©ì‹ì€ ëª…ì‹œì ìœ¼ë¡œ ë©”ëª¨ë¦¬ì˜ ì—”íŠ¸ë¦¬ë¥¼ ì„ íƒí•˜ê±°ë‚˜, ì„ íƒí•˜ì§€ ì•ŠëŠ” ë°©ì‹ì„ ì œê³µí•œë‹¤. 
weight ì— ëŒ€í•œ magnitudeë¥¼ ê°ì†Œì‹œí‚¤ê¸° ë³´ë‹¤ entropyë¥¼ ê°ì†Œì‹œí‚¤ëŠ” ë°©í–¥ìœ¼ë¡œ ì§„í–‰ë„ìˆë‹¤.</p>

<h2 id="11-generalization-through-memorization-nearest-neighbor-language-models">11. Generalization Through Memorization: Nearest Neighbor Language Models</h2>

<p>ë§ˆì§€ë§‰ ë ˆì´ì–´ì— ëŒ€í•´ì„œ ê·¸ í‘œí˜„ ê°’ì„ KNN neighbor searchí•˜ì—¬ ê°’ì„ ê°€ì ¸ì˜¤ëŠ” ë°©ì‹ì„ íƒí•˜ì˜€ë‹¤. ê·¸ë¦¬ê³  ë§ˆì§€ë§‰ í™•ë¥ ê°’ì„ LLMê³¼ search í•œ ê²ƒì„ ì„ì—ˆë‹¤. 
Wikitextì— ëŒ€í•´ì„œëŠ” KNN search í•œ ë¶€ë¶„ì„ ë” ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ($\lambda =0.5$) ì±…ì— ëŒ€í•´ì„œëŠ” KNN ë¶€ë¶„ì„ ëœ ì“°ê³  LLMì˜ ì•„ì›ƒí’‹ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ($\lambda=0.2$)ê°€ ë” ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì˜€ë‹¤. ì´ëŠ” Wikitextì˜ ê²½ìš° ì¢€ë” extractive í•œ ì„±ì§ˆì„ ê°€ì§€ê³  ìˆê¸° ë•Œë¬¸ì´ë‹¤.</p>

<p>ë§ˆì§€ë§‰ ë ˆì´ì–´ì— ëŒ€í•´ì„œ ê·¸ í‘œí˜„ ê°’ì„ KNN neighbor searchí•˜ì—¬ ê°’ì„ ê°€ì ¸ì˜¤ëŠ” ë°©ì‹ì„ íƒí•˜ì˜€ë‹¤. ê·¸ë¦¬ê³  ë§ˆì§€ë§‰ í™•ë¥ ê°’ì„ LLMê³¼ search í•œ ê²ƒì„ ì„ì—ˆë‹¤. 
Wikitextì— ëŒ€í•´ì„œëŠ” KNN search í•œ ë¶€ë¶„ì„ ë” ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ($\lambda =0.5$) ì±…ì— ëŒ€í•´ì„œëŠ” KNN ë¶€ë¶„ì„ ëœ ì“°ê³  LLMì˜ ì•„ì›ƒí’‹ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ($\lambda=0.2$)ê°€ ë” ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì˜€ë‹¤. ì´ëŠ” Wikitextì˜ ê²½ìš° ì¢€ë” extractive í•œ ì„±ì§ˆì„ ê°€ì§€ê³  ìˆê¸° ë•Œë¬¸ì´ë‹¤.</p>

<ul>
  <li>
<em>Representation</em> â€“&gt; <em>Next Wordë¡œ ë§µí•‘</em>
\((\mathcal{K}, \mathcal{V}) = \{ (f(c_i), w_i) | (c_i, w_i) \in \mathcal{D} \}\)</li>
</ul>

<p>ê° ë‹¨ì–´ë§ˆë‹¤ í™•ë¥ ì„ ê³„ì‚°í•´ì„œ ì—…ë°ì´íŠ¸ í•´ì¤€ë‹¤. 
\(p_{KNN}(y|x) \propto \sum_{(k_i, v_i) \in \mathcal{N}} 1_{y=v_i} \exp{-d(k_i, f(x))}\)</p>

<p>ì¶”ê°€ì ìœ¼ë¡œKNN ìœ¼ë¡œë¶€í„° ë½‘ì€ ì§€ì‹ì— ëŒ€í•´ì„œ LMì˜ í‘œí˜„ê³¼ linear interpolationì„ í•˜ì˜€ë‹¤.</p>

\[p(y|x) = \lambda p_{knn}(y|x) + (1-\lambda)p_{LM}(y|x)\]

<ul>
  <li>fast nearest neighbor retrieval in high dimensional spaces. FAISS speeds up search by clustering the keys and looking up neighbors based on the cluster centroids, while reducing memory usage by storing compressed versions of the vectors.</li>
</ul>

<h2 id="12-memory-augmented-theory-of-mind-network">12. Memory-Augmented Theory of Mind Network</h2>

<p><img src="https://onedrive.live.com/embed?resid=AE042A624064F8CA%212445&amp;authkey=%21ANjtEw50WYZZvNg&amp;width=660" width="660" height="auto"></p>

<p>ì´ ì—°êµ¬ì—ì„œ ë©”ëª¨ë¦¬ë¥¼ ì‚¬ìš©í•˜ëŠ” ë°©ì‹ì€ trajectoryì— ëŒ€í•´ì„œ ê³¼ê±° ì •ë³´ë¥¼ key-valueë¡œ ì €ì¥í•˜ê³  í˜„ì¬ ìƒíƒœì—ì„œ ì •ë³´ë¥¼ ë½‘ì•„ë‚´ëŠ” ë¶€ë¶„ì„ êµ¬í˜„í•˜ì˜€ë‹¤.</p>
<ol>
  <li>í˜„ì¬ trajectoryê°€ ì£¼ì–´ì§€ê³ , í˜„ì¬ ìƒíƒœì—ì„œ trajectoryì—ì„œ cosine ìœ ì‚¬ë„ ê¸°ë°˜ìœ¼ë¡œ M ê°œì˜ queryë¥¼ ìƒì„±í•œë‹¤.  ($q_m, m=1,2,\cdots, M$)</li>
  <li>ê³¼ê±° ì •ë³´ëŠ” key-value memoryì˜ í˜•íƒœë¡œ ì €ì¥ë˜ì–´ ìˆê³ , ê° $q_m$ë§ˆë‹¤ memoryë¥¼ retrievalí•œë‹¤.</li>
</ol>

\[\bar{v}_m = \sum_{v_j^t \in \mathcal{M}.value} \operatorname{attn}(q_m, k_j^t) v_j^t\]

<ol>
  <li>ì¿¼ë¦¬ë§ˆë‹¤ ìƒì„±ëœ valueë“¤ì„ ë‹¤ì‹œ attentionì„ ê¸°ë°˜ìœ¼ë¡œ ì„ëŠ”ë‹¤.</li>
</ol>

<p>ì´ëŸ¬í•œ ë°©ì‹ì€ trajectoryê°€ ì—¬ëŸ¬ state ë“¤ë¡œ êµ¬ì„±ë˜ì–´ìˆê¸°ì— hierarchicalí•˜ê²Œ ë©”ëª¨ë¦¬ë¥¼ ë½‘ëŠ” êµ¬ì¡°ì´ë‹¤. ì´ ê³¼ì •ì—ì„œ attentionìœ¼ë¡œ ì •ë³´ë¥¼ ì„ëŠ” ê²ƒì€ ë‘ ë²ˆ ë‚˜íƒ€ë‚œë‹¤. 
key-valueë¥¼ ìƒì„±í•˜ëŠ” ë°©ì‹ì€ LSTMì˜ hidden stateë¡œë¶€í„° forward í•¨ìˆ˜ë¥¼ ì´ìš©í•´ì„œ ì—°ì‚°í•œ ê²ƒì´ë‹¤.</p>


      </d-article>

      <d-appendix>
        <d-footnote-list></d-footnote-list>
        <d-citation-list></d-citation-list>
      </d-appendix>

<!--<div id="disqus_thread" style="max-width: 1000px; margin: 0 auto;">
    <script type="text/javascript">
        var disqus_shortname  = 'al-folio';
        var disqus_identifier = '/side_articles/240426_nerual_memory';
        var disqus_title      = "Literature Reviews: Neural Memory, kNN augmentation and some other recent trends.";
        (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        })();
    </script>
    <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</div>
    </div>
 -->
      <hr>
<div id="giscus_thread" style="max-width: 1000px; margin: 0 auto;">
    <script>
      let giscusTheme = localStorage.getItem("theme");
      let giscusAttributes = {
          "src": "https://giscus.app/client.js",
          "data-repo": "fxnnxc/fxnnxc",
          "data-repo-id": "MDEwOlJlcG9zaXRvcnkzMjQ2NjUxMTU=",
          "data-category": "Q&A",
          "data-category-id": "DIC_kwDOE1n_G84CYOk_",
          "data-mapping": "title",
          "data-strict": "0",
          "data-reactions-enabled": "1",
          "data-emit-metadata": "0",
          "data-input-position": "top",
          "data-theme": giscusTheme,
          "data-lang": "en",
          "crossorigin": "anonymous",
          "async": "",
      };
  
  
      let giscusScript = document.createElement("script");
      Object.entries(giscusAttributes).forEach(([key, value]) => giscusScript.setAttribute(key, value));
      document.getElementById("giscus_thread").appendChild(giscusScript);
    </script>
    <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" target="_blank" rel="noopener noreferrer">comments powered by giscus.</a>
</noscript>
  </div>
<!-- Footer -->    <footer class="sticky-bottom mt-5" style="border: none;border-top:0px">
      <div class="container" style="text-align: center; ">
        Â© Copyright 2024 Bumjini  . Powered by Jekyll with al-folio theme. Hosted by GitHub Pages.

      </div>
    </footer>
    
    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.2/dist/umd/popper.min.js" integrity="sha256-l/1pMF/+J4TThfgARS6KwWrk/egwuVvhRzfLAMQ6Ds4=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.min.js" integrity="sha256-SyTu6CwrfOhaznYZPoolVw2rxoY7lKYKQvqbtqN93HI=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    
    

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
  /*
   * This JavaScript code has been adapted from the article 
   * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar, 
   * published on the website https://css-tricks.com on the 7th of May, 2014.
   * Couple of changes were made to the original code to make it compatible 
   * with the `al-foio` theme.
   */
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    let navbarHeight = $("#navbar").outerHeight(true);
    $("body").css({ "padding-top": 0 });
    $("progress-container").css({ "padding-top": 0 });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  /*
   * The offset between the html document height and the browser viewport
   * height will be greater than zero if vertical scroll is possible.
   * This is the distance the user can scroll
   */
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>
  </div></body>
  
  <d-bibliography src="/assets/bibliography/all.bib">
  </d-bibliography>
  <script src="/assets/js/distillpub/overrides.js"></script>

  <center>
    <a  href="">
    <img src="/assets/common/KAIST-hi.gif" width="40px" style="margin-right:0px; padding-bottom: 3px;">
    </a>
  </center>
  <hr> 

  </html>
