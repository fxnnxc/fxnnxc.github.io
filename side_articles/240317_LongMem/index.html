<!DOCTYPE html>
<!-- _layouts/distill.html -->
<html>
    
<head>    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Bumjini | LongMem 논문 리뷰 (NeurIPS 2023)</title>
    <meta name="author" content="Bumjini  " />
    <meta name="description" content="이 포스팅은 Augmenting Language Models with Long-Term Memory 논문에 대한 공부입니다." />
    <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website" />


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous" />

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light" />

    <!-- Styles -->
    
    <link rel="shortcut icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>🪴</text></svg>">
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://fxnnxc.github.io/side_articles/240317_LongMem/">
    
    <!-- Dark Mode -->
    

  <!-- jQuery -->
<script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>
  <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams',
        inlineMath: [['$','$'], ['\\(','\\)']]
      },
      chtml: {
          scale: 1.0,
          minScale: .6,  
          mtextFontInherit: true,
          mtextInheritFont: true,
          merrorInheritFont: true,
        },
        svg: {
          scale: 1.2
        }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

  <!-- Distill js -->
  <script src="/assets/js/distillpub/template.v2.js"></script>
  <script src="/assets/js/distillpub/transforms.v2.js"></script>
  <script src="/assets/js/distillpub/overrides.js"></script>
  <!-- Page/Post style -->
  
</head>

  <d-front-matter>
    <script async type="text/json">{
      "title": "LongMem 논문 리뷰 (NeurIPS 2023)",
      "description": "이 포스팅은 Augmenting Language Models with Long-Term Memory 논문에 대한 공부입니다.",
      "published": "March 17, 2024",
      "authors": [
        {
          "author": "Bumjin Park",
          "authorURL": "",
          "affiliations": [
            {
              "name": "KAIST",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script>
  </d-front-matter>

  <body style="padding-top:0px" class="sticky-bottom-footer"> 
    
    <!-- Header --><header>
      
      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm sticky-top">
        <div class="container">
          <img>
          <!-- <img src="/assets/common/KAIST-hi.gif" width="40px" style="margin-right:0px; padding-bottom: 3px;"> -->
          
          <a class="navbar-brand title font-weight-lighter" href="https://fxnnxc.github.io/" style="margin-left:20px ;">
              <!--Bumjini-->
           <span class="font-weight-bold" style="font-size:larger;font-family:Times New Roman;">Bumjini    </span>
        </a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <!-- <li style="font-size: 17px" class="nav-item ">
                <a class="nav-link" href="/">  About Me</a>
              </li> -->
              
              <!-- Blog -->
              <li style="font-size: 17px" class="nav-item ">
                <a class="nav-link" href="/main_papers/"> Papers</a>
              </li>

              <li style="font-size: 17px" class="nav-item ">
                <a class="nav-link" href="/main_articles/"> Articles</a>
              </li>

              <li style="font-size: 17px" class="nav-item ">
                <a class="nav-link" href="/main_projects/"> Projects</a>
              </li>

              <!-- Other pages -->
              <li style="font-size: 17px" class="nav-item dropdown ">
                <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Others</a>
                <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown">
                  <a class="dropdown-item" href="/side_papers/">🗂️ side-paper</a>
                  <a class="dropdown-item" href="/side_articles/">🥕 side-articles</a>
                  <a class="dropdown-item" href="/book/">📚 book (korean)</a>
                </div>
              </li>
            </ul>
          </div>
        </div>
      </nav>
    </header>

    <!-- Content -->
    <div class="post distill">

      <d-title>
        <h1 style="font-size: 32px;">LongMem 논문 리뷰 (NeurIPS 2023)</h1>
        <p>이 포스팅은 Augmenting Language Models with Long-Term Memory 논문에 대한 공부입니다.</p>
      </d-title>

      <d-byline></d-byline>

      <d-article>
        <d-contents>
          <nav class="l-text figcaption" style="line-height: 15px;">
          <h3>Contents</h3>
            <div style="margin:0.0em; margin-bottom: 10px;margin-top: 10px;"><a style="margin:0.0em;" href="#introduction">Introduction</a></div>
            <ul style="margin:0.0em;margin-bottom: 5px; ">
              <li style="margin:0.0em; margin-bottom: 1px;"><a style="margin:0.0em;" href="#q-a">Q&amp;A</a></li>
              <li style="margin:0.0em; margin-bottom: 1px;"><a style="margin:0.0em;" href="#history">History</a></li>
              <li style="margin:0.0em; margin-bottom: 1px;"><a style="margin:0.0em;" href="#contribution">Contribution</a></li>
              
            </ul>
<div style="margin:0.0em; margin-bottom: 10px;margin-top: 10px;"><a style="margin:0.0em;" href="#methods">Methods</a></div>
            <ul style="margin:0.0em;margin-bottom: 5px; ">
              <li style="margin:0.0em; margin-bottom: 1px;"><a style="margin:0.0em;" href="#side-network">Side Network</a></li>
              <li style="margin:0.0em; margin-bottom: 1px;"><a style="margin:0.0em;" href="#cached-memory">Cached Memory</a></li>
              <li style="margin:0.0em; margin-bottom: 1px;"><a style="margin:0.0em;" href="#chunks">Chunks</a></li>
              
            </ul>
<div style="margin:0.0em; margin-bottom: 10px;margin-top: 10px;"><a style="margin:0.0em;" href="#experiments">Experiments</a></div>
            <ul style="margin:0.0em;margin-bottom: 5px; ">
              <li style="margin:0.0em; margin-bottom: 1px;"><a style="margin:0.0em;" href="#set-up">Set Up</a></li>
              <li style="margin:0.0em; margin-bottom: 1px;"><a style="margin:0.0em;" href="#long-context">Long Context</a></li>
              <li style="margin:0.0em; margin-bottom: 1px;"><a style="margin:0.0em;" href="#demonstrations">Demonstrations</a></li>
              
            </ul>
<div style="margin:0.0em; margin-bottom: 10px;margin-top: 10px;"><a style="margin:0.0em;" href="#notes">Notes</a></div>
            
          </nav>
        </d-contents>

        <p>이 글은 <a href="https://proceedings.neurips.cc/paper_files/paper/2023/hash/ebd82705f44793b6f9ade5a669d0f0bf-Abstract-Conference.html" target="_blank" rel="noopener noreferrer">Augmenting Language Models with Long-Term Memory
</a> <d-cite key="wang2024augmenting"> </d-cite> 논문에 대한 리뷰입니다.</p>

<h2 id="introduction"> 1. Introduction </h2>

<h3 id="q-a"> Pre-Questions and Answers  </h3>

<ol>
  <li>
<strong>Long-term 메모리는 어떻게 표현되었는가?</strong> : 캐시에 in-context를 저장하는 부분이 있으며, retrieval 을 위한 SideNet를 구현하였다. 메모리는 queue 형태로 오래된 메모리를 지운다. 메모리 사이즈가 충분히 크기 때문에 긴 정보를 저장할 수 있다.</li>
  <li>
<strong>Memory Retrieval Module의 입력과 출력은 무엇인가?</strong> prediction을 위한 query가 주어지면, 메모리에서 key-value에 대한 attended value combination을 반환한다. side-network 는 해당 정보를 sigmoid function으로 섞는다.</li>
  <li>
<strong>기존 Pretrain된 LLM을 finetuning하는 형태인가</strong>? 아니면 처음부터 학습하는 구조인가?  해당 연구는 pre-train된 상태를 활용하였고, GPT2모델 (407M)에 대해서 long-context를 학습하였다. 256 batch-size, 1024 sequence 길이를 활용하였다.</li>
</ol>

<h3 id="history"> Historical Notes </h3>

<p>논문에 언급되어 있는 이전 연구에 대한 주요 흐름과 이 논문에서 해결하는 문제는 다음과 같다.</p>

<ol>
  <li>GPT3는 GPT2에서 가진 1k 토큰 개수를 2k로 늘렸다. 늘어나는 토큰 개수는 더 넓은 in-context를 다룰 수 있게 만들었지만, long-range에 대한 근본적인 해결책은 될 수 없다.</li>
  <li>기존 MemTRM 모델은 non-differentiable 외부 메모리에 대한 retrieval 방식으로 효율성을 개선하였다. CPU에 메모리를 저장해두고 값을 가져와서 사용하는 방식이다.</li>
  <li>MemTRM에서는 memory staleness(메모리 진부)문제가 발생하였다. 이는 수많은 메모리가 모델이 업데이트 되는 과정에서 distributional shift와 같은 표현 차이로 비효율적인 표현공간이 형성되었다.</li>
</ol>

<h3 id="contribution"> Main Contribution of Paper </h3>

<p>논문에서는 두 가지 main task 가 있다.</p>
<ol>
  <li>책에 대한 모든 정보를 다루는 것 : PG22 데이터는 책에 대한 텍스트를 가지고 있다. 본 논문은 책의 내용을 모두 인지한 상태의 LLM을 목표로 한다.</li>
  <li>수천개의 task-relevant demonstration examples을 활용하는 것. ICL (in-context learning)는 demonstration으로부터 주어진 문제를 효율적으로 풀 수 있다. 본 논문에서 제안하는 방식 수많은 in-context example 을 메모리에 저장할 수 있다.</li>
</ol>

<h2 id="methods"> Methods  </h2>

<p>논문에서 제안한 방식을 이해하기 위해서는 3가지 모듈을 이해해야 한다.</p>

<ul>
  <li>Side Network</li>
  <li>Cached Memory Bank</li>
  <li>Tokens to a Chunk</li>
</ul>

<center>
<img src="https://onedrive.live.com/embed?resid=AE042A624064F8CA%211272&amp;authkey=%21ACjQzrD9FEbgMk4&amp;width=1024">
</center>

<h3 id="side-network"> Side Network  </h3>

<p>SideNetwork는 LLM의 residual representation과 이전 side network의 이전 레이어의 값을 섞는 방식으로 진행된다.</p>

\[\mathbf{H}^{l}_{\operatorname{Side}} 
= f_{\Theta^{l}_{\operatorname{Side}}}
\mathbf{H}_{\operatorname{Side}}^{l-1} + (\mathbf{H}^{2l}_{\operatorname{LLM}}
- \mathbf{H}^{2l-2}_{\operatorname{LLM}})\]

<p>Side Network는 LLM의 파라미터를 그대로 가져와서 메모리의 key-value를 섞기 위한 모듈이다. 논문에서는 섞는 과정을 학습하기 위해서 LLM을 weight를 그대로 가져와서 SideNet을 초기화하였다.</p>

<p>SideNet은 외부 메모리의 attention 연산으로부터 value를 가져온다. 기존 LLM의 in-context 정보와 메모리의 정보를 활용하기 위해서 <i> joint attention mechanism </i> 을 활용하였다.</p>

\[\begin{gather}
\mathbf{H}^l
 = \operatorname{sigmoid}(g) \cdot \mathbf{A} 
 + (1-\operatorname{sigmoid}(g)) \cdot \mathbf{M}  \\
 \mathbf{A} = \operatorname{softmax} (\frac{\mathbf{Q}\mathbf{K}^\top}{\sqrt{d}}) \mathbf{V}, 
 \mathbf{M} = \operatorname{Concat} \{ \operatorname{softmax}
 (\frac{\mathbf{Q}_i \tilde{\mathbf{K}}_i^{\top}}{\sqrt{d}}) \tilde{\mathbf{V}}_i 
 \}_{i=1}^{\vert x \vert}
 \end{gather}\]

<p>이 때 $\sim$이 붙은 key와 value는 메모리로부터 얻은 정보이다. $g$ 는 각 head별로 정의되는 학습하는 파라미터이다.</p>

<h3 id="cached-memory"> Cached Memory Bank  </h3>

<p>메모리는 $M$ 개수 key-value를 CPU에 저장하는 모듈이다. GPT에서 연산할 때, 발생하는 key, value 표현들을 메모리에 queue 형태로 넣는다. 논문에서 사용한 사이즈는 [8K, 16K, 32K, 65K]이다. 해당 메모리는 context정보를 저장하기 위해서 사용되므로, GPT의 내부 파라미터는 아니다.</p>

<h3 id="chunks"> Token-to-Chunk  </h3>

<p>외부 메모리는 많은 key를 가지고 있으므로 모든 key에 대해서 연산하는 것은 비효율적이다. 논문에서는 이를 개선하기 위해서 토큰을 chunk로 쪼개 attention을 계산하였고, top-K 개의 값을 가져왔다.</p>

<ol>
  <li>Chunk size $csz$를 정한다. (논문에서는 csz=4$)</li>
  <li>메모리 bank에 $M$개수의 key가 있다.</li>
  <li>연속된 key를 chunk로 묶는다. ($M/csz$ 개수 메모리)</li>
  <li>Attention을 계산하고 Top-$K/csz$ 개수의 키를 고른다.</li>
  <li>Chunk의 정보를 flatten 한다. ($K/csz * csz = K$ 개 메모리)</li>
</ol>

<h2 id="experiments"> Experiments  </h2>

<h3 id="set-up"> Set Up  </h3>

<ul>
  <li>Backbone LLM:  $L′ = 24,H = 16, d = 64$</li>
  <li>SideNet:  $L = 12, H = 16, d = 64$</li>
</ul>

<p>학습 세팅</p>
<ul>
  <li>전체 토큰 수: 26B</li>
  <li>global batch size: 256</li>
  <li>sequence length: 1024</li>
  <li>retrieved key: 64</li>
  <li>csz: 4</li>
  <li>memory-augmentation layer: 9-th layer of SideNet</li>
</ul>

<p>데이터 배치를 형성하는 방법은 길이가 비슷한 애들을 묶어서 고정된 길이로 만들고, batch index에 다른 길이의 문서를 넣는 방식을 사용하였다.</p>

<center>
<img src="https://onedrive.live.com/embed?resid=AE042A624064F8CA%211276&amp;authkey=%21AEnE6wW_nierJ6w&amp;width=1024">
</center>

<p>PG22에 대한 데이터 통계는 다음과 같다.</p>

<center>
<img src="https://onedrive.live.com/embed?resid=AE042A624064F8CA%211273&amp;authkey=%21ALIwMbAi5o3Jb-w&amp;width=1024%0A">
</center>

<h3 id="long-context"> Memorizing Long Context  </h3>

<p>메모리 사이즈를 고정하고 PG22데이터에 대한 Perplexity는 MemTRM과 TRIME 보다 더 낮은 것을 확인할 수 있다. 책의 정보를 암기하는데 있어서 이전 정보들을 같이 주는 것은 효율적이고, LONGMEM 방식의 외부 메모리 저장은 더 높은 성능을 보인 것이다.</p>

<center>
<img src="https://onedrive.live.com/embed?resid=AE042A624064F8CA%211277&amp;authkey=%21ADMroYpiZWdfPW4&amp;width=1024">
</center>

<h3 id="demonstrations"> Large Number of Demonstrations  </h3>

<p>Natural Language Understanding 문제에 대해서도 2000개의 demonstration을 주고 주어진 문제를 풀게 만들었다. GPT에 넣는 형태는 <code class="language-plaintext highlighter-rouge">di="Review: xi Sentiment: yi</code> 이다.</p>

<center>
<img src="https://onedrive.live.com/embed?resid=AE042A624064F8CA%211275&amp;authkey=%21AHZrLnUo8oZCXIs&amp;width=1024">
</center>

<p>메모리 사이즈가 반드시 크다고 좋은 것은 아니다. context의 사이즈가 작다면 적은 수의 메모리를 효율적으로 사용하는 것이 더 좋다. 아래 실험에서는 65K메모리에 대해서 상대적으로 적은 수의 메모리를 쓰는 경우 성능이 얼마나 향상되는지 확인하였다.</p>

<center>
<img src="https://onedrive.live.com/embed?resid=AE042A624064F8CA%211274&amp;authkey=%21ADPebJJw0BsIFtM&amp;width=1024">
</center>

<h2 id="notes"> Notes  </h2>

<p>주어진 context에 대해서 다음 단어를 예측하는 GPT의 쓰임이 늘어남에 따라서 더 길고 많은 정보들을 처리해야 한다. 메모리 기반으로 GPT의 내부 연산을 처리하는 방법은 더욱 정교하고 효율적인 계산을 위해서 필수적인 구조적 개선이다. NeurIPS 2022에 나온 memorizing transformer (MemTRM)과 NeurIPS 2023에 나온 이 연구처럼 앞으로도 더 많인 메모리 기반 모델링이 연구될 것 같다.</p>

      </d-article>

      <d-appendix>
        <d-footnote-list></d-footnote-list>
        <d-citation-list></d-citation-list>
      </d-appendix>

<!--<div id="disqus_thread" style="max-width: 1000px; margin: 0 auto;">
    <script type="text/javascript">
        var disqus_shortname  = 'al-folio';
        var disqus_identifier = '/side_articles/240317_LongMem';
        var disqus_title      = "LongMem 논문 리뷰 (NeurIPS 2023)";
        (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        })();
    </script>
    <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</div>
    </div>
 -->
      <hr>
<div id="giscus_thread" style="max-width: 1000px; margin: 0 auto;">
    <script>
      let giscusTheme = localStorage.getItem("theme");
      let giscusAttributes = {
          "src": "https://giscus.app/client.js",
          "data-repo": "fxnnxc/fxnnxc",
          "data-repo-id": "MDEwOlJlcG9zaXRvcnkzMjQ2NjUxMTU=",
          "data-category": "Q&A",
          "data-category-id": "DIC_kwDOE1n_G84CYOk_",
          "data-mapping": "title",
          "data-strict": "0",
          "data-reactions-enabled": "1",
          "data-emit-metadata": "0",
          "data-input-position": "top",
          "data-theme": giscusTheme,
          "data-lang": "en",
          "crossorigin": "anonymous",
          "async": "",
      };
  
  
      let giscusScript = document.createElement("script");
      Object.entries(giscusAttributes).forEach(([key, value]) => giscusScript.setAttribute(key, value));
      document.getElementById("giscus_thread").appendChild(giscusScript);
    </script>
    <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" target="_blank" rel="noopener noreferrer">comments powered by giscus.</a>
</noscript>
  </div>
<!-- Footer -->    <footer class="sticky-bottom mt-5" style="border: none;border-top:0px">
      <div class="container" style="text-align: center; ">
        © Copyright 2024 Bumjini  . Powered by Jekyll with al-folio theme. Hosted by GitHub Pages.

      </div>
    </footer>
    
    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.2/dist/umd/popper.min.js" integrity="sha256-l/1pMF/+J4TThfgARS6KwWrk/egwuVvhRzfLAMQ6Ds4=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.min.js" integrity="sha256-SyTu6CwrfOhaznYZPoolVw2rxoY7lKYKQvqbtqN93HI=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    
    

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
  /*
   * This JavaScript code has been adapted from the article 
   * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar, 
   * published on the website https://css-tricks.com on the 7th of May, 2014.
   * Couple of changes were made to the original code to make it compatible 
   * with the `al-foio` theme.
   */
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    let navbarHeight = $("#navbar").outerHeight(true);
    $("body").css({ "padding-top": 0 });
    $("progress-container").css({ "padding-top": 0 });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  /*
   * The offset between the html document height and the browser viewport
   * height will be greater than zero if vertical scroll is possible.
   * This is the distance the user can scroll
   */
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>
  </div></body>
  
  <d-bibliography src="/assets/bibliography/all.bib">
  </d-bibliography>
  <script src="/assets/js/distillpub/overrides.js"></script>

  <center>
    <a  href="">
    <img src="/assets/common/KAIST-hi.gif" width="40px" style="margin-right:0px; padding-bottom: 3px;">
    </a>
  </center>
  <hr> 

  </html>
