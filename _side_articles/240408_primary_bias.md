---
layout: distill
authors: 
    - name: Bumjin Park
      affiliations:
        name: KAIST
bibliography: all.bib
giscus_comments: true
disqus_comments: true
date: 2024-04-07
featured: true
toc:
  - name: Introduction
    subsections:
      - name: Q&A
      - name: History
      - name: Contribution
  - name: Idea
    subsections:
      - name: Heavy Priming
      - name: Primate Data  
  - name: Experiments
  - name: Conclusion

title: 'Primary Bias in RL ë…¼ë¬¸ ë¦¬ë·° (ICML 2022)'
description: 'ì´ í¬ìŠ¤íŒ…ì€ The Primacy Bias in Deep Reinforcement Learning ë…¼ë¬¸ì— ëŒ€í•œ ê³µë¶€ì…ë‹ˆë‹¤.'
---


ì´ ê¸€ì€ [The Primacy Bias in Deep Reinforcement Learning](https://proceedings.mlr.press/v162/nikishin22a.html) <d-cite key="wang2024augmenting"> </d-cite> ë…¼ë¬¸ì— ëŒ€í•œ ë¦¬ë·°ì…ë‹ˆë‹¤. 


<h2 id="introduction"> 1. Introduction </h2>

ê°•í™”í•™ìŠµ ì—ì´ì „íŠ¸ëŠ” ìˆœì²˜ì ìœ¼ë¡œ ìƒˆë¡œìš´ ë°ì´í„°ë¥¼ ë§ˆì£¼í•˜ê³  ëª¨ë¸ì„ ì—…ë°ì´íŠ¸í•˜ì—¬ ë” ë‚˜ì€ í–‰ë™ì„ ë°°ìš´ë‹¤. ì§€ì†ì ìœ¼ë¡œ ëª¨ë¸ì„ ì—…ë°ì´íŠ¸í•˜ë©´ì„œ ê¸°ì¡´ ì •ë³´ë“¤ì€ ìŠê±°ë‚˜ ì—…ë°ì´íŠ¸í•˜ë©° ìƒˆë¡œìš´ ì •ë³´ë“¤ì„ í•™ìŠµí•˜ì—¬ í–‰ë™ì„ ì—…ë°ì´íŠ¸ í•œë‹¤. 

ì•„ë¬´ ì •ë³´ë„ ì—†ëŠ” ì‚¬ëŒì´ ë°°ìš°ëŠ” ê²ƒê³¼ ì •ë³´ë¥¼ ì•Œê³  ìˆëŠ” ì‚¬ëŒì˜ ë°°ìš°ëŠ” ì†ë„ì— ì°¨ì´ê°€ ìˆìŒì€ ìëª…í•˜ë‹¤. ë˜í•œ ì˜ëª»ëœ ì •ë³´ë¥¼ ê°€ì§€ê³  ìˆë‹¤ë©´, ì´ë¥¼ ì§€ìš°ê³  ìƒˆë¡œìš´ ì •ë³´ë¥¼ í•™ìŠµí•˜ëŠ” ê²ƒì€ í•™ìŠµì´ ë”ìš± ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆë‹¤. 

ë§ˆì°¬ê¸°ì§€ë¡œ ëª¨ë¸ì˜ ì´ˆê¸° íŒŒë¼ë¯¸í„°ì— ëŒ€í•´ì„œ íŠ¹ì • í–‰ë™ì„ ë°°ìš°ëŠ” ê²ƒê³¼ ê¸°ì¡´ í•™ìŠµëœ ëª¨ë¸ íŒŒë¼ë¯¸í„°ì— ëŒ€í•´ì„œ ìƒˆë¡œìš´ í–‰ë™ì„ ë°°ìš°ëŠ” ê²ƒì€ ì°¨ì´ê°€ ìˆë‹¤. 

ì´ ì—°êµ¬ì—ì„œëŠ” ì´ˆê¸°ì— í•™ìŠµí•œ ì •ë³´ì— ëŒ€í•œ í¸í–¥ì´ ê°•í™”í•™ìŠµ ì—ì´ì „íŠ¸ì˜ ì´í›„ í•™ìŠµì— ëŒ€í•´ì„œë„ í° ì˜í–¥ì„ ë¼ì¹œë‹¤ëŠ” ì ì„ íƒêµ¬í•˜ì˜€ê³ , íŠ¹ì • ë ˆì´ì–´ì— ëŒ€í•œ reset ë°©ì‹ì´ ëª¨ë¸ ì„±ëŠ¥ì„ ë”ìš± ê°œì„ ì‹œí‚¬ ìˆ˜ ìˆë‹¤ëŠ” ì ì„ ë³´ì˜€ë‹¤. 

<h3 id="q-a"> Pre-Questions and Answers  </h3> 

1. **Primary Biasì˜ ì •ì˜ëŠ” ë¬´ì—‡ì¸ê°€?** : ì´ˆë°˜ì— ê´€ì°°í•œ ë°ì´í„°ì— ëŒ€í•œ í•™ìŠµì´ ì´í›„ í•™ìŠµì— ì˜í–¥ì„ ë¼ì¹˜ëŠ” ê²ƒ. 
2. **Primary BiasëŠ” ì–´ë–»ê²Œ ì¸¡ì •í•˜ëŠ”ê°€?** : ì´ˆê¸° ë°ì´í„°ì— ëŒ€í•´ì„œ í•™ìŠµëœ ë ˆì´ì–´ë¥¼ ë¦¬ì…‹í•œ ê²½ìš° í•™ìŠµ ì†ë„ê°€ ë”ìš± ë¹ ë¥´ë‹¤ë©´ íŒŒë¼ë¯¸í„°ê°€ biasëœ ê²ƒì´ë‹¤. 
3. **Primary BiasëŠ” RL ì•Œê³ ë¦¬ì¦˜ì— ì˜í–¥ì„ ë°›ëŠ”ê°€?** : ì•„ë‹ˆë‹¤. ë‹¤ì–‘í•œ ì•Œê³ ë¦¬ì¦˜ì— ëŒ€í•´ì„œ ëª¨ë‘ resetì„ í•´ì¤˜ì•¼ ë”ìš± ë¹ ë¥´ê²Œ í•™ìŠµë¨ì„ í™•ì¸í•˜ì˜€ë‹¤. 
4. **Primary BiasëŠ” ì–´ë–»ê²Œ í•´ì†Œë  ìˆ˜ ìˆëŠ”ê°€?** : íŒŒë¼ë¯¸í„° ë¦¬ì…‹. ê·¸ëŸ¬ë‚˜ ëª¨ë¸ êµ¬ì¡° í˜¹ì€ ì•Œê³ ë¦¬ì¦˜ë§ˆë‹¤ ë¦¬ì…‹ì„ í•˜ëŠ” ìœ„ì¹˜ê°€ ë‹¤ë¥´ë‹¤. ë˜í•œ reset periodëŠ” ê²½í—˜ì ìœ¼ë¡œ ì°¾ì•„ì ¸ì•¼ í•œë‹¤. 
5. **Primary Biasì— ëŒ€í•´ì„œ learning rateì„ í‚¤ìš°ëŠ” ê²½ìš° í•´ì†Œë  ìˆ˜ ìˆëŠ”ê°€?** : ì•„ë‹ˆë‹¤. learning rateì´ í¬ë‹¤ê³  í•´ë„ ì´ë¯¸ í•™ìŠµëœ í‘œí˜„ì„ ë°”ê¿”ì•¼ í•˜ê¸°ì—, ì ì ˆí•˜ì§€ ëª»í•œ pre-trained parameterëŠ” ë¬¸ì œê°€ ë  ìˆ˜ ìˆë‹¤. 


<h3 id="history"> Historical Notes </h3>

* ì£¼ë¡œ cognitive scienceì—ì„œ ì¸ì§€ ê´€ë ¨í•˜ì—¬ ê¸°ì¡´ì— ì•Œê³  ìˆë˜ í¸í–¥ì´ ì´í›„ì—ë„ ì˜ì‹ í˜¹ì€ ë¬´ì˜ì‹ì ìœ¼ë¡œ ë‚¨ì•„ìˆë‹¤ëŠ” ê²ƒì´ ì•Œë ¤ì ¸ìˆë‹¤. 

* Resetê´€ë ¨í•´ì„œëŠ” ìµœê·¼ ì—°êµ¬ëœ Dormant Neuron (í•™ìŠµ ê³¼ì •ì—ì„œ êº¼ì§€ëŠ” ë‰´ëŸ°ì´ ë°œìƒí•˜ëŠ”) í˜„ìƒê³¼ ì—°ê´€ë˜ì–´ ìˆë‹¤. 

* íŠ¹ì • ë ˆì´ì–´ (íŠ¹íˆ ë§ˆì§€ë§‰ ë ˆì´ì–´)ë¥¼ ë¦¬ì…‹í•˜ëŠ” ê²ƒì€ NLP í˜¹ì€ visionì—ì„œ ë§ˆì§€ë§‰ ë ˆì´ì–´ë¥¼ íŠ¹ì • í…ŒìŠ¤í¬ì— ëŒ€í•´ì„œ ìƒˆë¡­ê²Œ í•™ìŠµí•˜ê³  ê¸°ì¡´ feature extractorëŠ” freezeí•˜ëŠ” ê²ƒê³¼ ìœ ì‚¬í•˜ë‹¤. 

<h3 id="contribution"> Main Contribution of Paper </h3>

ì—ì´ì „íŠ¸ì˜ ë ˆì´ì–´ë¥¼ periodically resetting í•˜ì—¬ primary biasë¥¼ ì œê±°í•˜ì˜€ë‹¤. (Atari 100k, DeepMind Control Suite)


##  Idea
<blockquote>
â€œYour assumptions are your windows on the world. Scrub
them off every once in a while, or the light wonâ€™t come in.â€ <br><br> -Issac Asimov
</blockquote>

ë‘˜ ì¤‘ì—ì„œ ë” ë‚˜ì€ ì—ì´ì „íŠ¸ëŠ” ëˆ„êµ¬ì¼ê¹Œ? 

* ğŸ™ Bob : ì´ˆë°˜ì—ëŠ” ì˜ëª»ëœ ì •ë³´ì— ëŒ€í•´ì„œ í•™ìŠµí•˜ê³ , ì´í›„ ê¹¨ë—í•œ ë°ì´í„°ë¥¼ í•™ìŠµí•˜ì—¬ ë¦¬ì›Œë“œ 100 ë„ë‹¬ 
* ğŸŒ· Alice : ê¹¨ë—í•œ ë°ì´í„°ì— ëŒ€í•´ì„œ í•™ìŠµë§Œ ì§„í–‰í•˜ì—¬ ë¦¬ì›Œë“œ 100 ë„ë‹¬

ë‘˜ ë‹¤ ê¹¨ë—í•œ ë°ì´í„°ì— ëŒ€í•´ì„œ í•™ìŠµì„ ì§„í–‰í•´ë„, Bobì˜ ê²½ìš° ì´ì „ì— ë°°ì› ë˜ ì •ë³´ë“¤ì´ ë¨¸ë¦¿ì† ë¬´ì˜ì‹ì— ë‚¨ì•„ìˆì–´ ëª¨ë¸ ì„±ëŠ¥ì„ ì €í•˜ì‹œí‚¬ ìˆ˜ ìˆë‹¤ <d-footnote> a cognitive bias demonstrated by studies of human learning (Marshall & Werder, 1972; Shteingart et al., 2013) </d-footnote>.

ì´ëŠ” ì´ˆë°˜ì— í•™ìŠµí•œ í’ˆì§ˆì´ ë‚®ì€ ë°ì´í„°ê°€ ì´í›„ í•™ìŠµì— ì˜í–¥ì„ ì¤€ë‹¤ëŠ” ì ì„ ì‹œì‚¬í•œë‹¤. ì—°ì†ì ì¸ ë°ì´í„°ì— ëŒ€í•´ì„œ í•™ìŠµí•˜ëŠ” RLì´ ì§€ë‹ˆëŠ” primary biasë¥¼ íƒêµ¬í•  í•„ìš”ê°€ ìˆë‹¤. 

<blockquote>
<strong> The Primacy Bias in Deep RL:</strong> a tendency to overfit early
experiences that damages the rest of the learning process.
</blockquote>



### Heavy Priming

Heavy Priming Causes Unrecoverable Overfitting 

<blockquote>
Could overfitting on a single batch of early data
be enough to entirely disrupt an agentâ€™s learning process?
</blockquote>

ì‹¤í—˜ ì„¸íŒ…ì€ ë‹¤ìŒê³¼ ê°™ë‹¤. 

1. Soft Actor Critic 
2. quadruped-run (DeepMind Control suite)
3. *heavy priming*: after collecting 100 data points, we update the agent $10^5$ times using the resulting replay buffer, before resuming standard training.
<img src="https://onedrive.live.com/embed?resid=AE042A624064F8CA%211418&authkey=%21ALgeXc8Ho_XyIs8&width=472&height=471" width="472" height="471" />



### Primate Data  
Question: is the data collected by an overfitted agent unusable for learning?

1. We train a SAC agent with 9 updates per step in the MDP: due to the primacy bias, this agent performs poorly. 
2. Then, we initialize the same agent from scratch but use the data collected by the previous SAC agent as its initial replay buffer

<img src="https://onedrive.live.com/embed?resid=AE042A624064F8CA%211419&authkey=%21AHH_2zYpZjxZMvs&width=469&height=467" width="469" height="467" />

<blockquote>
The primacy bias is not a failure to collect proper data per se, but rather a failure to learn from it.
</blockquote>



## Experiments 

* **for SPR**<d-footnote> Data-efficient reinforcement learning with self-predictive representations (Schwarzer, M., 2020)</d-footnote>, we reset only the final linear layer of the 5-layer Q-network over the course of training spaced 2 Ã— 104 steps apart; 
* **for SAC**, we reset agentâ€™s networks entirely every 2 Ã— 105 steps since the networks have only 3 layers; 
* **for DrQ**, we reset the last 3 out of 7 layers of the policy and value networks 10 times over the course of training


<img src="https://onedrive.live.com/embed?resid=AE042A624064F8CA%211425&authkey=%21AAPekFXzwPdFmb8&width=929&height=252" width="929" height="252" />

### Learning Dynamics 
<img src="https://onedrive.live.com/embed?resid=AE042A624064F8CA%211428&authkey=%21AIjVdNASjbaRuCA&width=454&height=554" width="454" height="554" />

### Replay Ratio 

Replayë¥¼ ë”ìš± ìì£¼ í•˜ëŠ” ê²½ìš° ëª¨ë¸ì˜ íŒŒë¼ë¯¸í„°ëŠ” ë”ìš± ì´ˆê¸° ë°ì´í„°ì— í¸í–¥ì ìœ¼ë¡œ í¸í•œë‹¤. ë”°ë¼ì„œ resetì„ í–ˆì„ ë•Œ ì„±ëŠ¥í–¥ìƒì´ í¬ê²Œ ë‚˜íƒ€ë‚¬ë‹¤. 

<img src="https://onedrive.live.com/embed?resid=AE042A624064F8CA%211427&authkey=%21AJjziMeO_vpFNBk&width=927&height=235" width="927" height="235" />

### TD Learning 

TD learningì—ì„œëŠ” horizonì— ëŒ€í•œ $n$ ê°’ì„ ì»¨íŠ¸ë¡¤í•˜ì—¬ state-action valueì— ëŒ€í•œ ì¶”ì •ì„ ì§„í–‰í•œë‹¤. ë§Œì¼ $n$ì´ ì‘ë‹¤ë©´ varianceê°€ ì‘ì•„ì§€ê³ , $n$ì´ í¬ë‹¤ë©´ varianceê°€ ì»¤ì§€ê³  biasëŠ” ì‘ì•„ì§€ê²Œ ëœë‹¤. 

$$
\mathbb{E}_\pi [r(s_t, a_t) + \gamma r(s_{t+1}, r_{t+1}), \cdots, \gamma^n Q_\pi(s_{t+n}, a_{t+n})]
$$

Here, $n$ controls a trade-off between the (statistical) bias of QÏ€ estimates and the variance of the sum of future rewards.

**Varianceê°€ í´ìˆ˜ë¡ return ê°’ì€ ë”ìš± ë“¤ì­‰ë‚ ì­‰ì´ ë˜ê³ , ì´ˆê¸° ë°ì´í„°ì— ë”ìš± í¸í–¥ì ì´ë‹¤.**   íŒŒë¼ë¯¸í„° resetì„ í–ˆì„ ë•Œ ì„±ëŠ¥í–¥ìƒì´ ì»¸ë‹¤. 

<img src="https://onedrive.live.com/embed?resid=AE042A624064F8CA%211426&authkey=%21AAoMcbXQedYLSuc&width=936&height=263" width="936" height="263" />


### TD Failure  

TD failure modes Temporal-difference learning, when
employed jointly with function approximation and offpolicy
training, is known to be potentially unstable (Sutton
& Barto, 2018). In sparse-reward environments,
the critic network might converge to a degenerate solution
because of bootstrapping mostly on itâ€™s own outputs
(Kumar et al., 2020); having the non-zero reward
data might not sufficient to escape a collapse

<img src="https://onedrive.live.com/embed?resid=AE042A624064F8CA%211431&authkey=%21ALmOeG40FmF0PwU&width=483&height=402" width="483" height="402" />


### Reset What and How

We conjecture that the difference lies in the degree of representation learning required for each domain: a significant chunk of knowledge in Atari is contained in the agentâ€™s representations; it might be notably easier to learn features in DeepMind Control, especially when dealing with dense states.

## Conclusion

* ì´ˆê¸°ë°ì´í„°ì— ë„ˆë¬´ ë§ì€ learningì„ í•˜ë©´ ì•ˆëœë‹¤. 
* catastrophic forgettingì€ ì˜ ì¼ì–´ë‚˜ì§€ ì•Šì„ ìˆ˜ ìˆë‹¤. 
* ì£¼ê¸°ì ìœ¼ë¡œ ëª¨ë¸ì˜ íŒŒë¼ë¯¸í„°ë¥¼ reset í•´ì£¼ì 

