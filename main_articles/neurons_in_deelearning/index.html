<!DOCTYPE html>
<!-- _layouts/distill.html -->
<html>
    
<head>    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Bumjini | Neurons in Deep Learning</title>
    <meta name="author" content="Bumjini  " />
    <meta name="description" content="How can we interpret neurons in deep learning? <br> [üìö Neuron Series 2]" />
    <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website" />


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous" />

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light" />

    <!-- Styles -->
    
    <link rel="shortcut icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ü™¥</text></svg>">
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://fxnnxc.github.io/main_articles/neurons_in_deelearning/">
    
    <!-- Dark Mode -->
    

  <!-- jQuery -->
<script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>
  <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams',
        inlineMath: [['$','$'], ['\\(','\\)']]
      },
      chtml: {
          scale: 1.0,
          minScale: .6,  
          mtextFontInherit: true,
          mtextInheritFont: true,
          merrorInheritFont: true,
        },
        svg: {
          scale: 1.2
        }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

  <!-- Distill js -->
  <script src="/assets/js/distillpub/template.v2.js"></script>
  <script src="/assets/js/distillpub/transforms.v2.js"></script>
  <script src="/assets/js/distillpub/overrides.js"></script>
  <!-- Page/Post style -->
  
</head>

  <d-front-matter>
    <script async type="text/json">{
      "title": "Neurons in Deep Learning",
      "description": "How can we interpret neurons in deep learning? <br> [üìö Neuron Series 2]",
      "published": "August 10, 2023",
      "authors": [
        {
          "author": "Bumjin Park",
          "authorURL": "",
          "affiliations": [
            {
              "name": "KAIST",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script>
  </d-front-matter>

  <body style="padding-top:0px" class="sticky-bottom-footer"> 
    
    <!-- Header --><header>
      
      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm sticky-top">
        <div class="container">
          <img>
          <!-- <img src="/assets/common/KAIST-hi.gif" width="40px" style="margin-right:0px; padding-bottom: 3px;"> -->
          
          <a class="navbar-brand title font-weight-lighter" href="https://fxnnxc.github.io/" style="margin-left:20px ;">
              <!--Bumjini-->
           <span class="font-weight-bold" style="font-size:larger;font-family:Times New Roman;">Bumjini    </span>
        </a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <!-- <li style="font-size: 17px" class="nav-item ">
                <a class="nav-link" href="/">  About Me</a>
              </li> -->
              
              <!-- Blog -->
              <li style="font-size: 17px" class="nav-item ">
                <a class="nav-link" href="/main_papers/"> Papers</a>
              </li>

              <li style="font-size: 17px" class="nav-item ">
                <a class="nav-link" href="/main_articles/"> Articles</a>
              </li>

              <li style="font-size: 17px" class="nav-item ">
                <a class="nav-link" href="/main_projects/"> Projects</a>
              </li>

              <!-- Other pages -->
              <li style="font-size: 17px" class="nav-item dropdown ">
                <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Others</a>
                <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown">
                  <a class="dropdown-item" href="/side_papers/">üóÇÔ∏è side-paper</a>
                  <a class="dropdown-item" href="/side_articles/">ü•ï side-articles</a>
                  <a class="dropdown-item" href="/book/">üìö book (korean)</a>
                </div>
              </li>
            </ul>
          </div>
        </div>
      </nav>
    </header>

    <!-- Content -->
    <div class="post distill">

      <d-title>
        <h1 style="font-size: 32px;">Neurons in Deep Learning</h1>
        <p>How can we interpret neurons in deep learning? <br> [üìö Neuron Series 2]</p>
      </d-title>

      <d-byline></d-byline>

      <d-article>
        

        <h2 id="introduction">Introduction</h2>

<p>In the previous section, we introduced neurons in deep learning by comparing neurons in neuroscience. The previous section only introduced neurons in a linear weight and bias. With the same interpretation, we introduce several neurons in deep learning modules including multi layer perceptron (MLP), convolutional neural network (CNN), recurrent neural network, and generative pretrained models (GPT). In addition, we also provide a neuron perspective on key-value memory  in GPT. We believe this interpretation of neurons can further help to understand the general computation of deep neural networks.</p>

<h2 id="linear-neurons-in-mlp">Linear Neurons in MLP</h2>

<p>Let see the most basic module, weights in  a linear layer. The weights are a matrix whose rows are the outputs for neurons and the columns for the synapses of these neurons.  You can think that each row (neuron) computes independently the input signal $x$.</p>

<p>With the assumption that $W \in \mathbb{R}^{\textcolor{red}{m} \times \textcolor{blue}{n}}$</p>
<ul>
  <li>üå± number of neurons : $\textcolor{red}{m}$</li>
  <li>üå± number of synapses per a neuron : $\textcolor{blue}{n}$</li>
  <li>üå± number of synapses in total : $\textcolor{red}{m} \times \textcolor{blue}{n}$</li>
</ul>

<p>MLP is a series of linear neurons with activations in between them. The activations are thresholds to pass the information with valid signals. For example, ReLU cuts negative signals and Tanh provides gating mechanisms of signals. Therefore, in the interpretation of neurons, we can think of activations as modifiers of neurons‚Äôs outputs.</p>

<p>The Figure below shows two layer MLP. Each layer has two neurons and the neurons in the first layer have three synapses for each neuron, while the neurons in the second layer have two synapses for each neuron.</p>

<figure style="text-align:left; display:block;width:100;">
<img src="/assets/kor/neurons_as_neurons/linear.png" style="width:100%">
<figcaption>
  There are 2 neurons in each layer and 4 neurons in total. Synapses in a neuron computes dot-product with the input signals. 
</figcaption>
</figure>

<p>The Figure below shows the four layer MLP. In each layer, three neurons exist and each neuron has three synapses. Note that all the pre-neurons and post-neurons are connected with synapses. We think that is why MLP is called a densely connected neural network.</p>

<figure style="text-align:left; display:block;width:100;">
<img src="/assets/kor/neurons_as_neurons/mlp.png" style="width:100%">
<figcaption>
  An example of four layer MLP. The first panel shows weight perspectives, while the second panel shows neuron‚Äôs perspective. Note that all the neurons output only a single scalar respectively.   
</figcaption>
</figure>

<h2 id="convolutional-neuron">Convolutional Neuron</h2>

<p>Convolutional layer is also considered as a linear layer as we can reformulate the convolution operation as a linear operation. However, we use another tool to interpret the convolutional neuron. The most frequent way of creating a convolutional layer is  using the notation: (<code class="language-plaintext highlighter-rouge">out_channels</code>, <code class="language-plaintext highlighter-rouge">in_channels</code>, <code class="language-plaintext highlighter-rouge">kernel</code>, <code class="language-plaintext highlighter-rouge">kernel</code>, ..). With this notation, the <code class="language-plaintext highlighter-rouge">out_channels</code> parts are the same with the rows in the linear neuron and (<code class="language-plaintext highlighter-rouge">in_channels</code>, <code class="language-plaintext highlighter-rouge">kernel</code>, <code class="language-plaintext highlighter-rouge">kernel</code>) are the synapses for a neuron. With this interpretation, the only difference with the linear layer is (<code class="language-plaintext highlighter-rouge">kernel</code>, <code class="language-plaintext highlighter-rouge">kernel</code>) part where the convolution operation is applied in the 2D plane. If you assume that kernel size is 1, then you can think of a linear neuron!</p>

<ul>
  <li>üå± number of neurons :  <code class="language-plaintext highlighter-rouge">out_channels</code>
</li>
  <li>üå± number of synapses per a neuron :  <code class="language-plaintext highlighter-rouge">in_channels</code>$\times$ <code class="language-plaintext highlighter-rouge">kernel</code> $\times$ <code class="language-plaintext highlighter-rouge">kernel</code>
</li>
  <li>üå± number of synapses in total :  <code class="language-plaintext highlighter-rouge">out_channels</code> $\times$  <code class="language-plaintext highlighter-rouge">in_channels</code> $ \times$ <code class="language-plaintext highlighter-rouge">kernel</code> $\times$ <code class="language-plaintext highlighter-rouge">kernel</code>
</li>
</ul>

<hr>

<h2 id="recurrent-neuron">Recurrent Neuron</h2>

<p>Beside MLP and CNN, the other basic module is a recurrent neural network (RNN) which successively computes an input with linear operations in it with hidden states. Computationally, the recurrent neuron is the same with the linear neuron except input and output types. Note that the recurrent neuron should provide outputs which are re-injected to the input of that neuron. The advanced RNN modules such as LSTM <d-cite key="hochreiter1997long"></d-cite> and GRU <d-cite key="chung2014empirical"></d-cite> use the gating mechanism in them to process the long-term memory well. These architectures do not change the interpretation of neurons as they are combinations of linear neurons.</p>

<hr>

<h2 id="neurons-in-gpt-block">Neurons in GPT Block</h2>

<p>Now we discuss neurons in more advanced architectures such as GPT. GPT consists of a token embedding module, transformer decoder blocks and task specific heads such as LMHead. We verify neurons in a GPT block and leave the interpretation of other modules as they are just linear-like neurons. GPT block has two main parts MLP and self-attention (or cross-attention), and layer-norm.</p>

<h4 id="mlp">MLP</h4>

<p>MLP block is a two layer MLP whose first layer is sometimes called key and second layer sometimes is called value. We use $d_{model}$ which is the dimension of GPT representations to describe the number of neurons and synapses. The key part is a linear layer with weight \(W_{key} \in \mathbb{R}^{\textcolor{red}{4\cdot d_{model}} \times \textcolor{blue}{d_{model}}}\) whose number of neurons is four times larger than the number of synapses. On the other hand the value part is a linear layer with weight \(W_{value} \in \mathbb{R}^{\textcolor{red}{ d_{model}} \times \textcolor{blue}{4\cdot d_{model}}}\) whose number of neurons is four times smaller than the number of synapses. Therefore, this architecture is a bottleneck architecture and many researchers interpret it as a neural memory.</p>

<p>Consider $d_{model} = 12,288$ which is the hidden dimension size of GPT3 (175B).</p>

<ul>
  <li>üå± number of neurons :  49,152 (key neuron) + 12,288 (value neuron)</li>
  <li>üå± number of synapses per a neuron : 12,288 (key neuron), 49,152 (value neuron)</li>
  <li>üå± number of synapses in total :  <strong>1,207,959,552</strong>  <br> (= 49,152 $\times$ 12,288 (key neuron) +  12,288 $\times$ 49,152 (value neuron))</li>
</ul>

<h4 id="attn">ATTN</h4>

<p>In ATTN, there are four linear layers: query, key, value, output and each layer has weight \(W \in \mathbb{R}^{\textcolor{green}{d_{model}} \times \textcolor{green}{d_{model}}}\). Therefore, we can easily interpret neurons in ATTN<d-footnote> If you assume QK, OV  circuits, the number of neurons are reduce by 2 times. <d-cite key="elhage2021mathematical"></d-cite> </d-footnote>. Note that self-attention and cross-attention are about how to use these neurons and do not affect the interpretation of neurons</p>

<p>Consider $d_{model} = 12,288$ which is the hidden dimension size of GPT3 (175B).</p>

<ul>
  <li>üå± number of neurons :<strong>61,440</strong> = 12,288 $\times$ 4 (Q, K, V, O)</li>
  <li>üå± number of synapses per a neuron : : <strong>12,288</strong> (Q, K, V, O)</li>
  <li>üå± number of synapses in total :  <strong>603,979,776</strong> = (12,288 $\times$ 4) $\times$ 12,288</li>
</ul>

<hr>

<h2 id="neurons-in-gpt3-175b">Neurons in GPT3 (175B)</h2>

<p>Finally, we count the number of neurons and synapses in GPT3 (175B) blocks with the above interpretation<d-footnote> Caveats: We do not count the layer-norm in a block and the number of weights would not exactly match with the number of synapses in this case. Also, bias is a parameter in GPT, but is not counted for neuron and synapse. </d-footnote>.   GPT3 has 96 blocks and each block has MLP whose number synapses is <strong>1,207,959,552</strong> and ATTN whose number of synapses is <strong>603,979,776</strong>. We can simply add these two and multiply 96 to compute the total number of synapses.</p>

<ul>
  <li>üçÄ (<code class="language-plaintext highlighter-rouge">GPT3</code>) number of synapses : <strong>173B</strong>  <br> (173,946,175,488 = 96 $\times$ (1,207,959,552 + 603,979,776)) <d-footnote>  2B difference (175B and 173B) is due to bias, embedding, layer-norm and LM heads modules. </d-footnote>
</li>
  <li>‚òòÔ∏è (<code class="language-plaintext highlighter-rouge">GPT3</code>) number of neurons :  <strong>11M</strong> <br> (11,796,480 = 96 $\times$(61,440+61,440))</li>
  <li>‚òòÔ∏è (<code class="language-plaintext highlighter-rouge">GPT3</code>) number of synapses per a neuron : <strong>14,745</strong>
</li>
</ul>

<p>As humans have 7,000 synapses per neuron, we can think that GPT3 has a similar number or even larger number of connections between neurons. However, humans have much more neurons compared to GPT.  We believe such a difference can affect the general learning process. For example, if the number of neurons is large, we can see them as an ensemble of simple knowledge. On the other hand, if the number of synapses is large, expert neurons will provide knowledge.</p>

<h2 id="conclusion">Conclusion</h2>

<p>As we think more about a deep neural network with a metaphor to a human brain, the computational graph is highly interpretable and the individual components handle very simple computations like neurons in our brain. Recent work in interpretability is about discovering meanings of neurons and the work like this can shed light on the interpretation of the deep neural network. We believe still deep dissection of neural network can give better interpretation of deep neural networks.</p>

<hr>

<h2 id="appendix-interpret-key-neurons-and-value-neurons">(Appendix) Interpret Key Neurons and Value Neurons</h2>

<p>In the MLP section, we discuss the key-value neurons. In this section, we provide more detailed neuron-perspectives on these modules. The Figure below shows key-value computation with sizes, 3 for input, 8 for key, and 2 for value neurons <d-footnote> We use 3 for the better visualization of the input, which is originally 2 in GPT.  </d-footnote>.</p>

<figure style="text-align:left; display:block;width:100;">
<img src="/assets/kor/neurons_as_neurons/key-value.png" style="width:100%">
<figcaption>
  The key-neurons output scalars which are multiplied with weights (synapses) in the value-neurons. Then, the values neurons compute them (linear sum) to make a single output respectively.
</figcaption>
</figure>

<p>If we just assume that a neuron just provide a single output, we can interpret a neuron in a row-wise manner. That is, we don‚Äôt think about the meaning of weights and just consider the output value of each neuron. The Figure below shows the neuron-wise interpretation.</p>

<figure style="text-align:left;">
<img src="/assets/kor/neurons_as_neurons/neuron_inter.png" style="width:100%">
<figcaption>
  The row-wise neuron interpretation. As a single neuron has multiple synapses, the aggregated signal will go out and further be cut with activations. 
</figcaption>
</figure>

<p>Unlike the neuron-wise interpretation where we interpret each neuron separately, we usually interpret multiple neurons together by considering the directional vector. In this case, the weights in value-neurons are not just used for aggregation of synapses, but could be interpreted by directionally meaningful information. The Figure below shows the example. In this case, the key is used to scale the vectors.</p>

<figure style="text-align:left;">
<img src="/assets/kor/neurons_as_neurons/value_inter.png" style="width:100%">
<figcaption>
  Column-interpretation of neurons. It is more natural to think a weight matrix as directional vectors. The key-neurons output scales for the directional vectors. 
</figcaption>
</figure>

      </d-article>

      <d-appendix>
        <d-footnote-list></d-footnote-list>
        <d-citation-list></d-citation-list>
      </d-appendix>

<!--<div id="disqus_thread" style="max-width: 1000px; margin: 0 auto;">
    <script type="text/javascript">
        var disqus_shortname  = 'al-folio';
        var disqus_identifier = '/main_articles/neurons_in_deelearning';
        var disqus_title      = "Neurons in Deep Learning";
        (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        })();
    </script>
    <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</div>
    </div>
 -->
      <hr>
<div id="giscus_thread" style="max-width: 1000px; margin: 0 auto;">
    <script>
      let giscusTheme = localStorage.getItem("theme");
      let giscusAttributes = {
          "src": "https://giscus.app/client.js",
          "data-repo": "fxnnxc/fxnnxc",
          "data-repo-id": "MDEwOlJlcG9zaXRvcnkzMjQ2NjUxMTU=",
          "data-category": "Q&A",
          "data-category-id": "DIC_kwDOE1n_G84CYOk_",
          "data-mapping": "title",
          "data-strict": "0",
          "data-reactions-enabled": "1",
          "data-emit-metadata": "0",
          "data-input-position": "top",
          "data-theme": giscusTheme,
          "data-lang": "en",
          "crossorigin": "anonymous",
          "async": "",
      };
  
  
      let giscusScript = document.createElement("script");
      Object.entries(giscusAttributes).forEach(([key, value]) => giscusScript.setAttribute(key, value));
      document.getElementById("giscus_thread").appendChild(giscusScript);
    </script>
    <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" target="_blank" rel="noopener noreferrer">comments powered by giscus.</a>
</noscript>
  </div>
<!-- Footer -->    <footer class="sticky-bottom mt-5" style="border: none;border-top:0px">
      <div class="container" style="text-align: center; ">
        ¬© Copyright 2024 Bumjini  . Powered by Jekyll with al-folio theme. Hosted by GitHub Pages.

      </div>
    </footer>
    
    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.2/dist/umd/popper.min.js" integrity="sha256-l/1pMF/+J4TThfgARS6KwWrk/egwuVvhRzfLAMQ6Ds4=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.min.js" integrity="sha256-SyTu6CwrfOhaznYZPoolVw2rxoY7lKYKQvqbtqN93HI=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    
    

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
  /*
   * This JavaScript code has been adapted from the article 
   * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar, 
   * published on the website https://css-tricks.com on the 7th of May, 2014.
   * Couple of changes were made to the original code to make it compatible 
   * with the `al-foio` theme.
   */
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    let navbarHeight = $("#navbar").outerHeight(true);
    $("body").css({ "padding-top": 0 });
    $("progress-container").css({ "padding-top": 0 });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  /*
   * The offset between the html document height and the browser viewport
   * height will be greater than zero if vertical scroll is possible.
   * This is the distance the user can scroll
   */
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>
  </div></body>
  
  <d-bibliography src="/assets/bibliography/all.bib">
  </d-bibliography>
  <script src="/assets/js/distillpub/overrides.js"></script>

  <center>
    <a  href="">
    <img src="/assets/common/KAIST-hi.gif" width="40px" style="margin-right:0px; padding-bottom: 3px;">
    </a>
  </center>
  <hr> 

  </html>
