<!DOCTYPE html>
<!-- _layouts/distill.html -->
<html>
    
<head>    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Bumjini | [2] Related Work for SIP [Data Provenance]</title>
    <meta name="author" content="Bumjini  " />
    <meta name="description" content="Literature Review of Provenance." />
    <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website" />


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous" />

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light" />

    <!-- Styles -->
    
    <link rel="shortcut icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>🪴</text></svg>">
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://fxnnxc.github.io/Rsip/230917_data_provenance/">
    
    <!-- Dark Mode -->
    

  <!-- jQuery -->
<script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>
  <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams',
        inlineMath: [['$','$'], ['\\(','\\)']]
      },
      chtml: {
          scale: 1.0,
          minScale: .6,  
          mtextFontInherit: true,
          mtextInheritFont: true,
          merrorInheritFont: true,
        },
        svg: {
          scale: 1.2
        }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

  <!-- Distill js -->
  <script src="/assets/js/distillpub/template.v2.js"></script>
  <script src="/assets/js/distillpub/transforms.v2.js"></script>
  <script src="/assets/js/distillpub/overrides.js"></script>
  <!-- Page/Post style -->
  
</head>

  <d-front-matter>
    <script async type="text/json">{
      "title": "[2] Related Work for SIP [Data Provenance]",
      "description": "Literature Review of Provenance.",
      "published": "September 17, 2023",
      "authors": [
        {
          "author": "Bumjin Park",
          "authorURL": "",
          "affiliations": [
            {
              "name": "KAIST",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script>
  </d-front-matter>

  <body style="padding-top:0px" class="sticky-bottom-footer"> 
    
    <!-- Header --><header>
      
      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm sticky-top">
        <div class="container">
          <img>
          <!-- <img src="/assets/common/KAIST-hi.gif" width="40px" style="margin-right:0px; padding-bottom: 3px;"> -->
          
          <a class="navbar-brand title font-weight-lighter" href="https://fxnnxc.github.io/" style="margin-left:20px ;">
              <!--Bumjini-->
           <span class="font-weight-bold" style="font-size:larger;font-family:Times New Roman;">Bumjini    </span>
        </a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <!-- <li style="font-size: 17px" class="nav-item ">
                <a class="nav-link" href="/">  About Me</a>
              </li> -->
              
              <!-- Blog -->
              <li style="font-size: 17px" class="nav-item ">
                <a class="nav-link" href="/main_papers/"> Papers</a>
              </li>

              <li style="font-size: 17px" class="nav-item ">
                <a class="nav-link" href="/main_articles/"> Articles</a>
              </li>

              <li style="font-size: 17px" class="nav-item ">
                <a class="nav-link" href="/main_projects/"> Projects</a>
              </li>

              <!-- Other pages -->
              <li style="font-size: 17px" class="nav-item dropdown ">
                <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Others</a>
                <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown">
                  <a class="dropdown-item" href="/side_papers/">🗂️ side-paper</a>
                  <a class="dropdown-item" href="/side_articles/">🥕 side-articles</a>
                  <a class="dropdown-item" href="/book/">📚 book (korean)</a>
                </div>
              </li>
            </ul>
          </div>
        </div>
      </nav>
    </header>

    <!-- Content -->
    <div class="post distill">

      <d-title>
        <h1 style="font-size: 32px;">[2] Related Work for SIP [Data Provenance]</h1>
        <p>Literature Review of Provenance.</p>
      </d-title>

      <d-byline></d-byline>

      <d-article>
        

        <h2 id="list-of-papers">List of Papers</h2>

<ul>
  <li>Membership Inference Attacks on Sequence-to-Sequence Models: Is My Data In Your Machine Translation System? <a href="#membership-inference-attacks-on-sequence-to-sequence-models-is-my-data-in-your-machine-translation-system">🔗</a> <d-cite key="hisamoto2020membership"></d-cite>
</li>
  <li>Membership Inference Attacks Against Machine Learning Models <a href="#membership-inference-attacks-against-machine-learning-models">🔗</a>  <d-cite key="shokri2017membership"></d-cite>
</li>
  <li>Watermarking Text Data on Large Language Models for Dataset Copyright Protection <a href="#watermarking-text-data-on-large-language-models-for-dataset-copyright-protection">🔗</a> <d-cite key="liu2023watermarking"></d-cite>
</li>
  <li>Robust Multi-bit Natural Language Watermarking through Invariant Features <a href="#robust-multi-bit-natural-language-watermarking-through-invariant-features">🔗</a> <d-cite key="yoo2023robust"></d-cite>
</li>
  <li>CodeIPPrompt: Intellectual Property Infringement Assessment of Code Language Models  <a href="#codeipprompt-intellectual-property-infringement-assessment-of-code-language-models">🔗</a> <d-cite key="yu2023codeipprompt"></d-cite>
</li>
  <li>A Watermark for Large Language Models <a href="#a-watermark-for-large-language-models">🔗</a> <d-cite key="kirchenbauer2023watermark"></d-cite>
</li>
  <li>Evading Water mark based Detection of AI-Generated Content <a href="#evading-water-mark-based-detection-of-ai-generated-content">🔗</a>  <d-cite key="jiang2023evading"></d-cite>
</li>
  <li>How to Protect Copyright Data in Optimization of Large Language Models?  [🔗] <d-cite key="shoker2023confidence"></d-cite> (#how-to-protect-copyright-data-in-optimization-of-large-language-models) <d-cite key="chu2023protect"></d-cite>
</li>
  <li>Confidence-Building Measures for Artificial Intelligence: <a href="#confidence-building-measures-for-artificial-intelligence-workshop-proceedings">🔗</a>
</li>
  <li>ContextLS  <d-cite key="yang2022tracing"></d-cite>
</li>
  <li>AWT  <d-cite key="abdelnabi2021adversarial"></d-cite>
</li>
</ul>

<h2 id="short-summary">Short Summary</h2>

<ul>
  <li>Paper <d-cite key="hisamoto2020membership"></d-cite> :  The authors propose MI framework on sequence-to-sequence generation and shows similar private data leakage with classification problems.</li>
  <li>Paper <d-cite key="shokri2017membership"></d-cite>  : The Membership Inference problem which determines whether a private data is used in the training of LMs, firstly proposed in this work.</li>
  <li>Paper  <d-cite key="liu2023watermarking"></d-cite> : The backdoor attack approach, which makes a desired output with a stealthy included trigger, is applied to watermark input sequence.</li>
  <li>Paper <d-cite key="yu2023codeipprompt"></d-cite> : This work shows that LM based code generators memorize the code beyond the licensed usage.</li>
  <li>Paper <d-cite key="kirchenbauer2023watermark"></d-cite> : This work proposed watermark framework based on the distribution of selected tokens. This work is the firts work which utilizes the watermark on the distribution of tokens in generation.</li>
  <li>Paper <d-cite key="jiang2023evading"></d-cite> : T</li>
  <li>[]</li>
</ul>

<hr>

<p><img src="https://drive.google.com/uc?export=view&amp;id=1DaSSF2dSi46Vgt5c2rogKTIlGEwyX8Le" style="margin-left:-7rem;width:140%"></p>

<hr>

<h3 id="membership-inference-attacks-against-machine-learning-models">Membership Inference Attacks Against Machine Learning Models</h3>

<p>Membership inference: given a machine learning model and a record, determine whether this record was used as part of the model’s training dataset or not.</p>

<p>The adversary’s access wto the model is limited to black-box queries that return the model’s output oon a given input.</p>

<blockquote>
  <p>Backdoor Attack : lead a model to infer the desired output with a stealthy included trigger.  As backdoor attack can output the desired pattern, it could be used for watermarks.</p>
</blockquote>

<hr>

<h3 id="membership-inference-attacks-on-sequence-to-sequence-models-is-my-data-in-your-machine-translation-system">Membership Inference Attacks on Sequence-to-Sequence Models: Is My Data In Your Machine Translation System?</h3>

<p>MI : given a machine learning model and a record, determine whether this record was used as part of the model’s training dataset or not.</p>

<p>MI+Seq-to-Seq : Given black-box access to an MT model, is it possible to determine whether a particular sentence pair was in the training set for that model?</p>

<p>Simple one-off attacks based on shadow models, which proved successful in classification problems, are not successful on sequence generation problems: this is a result that favors the defender. Nevertheless, we describe the specific conditions where sequence-to-sequence models still leak private information.</p>

<p>A training set for MT consists of a set of sentence pairs ${ (f_i^{(d)}, e_i^{(d)})}$. We use a label $d\in { \ell_1, \ell_2, \cdots, }$ to indicate the domain (the subcorpus or the data source).</p>

<hr>

<h3 id="watermarking-text-data-on-large-language-models-for-dataset-copyright-protection">Watermarking Text Data on Large Language Models for Dataset Copyright Protection</h3>

<ul>
  <li>Authors : Yixin Liu, Hongsheng Hu, Xuyun Zhang, Lichao Sun</li>
  <li>Published Date : 2023.05.22, Arxiv</li>
  <li>Record Date :</li>
</ul>

<p>Specifically, there is a potential risk to individual’s privacy if their private data is used to train these LLMs without any authorization. This has incited significant apprehension regarding both data privacy and copyright protection.</p>

<p>To prevent the illegal or unauthorized usage of private text data in LLMs, individuals should be able to verify whether or not a company used their personal text to train a model.</p>

<hr>

<h3 id="robust-multi-bit-natural-language-watermarking-through-invariant-features">Robust Multi-bit Natural Language Watermarking through Invariant Features</h3>

<ul>
  <li>Authors : KiYoon Yoo, Wonhyuk Ahn, Jiho Jang, Nojun Kwak</li>
  <li>Published Date : ACL 2023</li>
  <li>Record Date : 2023.09.15</li>
</ul>

<p>This calls for a secure watermarking system to guarantee copyright protection through leakage tracing or ownership identification.</p>

<p>Our full method improves upon the previous work on robustness by +16.8% point on average on four datasets, three corruption types, and two corruption ratios.</p>

<p>Digital watermarking is a technology that enables the embedding of information into multimedia in an unnoticeable way without degrading the original utility of the content.</p>

<p>Deep watermarking has emerged as a new paradigm that improves the three key aspects of watermarking: payload (i.e., the number of bits embedded), robustness (i.e., the number of bits embedded), robustness (i.e., accuracy of the extracted message), and quality of embedded media.</p>

<p>Previous research has focused on techniques such as lexical substitution with predefined rules and dictionaries or structural transformation.</p>

<p>Recent work have either replaced the predefined set of rules with learning-based methodology, thereby removing heuristics or vastly improved the quality of lexical substitutions.</p>

<p>A well-known proposition of a classical image watermarking work: That watermarks should <em>“be placed explicitly in the perceptually most significant components”</em>  of an image. If this is achieved, the adversary must corrupt the content’s fundamental structure to destroy the watermark. This degrades the utility of the original content, rendering the purpose of pirating futile.</p>

<p>Modification in individual pixels is much more imperceptible than on individual words. But to this, while we adhere to the gist of the proposition, we do not embed directly on the most significant component. Instead, we identify features that are semantically or syntactically fundamental components of the text and thus, invariant to minor modifications in texts.</p>

<hr>

<h3 id="codeipprompt-intellectual-property-infringement-assessment-of-code-language-models">CodeIPPrompt: Intellectual Property Infringement Assessment of Code Language Models</h3>

<ul>
  <li>Authors : Zhiyuan Yu, Yuhao Wu, Ning Zhang, Chenguang Wang, Yevgeny Vorobeychik, Chaowei Xiao.</li>
  <li>Published Date :</li>
  <li>Record Date :</li>
</ul>

<hr>

<h3 id="a-watermark-for-large-language-models">A Watermark for Large Language Models</h3>

<ul>
  <li>Authors : John Kirchenbauer, Jonas Geiping, Yuxin Wen, Jonathan Katz, Ian Miers, Tom Goldstein</li>
  <li>Published Date : 2023 ICML (Best Paper Award)</li>
  <li>Record Date : 2023.09.18</li>
</ul>

<h3 id="detecting-the-watermark">Detecting the watermark</h3>

<p>A third party with knowledge of the has function and random number generator can re-produce the red list for each token and count how many times the red list rule is violated.</p>

<blockquote>
  <p>$H_0$ : The text sequence is generated with no knowledge of the red list rule.</p>
</blockquote>

<p><img src="https://drive.google.com/uc?export=view&amp;id=1MA7qvM0p0L_lIWNUPqFxht7_iQ1r3yuw" style="width:100%"></p>

<blockquote>
  <h3 id="spark-entropy">Spark Entropy</h3>

  <p>the spike entropy of $p$ with modulus $z$ as</p>

\[S(p,z) = \sum_k \frac{p_k}{1+z p_k}\]

</blockquote>

<hr>

<p><span class="spanbox"> Copyright </span></p>

<h3 id="evading-watermark-based-detection-of-ai-generated-content">Evading Watermark based Detection of AI-Generated Content</h3>

<ul>
  <li>Authors : Zhengyuan Jiang, Jinghuai Zhang, Neil Zhenqiang Gong</li>
  <li>Published Date : 2023.08.22  (Arvix) / ACM Conference on Computer and Communications Security (CCS), 2023 <d-cite key="jiang2023evading"></d-cite>
</li>
  <li>Record Date : 2023.09.12</li>
</ul>

<p>The watermark enables proactive detection of Ai-generated content in the future: a content is AI-generated if a similar watermark can be extracted from it.</p>

<ul>
  <li>DALL-E  : visible watermark at the bottom right corner of its generated images.</li>
  <li>Stable Diffusion  : non-learning-based watermarking method</li>
  <li>Meta : learning-based watermarking methods</li>
</ul>

<hr>

<ul>
  <li>image, watermark (bitstring)</li>
  <li>encoder : given an image and a watermark, an encoder embeds the watermark into the image to produce a <em>watermarked image</em>
</li>
  <li>decoder : given a <em>watermarked image</em> generates the watermark inside of it.</li>
</ul>

<p>An image is predicted as AI-generated if the bitwise accuracy of the decoded watermark is larger than a threshold $\tau$, where bitwise accuracy is the fraction of matched bits in the decoded watermark and the ground-truth one.
The threshold should be larger than 0.5 since the bitwise accuracy of original images without watermarks would be around 0.5.</p>

<p>Robustness against <em>post-processing</em>, which post-processes an AI-generated image, is crucial for a watermark-based detector.</p>

<p>See <a href="/side_articles/ml/#false-positive-rate">ML-False Positive Rate</a></p>

<hr>

<blockquote>
  <p>As threshold $\tau$ in a prediction increased, the false positive rate decreases as we have less number of positive predictions.</p>
</blockquote>

<h4 id="single-tail-detector">Single-tail Detector</h4>

<p>An image $I$ is predicted as AI-generated if the bitwise accuracy of its decoded watermark is larger than a threshold $\tau$, i.e., $BA(D(I), w)&gt; \tau$. 
Bitwise accuracy is computed by</p>

\[BA(D(I_0), w) = \frac{m}{n}\]

<p>The service provider should pick the ground-truth watermark $w$ uniformly at random. 
Thus, the decoded watermark $D(I_0)$ is not related to the randomly picked $W$, and each bit of $D(I_0)$ is not related to the randomly picked $w$, and 
each bit of $D(I_0)$ matches with the corresponding bit of $w$ with probability 0.5. As a result, $m$ is a random variable and follows a binominal distribution $B(n, 0.5)$.</p>

<p>Therefore, the FPR of the single-tail detector with threshold $\tau$ can be calculated as follows:</p>

\[\begin{aligned}
FPR (\tau) &amp;= Pr(BA(D(I_0), w) &gt; \tau) \\
          &amp;= Pr(m &gt; n\tau) = \sum_{k= \lceil n\tau \rceil}^n \begin{pmatrix} n \\ k \end{pmatrix} \frac{1}{2^k} \frac{1}{2^{n-k}}
\end{aligned}\]

<p>Thus, to make FPR rate less than $\eta$, we should find $\tau$ such that</p>

<p>\(\tau^{*} = \arg \min_{\tau}  \sum_{k= \lceil n\tau  \rceil}^n \begin{pmatrix} n \\ k \end{pmatrix} &lt; \eta\)
For instance, when $n=256$ and $\eta = 10^{-4}$, we have $\tau \ge \tau^* \approx 0.613$.</p>

<h4 id="double-tail-detector">Double-tail Detector</h4>

<p>As adversarial attack can evade the watermark, the bitwise accuracy should remain at 0.5. 
The authors proposed a double-tail detector that detects an image $I$ asa  AI-generated if its decoded watermark ahs a bitwise accuracy 
larger than $\tau$ or smaller than $1-\tau$, i.e., $BA(D(I), w) &gt; \tau$ or $BA(D(I), w) &lt; 1-\tau$.
The FPR of the double-tail detector with threshold $\tau$ is:</p>

\[\begin{aligned}
FPR_{double}(\tau) &amp;= Pr(BA(D(I_o), w)&gt; \tau \operatorname{ or } BA(D(I_o), w) &lt;  1-\tau) \\
                   &amp;= Pr(m&gt;n\tau \operatorname{ or } m &lt; n -n\tau) = 2 \sum_{k=\lceil n\tau \rceil}^n \begin{pmatrix} n \\ k \end{pmatrix} \frac{1}{2^n}
\end{aligned}\]

<hr>

<p><span class="spanbox"> Copyright </span></p>

<h3 id="confidence-building-measures-for-artificial-intelligence-workshop-proceedings">Confidence-Building Measures for Artificial Intelligence: Workshop Proceedings</h3>

<ul>
  <li>Authors : Sara Shoker, Andrew Reddie, et al.,</li>
  <li>Published Date : 2023.08.03 (Arxiv) <d-cite key="shoker2023confidencebuilding"></d-cite>
</li>
  <li>Record Date : 2023.09.12</li>
</ul>

<p>Provenance and watermarking methods can improve <strong>traceability</strong>, alleviate concerns about <strong>the origin of the AI generated or edited content</strong>, and promote trust among parties.</p>

<p>If properly vetted against adversarial manipulation, they can also help states use AI-generated products more confidently, knowing that <strong>the outcomes can be traced back to their source</strong>.</p>

<p>Coalition for Content Provenance and Authenticity (C2PA), whose members include Adobe, Microsoft, Intel, and so on, is an industry-led initiative that develops technical standards for establishing <strong>the source and history of media content</strong>.</p>

<p>C2PA specifications, provenance methods can be split between “hard” and “soft” binding</p>

<ul>
  <li>
<strong>SOFT</strong> : Watermarking (they are more easily undermined with modification of contents)</li>
  <li>
<strong>HARD</strong> : methods for applying unique identifiers to data assets and other cryptographic methods. (using cryptographically-bound provenance can include information about the origin of a piece of content such as AI model or version used to create it. )</li>
</ul>

<p>More Info on <em><a href="https://syntheticmedia.partnershiponai.org/#learn_more" target="_blank" rel="noopener noreferrer">PAI’s Responsible Practices for Synthetic Media</a></em></p>

<p>Watermarking can serve as a verification mechanism to confirm the authenticity and integrity of AI generations. 
Watermarking involves embedding low probability sequences of tokens into the outputs produced by AI systems.</p>

<ul>
  <li>
    <p><strong>Drawbacks</strong> : Watermarks are not <span class="spanbox" style="background-color:#FFEEEE;"> tamper-proof </span> (변조 방지). Bad actors can use “paraphrasing attacks” to <strong>(1) remove text watermarks</strong>, <strong>(2) spoofing to infer hidden watermark signatures</strong>, or even <strong>(3) add watermarks to authentic content</strong>.</p>
  </li>
  <li>
    <p>Removal of Watermark <strong>Evading Watermark based Detection of AI-Generated Content</strong>  <d-cite key="jiang2023evading"></d-cite> [<a href="https://arxiv.org/abs/2305.03807" target="_blank" rel="noopener noreferrer">Arvix</a>].</p>
  </li>
</ul>

<p>Open provenance standards and open sourcing <strong>AI detection technologies</strong> should be encouraged to help reduce the cost of security.</p>

<p>The proliferation of foundation models means that provenance and watermarking is unlikely to be applied evenly by all developers.</p>

<hr>

<p><span class="spanbox"> Copyright </span></p>

<h3 id="how-to-protect-copyright-data-in-optimization-of-large-language-models">How to Protect Copyright Data in Optimization of Large Language Models?</h3>

<ul>
  <li>Authors : Timothy Chu, Zhao Song, Chiwun Yang</li>
  <li>Published Date : 2023.08.23 (Arxiv) <d-cite key="chu2023protect"></d-cite>
</li>
  <li>Record Date : 2023.09.11</li>
</ul>

<p>LLMs are built on the transformer neural network architecture, which in turn relies on a mathematical computation called Attention that uses the softmax function. \</p>

<p>To solve copyright regression for the softmax function, we show that the objective function of the softmax copyright regression is convex, and that its Hessian is bounded.</p>

<ul>
  <li>
    <p>[Gil 19 ] investigates copyright infringement in AI-generated artwork and argues that using copyrighted works during the training phase of AI programs does not result in infringement liability.</p>
  </li>
  <li>
    <p>[VKB23] proposes a frameowkr that provides stronger protection against sampling protected content, by defining near access-freeness (NAF)</p>
  </li>
</ul>

<blockquote>
($\tau$-Copyright-Protected )
<br>

If there is a trained model $f_\theta$ with parameter $\theta$ that satisfies 

$$
\frac{L(f_\theta(A_1), b_1)}{n_1} \ge \tau + \frac{L(f_\theta(A_2)), b_2}{n_2}
$$

then we say this model $f_\theta$ is $\tau$-Copyright-Protected. 
</blockquote>


      </d-article>

      <d-appendix>
        <d-footnote-list></d-footnote-list>
        <d-citation-list></d-citation-list>
      </d-appendix>

<!--
    </div>
 -->
      <hr>
<!-- Footer -->    <footer class="sticky-bottom mt-5" style="border: none;border-top:0px">
      <div class="container" style="text-align: center; ">
        © Copyright 2024 Bumjini  . Powered by Jekyll with al-folio theme. Hosted by GitHub Pages.

      </div>
    </footer>
    
    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.2/dist/umd/popper.min.js" integrity="sha256-l/1pMF/+J4TThfgARS6KwWrk/egwuVvhRzfLAMQ6Ds4=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.min.js" integrity="sha256-SyTu6CwrfOhaznYZPoolVw2rxoY7lKYKQvqbtqN93HI=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    
    

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
  /*
   * This JavaScript code has been adapted from the article 
   * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar, 
   * published on the website https://css-tricks.com on the 7th of May, 2014.
   * Couple of changes were made to the original code to make it compatible 
   * with the `al-foio` theme.
   */
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    let navbarHeight = $("#navbar").outerHeight(true);
    $("body").css({ "padding-top": 0 });
    $("progress-container").css({ "padding-top": 0 });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  /*
   * The offset between the html document height and the browser viewport
   * height will be greater than zero if vertical scroll is possible.
   * This is the distance the user can scroll
   */
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>
  </div></body>
  
  <d-bibliography src="/assets/bibliography/all.bib">
  </d-bibliography>
  <script src="/assets/js/distillpub/overrides.js"></script>

  <center>
    <a  href="">
    <img src="/assets/common/KAIST-hi.gif" width="40px" style="margin-right:0px; padding-bottom: 3px;">
    </a>
  </center>
  <hr> 

  </html>
