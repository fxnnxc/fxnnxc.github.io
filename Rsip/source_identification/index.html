<!DOCTYPE html>
<!-- _layouts/distill.html -->
<html>
    
<head>    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Bumjini | Source Identification for Generative Models</title>
    <meta name="author" content="Bumjini  " />
    <meta name="description" content="Can we provide the source of generation?" />
    <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website" />


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous" />

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light" />

    <!-- Styles -->
    
    <link rel="shortcut icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ü™¥</text></svg>">
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://fxnnxc.github.io/Rsip/source_identification/">
    
    <!-- Dark Mode -->
    

  <!-- jQuery -->
<script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>
  <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams',
        inlineMath: [['$','$'], ['\\(','\\)']]
      },
      chtml: {
          scale: 1.0,
          minScale: .6,  
          mtextFontInherit: true,
          mtextInheritFont: true,
          merrorInheritFont: true,
        },
        svg: {
          scale: 1.2
        }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

  <!-- Distill js -->
  <script src="/assets/js/distillpub/template.v2.js"></script>
  <script src="/assets/js/distillpub/transforms.v2.js"></script>
  <script src="/assets/js/distillpub/overrides.js"></script>
  <!-- Page/Post style -->
  
</head>

  <d-front-matter>
    <script async type="text/json">{
      "title": "Source Identification for Generative Models",
      "description": "Can we provide the source of generation?",
      "published": "July 10, 2023",
      "authors": [
        {
          "author": "Bumjin Park",
          "authorURL": "",
          "affiliations": [
            {
              "name": "KAIST",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script>
  </d-front-matter>

  <body style="padding-top:0px" class="sticky-bottom-footer"> 
    
    <!-- Header --><header>
      
      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm sticky-top">
        <div class="container">
          <img>
          <!-- <img src="/assets/common/KAIST-hi.gif" width="40px" style="margin-right:0px; padding-bottom: 3px;"> -->
          
          <a class="navbar-brand title font-weight-lighter" href="https://fxnnxc.github.io/" style="margin-left:20px ;">
              <!--Bumjini-->
           <span class="font-weight-bold" style="font-size:larger;font-family:Times New Roman;">Bumjini    </span>
        </a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <!-- <li style="font-size: 17px" class="nav-item ">
                <a class="nav-link" href="/">  About Me</a>
              </li> -->
              
              <!-- Blog -->
              <li style="font-size: 17px" class="nav-item ">
                <a class="nav-link" href="/main_papers/"> Papers</a>
              </li>

              <li style="font-size: 17px" class="nav-item ">
                <a class="nav-link" href="/main_articles/"> Articles</a>
              </li>

              <li style="font-size: 17px" class="nav-item ">
                <a class="nav-link" href="/main_projects/"> Projects</a>
              </li>

              <!-- Other pages -->
              <li style="font-size: 17px" class="nav-item dropdown ">
                <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Others</a>
                <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown">
                  <a class="dropdown-item" href="/side_papers/">üóÇÔ∏è side-paper</a>
                  <a class="dropdown-item" href="/side_articles/">ü•ï side-articles</a>
                  <a class="dropdown-item" href="/book/">üìö book (korean)</a>
                </div>
              </li>
            </ul>
          </div>
        </div>
      </nav>
    </header>

    <!-- Content -->
    <div class="post distill">

      <d-title>
        <h1 style="font-size: 32px;">Source Identification for Generative Models</h1>
        <p>Can we provide the source of generation?</p>
      </d-title>

      <d-byline></d-byline>

      <d-article>
        <d-contents>
          <nav class="l-text figcaption" style="line-height: 15px;">
          <h3>Contents</h3>
            <div style="margin:0.0em; margin-bottom: 10px;margin-top: 10px;"><a style="margin:0.0em;" href="#problems">Problems</a></div>
            
          </nav>
        </d-contents>

        <blockquote>
  <p>This is a problem formulation for finding the source of generation.</p>
</blockquote>

<h2 id="1-introduction">1. Introduction</h2>

<p>In the training of a deep neural network, we collect a large amount of data and process the data only for training purposes. 
The pre-process mostly decreases the amount of information to abandon irrelevant information in data and provides compact representations for the training data by discarding information which is not helpful for the training. The process of filtering information is essential to properly match the training objective efficiently and robustly <d-footnote> For example, in the classification model, we use correlative features. In the field of NLP, we use natural sentences without meta data. </d-footnote>. Although such filtering can benefit the training, we discard even necessary meta data which helps to identify the information such as <strong>the source of data</strong>. One example would be the copyright of lyrics or contents in a book. For example, can you identify the source of the sentence below? That is, who said the sentence.</p>
<blockquote>
  <p>Creating safe AGI that benefits all of humanity  <br>   - From : ???</p>
</blockquote>

<p>The source of the sentence is <a href="https://openai.com/" target="_blank" rel="noopener noreferrer">website of OpenAI ¬©</a>. As the source is entangled with the sentence, we get additional information such as</p>
<ul>
  <li>The underlying context of the sentence</li>
  <li>The factuality of the sentence</li>
  <li>The meaning of AGI (if you are in the AI field.)</li>
</ul>

<p>Even though there are benefits of knowing the source location of the data, the source link of data may not required to train neural networks <d-footnote> For example, the causal language modeling of GPT<d-cite key="brown2020language"></d-cite> and the masked language modeling of BERT do not require the source link to maximize the training objective. </d-footnote>. However, the source information is highly important when our objective is not performance but tracing the source of the information that the generative model provides. In short,</p>

<blockquote>
  <text style="color:red"> Only for the training purpose, discarding irrelevant information is beneficial. But, we don't know made the information.  </text>
</blockquote>

<p>Within the progress of generative models, another training mechanism is required to ensure the safety issues for both models and data.</p>

<hr>

<h2 id="2-problem-definition">2. Problem Definition</h2>

<p>The fundamental problem is the identification of the source of a corpus called as source identification problem (SIP).  Given a corpus, scrapped from a website or generated by chatGPT, we identify the original source of the corpus. The casual link of source $S$, text $x$, generator $G$, and the generated text $\hat{x}$ is as follows:</p>

\[\begin{equation}
s \rightarrow x \rightarrow  G \rightarrow  \hat{x}
\end{equation}\]

<p>The SIP is a identification problem of the source of the text $x$ or the generated text.</p>

\[\begin{gather}
x \rightarrow s \\ 
\hat{x} \leftarrow s
\end{gather}\]

<p>However, it is a ill-posed problem as multiple sources are possible. 
As finding the source from corpus is <strong>neither injective nor surjective</strong>. 
For example, the corpus <em>‚Äúapple is delicious‚Äù</em> is a quite common sentence and lots of websites can include the corpus (non-injective) and the corpus <em>‚Äúfine apple is pricked by pineapple.‚Äù</em> may not have the source, but generated by a generative model (non-surjective). In addition if the text is slightly changed by adding <em>‚ÄúThe‚Äù</em>, the corpora have almost same meaning and the sources must be equal.</p>

\[\begin{equation}
\hat{x} \rightarrow \{s_1, s_2, \cdots\}
\end{equation}\]

<p>As such, the source verification problem is a problem of generating possible multiple sources of a given corpus. Although, functionally learning the Equation (4) can solve the source identification problem, we don‚Äôt know the actual meaning of mapping corpora to sources. One reason is that a corpus has semantic and lexical information and two different corpora can have exactly same information.</p>

<hr>
<h2 id="3-building-blocks-of-sip">3. Building Blocks of SIP</h2>

<p>Before proceeding on the detailed discussion on SIP, we discuss on the proper definitions for sources and corpora. <strong>C-corpora (copyrighted-corpora)</strong> are semantic or lexical contents in the source.   <strong>Source</strong> is the genuine location of c-corpora.</p>

<h3 id="31-copyrighted-corpora">3.1 Copyrighted-Corpora</h3>

<h4 id="which-information">Which information</h4>

<p>The copyrighted-corpora is  copyrighted a corpora in two perspectives, semantic anc lexical.</p>

<blockquote>
  <p>Ex) Creating safe AGI that benefits all of humanity</p>
  <ul>
    <li>Semantic : AI system benefits all people</li>
    <li>Lexical : Creating, safe, AGI, benefit, humanity</li>
  </ul>
</blockquote>

<p>As such, we have the following properties for copyrighted-corpora</p>
<ul>
  <li>
<code class="language-plaintext highlighter-rouge">Semantic Source</code>   : the semantic meaning of a copyrighted-corpus comes from the source</li>
  <li>
<code class="language-plaintext highlighter-rouge">Lexical Source</code>  : the lexical information of a copyrighted-corpus comes from the sentence</li>
</ul>

<h4 id="which-sources">Which Sources</h4>

<p>One additional property is that a corpus can be formed by multiple sources of corpora. For example, we can combine two corpus from OpenAI and Meta each to make the following sentence.  [¬© from <a href="https://about.meta.com/actions/" target="_blank" rel="noopener noreferrer">Meta‚Äôs Action</a>]</p>
<blockquote>
  <p>Ex) Creating safe AGI that benefits all of humanity <strong>by keeping people safe and making a positive impact.</strong></p>
</blockquote>

<h4 id="definition-of-copyrighted-corpora">Definition of Copyrighted-Corpora</h4>

<blockquote>
  <p><strong style="font-style:normal">Definition [Copyrighted-Corpora]</strong> <br> <text style="font-style:normal"> Copyrighted-corpora is a corpora whose lexical or semantic meaning is similar with corpus from a source or combined by multiple corpus originated from possibly multiple corpus.  </text></p>
</blockquote>

<p>This definition is similar to how we think about copyright.  Even though a sentence is not totally matched with any sentence in training dataset, 
Multiple sentences from other sources can be used to form the sentence by extracting (1) semantic information and (2) lexical information.</p>

<h3 id="32-source">3.2 Source</h3>

<p>Source is the origin of sentences. The origin could be companies, persons, websites which have the copyright of the sentences. We list some possible sources.</p>

<ul>
  <li>
<code class="language-plaintext highlighter-rouge">Company</code> : The company who has the copyright of overall contents (OpenAI, Meta, ‚Ä¶)</li>
  <li>
<code class="language-plaintext highlighter-rouge">Person</code>  : The labeled person who talked or wrote sentences (Martin Ruther King, ‚Ä¶)</li>
  <li>
<code class="language-plaintext highlighter-rouge">Website</code> : The well known website (Wiki, Reddit, ‚Ä¶ )</li>
  <li>
<code class="language-plaintext highlighter-rouge">Hyperlinks</code> : Specific link of website  (https://distill.pub/2020/circuits/)</li>
</ul>

<h2 id="related-work">Related Work</h2>

<p>There are several ways to provide the source link of generated sentences.</p>

<ol>
  <li>Generation : directly generate the sources with labels.</li>
  <li>Watermark: inject identifiable keys to sentences which is visible in the inference steps</li>
  <li>Web search engine : search websites with a GPT and provide links</li>
  <li>Prompt tuning : ask GPT to provide the source</li>
</ol>

<table>
  <thead>
    <tr>
      <th>Methods</th>
      <th>Procedure</th>
      <th>Pros</th>
      <th>Cons</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Generation</strong></td>
      <td>$G \rightarrow \hat{x}  \rightarrow \hat{G} \rightarrow s$</td>
      <td>Easily learnable</td>
      <td>in-scalable, hard to believe</td>
    </tr>
    <tr>
      <td><strong>Watermark</strong></td>
      <td>$G \rightarrow (s), h \rightarrow \hat{x}$</td>
      <td>Only requires inference steps</td>
      <td>Unclear how to apply it to NLP.(Fourier Frequency)</td>
    </tr>
    <tr>
      <td><strong>Web Search</strong></td>
      <td>$G \rightarrow [s_1,s_2,\cdots ] \rightarrow s_k \rightarrow \hat{x}$</td>
      <td>Direct identification</td>
      <td>does not provide model‚Äôs knowledge in inference</td>
    </tr>
    <tr>
      <td><strong>Prompt Tuning</strong></td>
      <td>$G \rightarrow \hat{x}, S$</td>
      <td>The most easiest way</td>
      <td>hard to believe</td>
    </tr>
  </tbody>
</table>

<h3 id="identification-via-generation">Identification via Generation</h3>

<p>The most simple way is to train another GPT model to recover the source link of the sentences. 
When a GPT model $G$ generates sentences $\hat{x}$, another GPT-like model generates the description of sources $S$.</p>

\[G \rightarrow \hat{x}  \rightarrow \hat{G} \rightarrow s\]

<h3 id="watermark">Watermark</h3>
<p>Previous work is done in vision domain which has continuous representations.
Watermark is a simple way to encode information in the representation and previously explored in diffusion models <d-cite key="fernandez2023stable"></d-cite>,<d-cite key="wen2023tree"></d-cite>. Although it is compelling way of encoding a secret key in the generated stuffs. It is only available for syntactic representation and unclear for the semantic meaning itself. When a GPT model $G$ generates sentences $\hat{x}$, the inner representation $h$ provokes the source of sentences.</p>

\[G \rightarrow (s), h \rightarrow \hat{x}\]

<h3 id="webgpt-and-bing">WebGPT and Bing</h3>

<p>One way to prevent the copyright issues is to make a model directly uses the corpus in a source in the inference procedure. This method  can  provide the source link to the end users. Recently WebGPT uses the Bing search engine to improve the factuality and to provide the external links <d-cite key="nakano2021webgpt"></d-cite>, [<a href="https://openai.com/research/webgpt" target="_blank" rel="noopener noreferrer">OpenAI‚Äôs blog</a>].</p>

\[G \rightarrow [s_1,s_2,s_3,\cdots ] \rightarrow s_k \rightarrow \hat{x}\]

<h3 id="gpt-prompt-tuning">GPT Prompt tuning</h3>

<p>One direct way is to ask the model to provide the source link [<a href="https://www.zdnet.com/article/how-to-make-chatgpt-provide-sources-and-citations/" target="_blank" rel="noopener noreferrer">related post</a>].</p>

<blockquote>
  <p>Please provide sources for the previous answer <br> Please provide URL sources <br> Please provide 10 URL sources</p>
</blockquote>

\[G \rightarrow \hat{x}, S\]

<h1 id="methods">Methods</h1>

<figure style="display:block; grid-column:middle; width:100%;">
<img src="/assets/bjp/source_identification/mapping.png">
<figcaption>

</figcaption>
</figure>

<p>We propose Source generation fine-tuning framework to learn the auxiliary task.</p>

<figure style="display:block; grid-column:middle; width:100%">
<img src="/assets/bjp/source_identification/sip_token_attention.png">
<figcaption>
  Figure 1. An illustration of token communication in fine-tuned GPT for source generative task. In this example, the generated texts are composed by two texts of sources Wikipedia and Oxford dictionary. 
</figcaption>
</figure>

<figure style="display:block; grid-column:middle; width:100%;">
<img src="/assets/bjp/source_identification/sip_main_figure.png">
<figcaption>
  Figure 2. Framework of Source Generative Task.  (1) pre-training data is prepared without source tags. (2) A GPT model is trained with pre-training data by language modeling. (3) The pre-training data is tagged with source information. (4) The GPT model is fine-tuned to generate source of the generated texts.
</figcaption>
</figure>

<h2 id="watermark-1">Watermark</h2>

<h3 id="defender-watermark-">Defender: Watermark üåä</h3>

<p>For content $x$, watermark $b$ is given to protect the content. The defender encode an invisible watermark in the content 
with encoder $E$ takes the content and watermark to  generate a watermarked information $x_{wm}$:</p>

\[x_{wm} = E(x,b)\]

<p>The decoder $D$ takes $x_{wm}$ and recovers the watermark $b$.</p>

<h3 id="attacker--distort-watermark--">Attacker : Distort Watermark  üëæ</h3>

<p>The attacker tries to modify the watermarked $w_{wm}$ image by attacked content $\tilde{x}_{wm}$ with modifier $g$ to remove the encoded information $b$ in it.</p>

\[\tilde{x}_{wm} = g(w_{wm})\]

<p>The goal of the attacker is to minimize the recovery of watermark in the content.</p>

\[\min L(b, D(\hat{x}_{wm}))\]

<p>where $L$ is a matching loss such as bitwise accuracy.</p>

<h2 id="provenance-detection">Provenance Detection</h2>

<hr>

<h3 id="watermark-videos">Watermark videos</h3>

<p>pseudorandom function $f(w_{t-c+1}, \cdots, w_{t-1}, i)$ maps the latest token $c$ to $r_{t,i} \in [0,1]$. This is pseudo probability distribution.</p>

<h3 id="parameter-efficient-fine-tuning">Parameter Efficient Fine-tuning</h3>

<hr>

<h2 id="20230923--given-a-language-model-and-any-text-verify-the-source-prediction-performance">2023.09.23 : Given A Language Model and Any Text, Verify the Source Prediction Performance</h2>

<p>Let $f$ be a generative language model and  $\theta$ be the trained parameters with large corpus. 
Our goal is to verify the source $y$ of text tokens $x=(x_1, x_2, \cdots, x_n)$.  To decode the source of text, we use the logits of decoder outputs which we call 
<em>traced logits of text $x$</em>.</p>

\[T(x) = (\hat{p}_2, \hat{p}_3, \cdots, \hat{p}_{n+1})\]

<p>where $\hat{p}_i =  P( \cdot \vert  x_1, x_2, \cdots, x_i  ; \theta) \in \mathbb{R}^{V}$ is the conditional probability distribution over vocabulary $V$ given previous tokens.</p>

<p>This logits are different from previous watermark based data provenance. As the watermark assumes that the generated text is decoded output of the generative model, while our method does not have such assumption. Therefore, we consider how a generative model encodes any text.</p>

<hr>

<h3 id="20231009">2023.10.09</h3>

<p>Î™®Îç∏Ïù¥ Î¨∏Ïû•Ïóê ÎåÄÌïú Ï†ïÎ≥¥Î•º ÏïîÍ∏∞ÌïòÍ≥† ÏûàÏúºÎ©¥ÏÑú, ÌäπÏ†ï Î†àÏù¥Î∏îÏóê ÎåÄÌïú ÌôïÎ•†Í∞íÏù¥ ÎÜíÏùÄ Í≤ΩÏö∞, Ìï¥Îãπ ÏõêÏ≤ú ÏÜåÏä§Ïóê ÎåÄÌïú Ï∂úÏ≤òÏùò ÌôïÏã†Ïù¥ ÎÜíÏïÑÏßÑÎã§.</p>

<p>ÎÑ§Í±∞Ìã∞Î∏å ÏÉòÌîåÏùò Ï¢ÖÎ•òÏóê ÎåÄÌï¥ÏÑú Ï†ïÎ≥¥Î•º ÌåêÎã®Ìï¥Ïïº ÌïúÎã§. ÏòàÎ°ú, ÏôÑÎ≤ΩÌïòÍ≤å ÏùºÏπòÌïòÎäî Î¨∏Ïû•Îßå Ìè¨Ìï®ÌïòÎäî Í≤ΩÏö∞Î∂ÄÌÑ∞, Î¨∏Ïû•Ïùò ÏùòÎØ∏Í∞Ä ÎπÑÏä∑Ìïú Í≤ΩÏö∞, Í∑∏Î¶¨Í≥† ÏÇ¨Ïö©ÌïòÎäî Ïñ¥ÌúòÍ∞Ä ÎπÑÏä∑Ìïú Í≤ΩÏö∞Î•º ÏÉùÍ∞ÅÌï† Ïàò ÏûàÎã§. ÎòêÌïú, Ìï¥Îãπ ÏõêÏ≤ú Îç∞Ïù¥ÌÑ∞Ïóê ÎåÄÌï¥ÏÑú ÏõêÎ≥∏Í≥º GPT Ïù¥ ÏÉòÌîåÎßÅÏùÑ Ìïú Í≤ÉÏùÑ ÎπÑÍµêÌïòÎäî Í≤ΩÏö∞ÎèÑ Í∞ÄÎä•ÌïòÎã§. Ïù¥Îäî ÏõêÏ≤úÏÜåÏä§ ÌÉêÏßÄÏóê ÎåÄÌï¥ÏÑú ÏùºÎ∞òÌôîÏùò ÏàòÏ§ÄÏù¥ ÏöîÍµ¨ÌïòÎäî Ï†ïÎèÑÏóê Îî∞ÎùºÏÑú Îã§Î•¥Î©∞, Ïù¥Î°ú Ïù∏Ìï¥ÏÑú ÏòàÏ∏° Î™®Îç∏Ïùò Ï¢ÖÎ•òÏôÄ ÏÑ±Îä•ÎèÑ Îã¨ÎùºÏ†∏Ïïº ÌïúÎã§Îäî Í≤ÉÏùÑ ÏùòÎØ∏ÌïúÎã§. SoftÌïú Negative Sample ÏùÑ ÏÇ¨Ïö©ÌïòÎäî Í≤ΩÏö∞ Îã®ÏàúÌïú Î™®Îç∏Î°úÎèÑ ÏõêÏ≤úÏÜåÏä§Î•º ÌÉêÏßÄÌï† Ïàò ÏûàÎã§. Í∑∏Îü¨ÎÇò, ÏõêÏ≤úÏÜåÏä§Í∞Ä ÎπÑÏä∑Ìïú Îç∞Ïù¥ÌÑ∞ÎùºÎ©¥, Îëê ÌÅ¥ÎûòÏä§Î•º Íµ¨Î∂ÑÌïòÎäî Í≤ÉÏùÄ Ïâ¨Ïö¥ ÏùºÏù¥ ÏïÑÎãàÎ©∞, Ïñ¥Îñ†Ìïú ÌäπÏßïÏúºÎ°úÎ∂ÄÌÑ∞ Íµ¨Î∂ÑÌï†ÏßÄ Í≥†ÎØºÌï¥Ïïº ÌïúÎã§. ÌÅ¥ÎûòÏãúÌååÏù¥Ïñ¥Îäî Í≤∞Íµ≠,</p>

<hr>

<h3 id="20231010--ÌïôÏäµÎç∞Ïù¥ÌÑ∞Ïùò-ÌôïÏû•--xml-Îç∞Ïù¥ÌÑ∞-Î∂ÑÏÑù-Î∞è-Ï∂îÍ∞Ä">2023.10.10 : ÌïôÏäµÎç∞Ïù¥ÌÑ∞Ïùò ÌôïÏû• : XML Îç∞Ïù¥ÌÑ∞ Î∂ÑÏÑù Î∞è Ï∂îÍ∞Ä</h3>

<ul>
  <li>code release <a href="https://github.com/fxnnxc/source_identification_problem/tree/v23.10.10.1" target="_blank" rel="noopener noreferrer">v23.10.10</a>
</li>
  <li>XML <a href="https://github.com/fxnnxc/source_identification_problem/tree/v23.10.10.1/xml" target="_blank" rel="noopener noreferrer">README</a>
</li>
  <li>SIP Dataset <a href="https://github.com/fxnnxc/source_identification_problem/blob/v23.10.10.1/test/dataset.ipynb" target="_blank" rel="noopener noreferrer">Notebook</a>
</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">====================</span>
<span class="k">**</span> INCLUDE <span class="k">**</span>
LF-Amazon-131K
trn
From label 294805 131073
294805it <span class="o">[</span>00:00, 1010858.06it/s]
294805it <span class="o">[</span>00:00, 2783772.85it/s]
num inputs: 294805
num outputs: 294805
<span class="o">[</span><span class="s1">'Methodical Bible study: A new '</span>, <span class="s1">'GeoPuzzle U.S.A. and Canada - '</span><span class="o">]</span>
<span class="o">[</span><span class="s1">'4315:1.0\n'</span>, <span class="s1">'112532:1.0 113827:1.0\n'</span><span class="o">]</span>
<span class="o">====================</span>
Wikipedia-500K
trn
From label 1813391 501070
1813391it <span class="o">[</span>00:11, 163675.08it/s]
1813391it <span class="o">[</span>00:00, 2942573.18it/s]
num inputs: 1813391
num outputs: 1813391
<span class="o">[</span><span class="s1">'Anarchism redirect2|anarchis'</span>, <span class="s1">'Albedo other uses use dm'</span><span class="o">]</span>
<span class="o">[</span><span class="s1">'81199:1.0 83757:1.0 83805:1.0 '</span>, <span class="s1">'144177:1.0 144212:1.0 182348:1'</span><span class="o">]</span>

</code></pre></div></div>

<hr>

<h3 id="20231011--cross-entropy-with-increasing-number-of-labels-Ïã§Ìóò">2023.10.11 : Cross Entropy with Increasing Number of Labels Ïã§Ìóò</h3>

<p>Îã®ÏàúÌïòÍ≤å, Classifier Î•º ÌïôÏäµÌïú Í≤∞Í≥ºÎ•º ÏùºÎã® Î™®ÏúºÍ≥†, ÌïôÏäµ Î∞©Î≤ïÏùÑ Ï†êÏßÑÏ†ÅÏúºÎ°ú Í∞úÏÑ†Ìï¥Ïïº ÌïúÎã§. Í≤∞Íµ≠ SIP Î¨∏Ï†úÎ•º XML Î¨∏Ï†úÏóê Í∞ÄÍπùÍ∏∞ ÎïåÎ¨∏Ïù¥Îã§. Í∑∏ Ï†ÑÏóê, Îã®Ïàú Classifier Ïóê ÎåÄÌï¥ÏÑú ÌïôÏäµÌïú Í≤∞Í≥ºÎ•º ReportÌïòÎäî Í≤ÉÏùÄ  ÌïÑÏöîÌïòÎã§. Ïù¥ Îïå, CrossEntropy Loss Î•º Ìïú Í≤ÉÍ≥º Binary Cross Entropy Loss Î•º ÏÇ¨Ïö©Ìïú Í≤ÉÏùÑ ÎπÑÍµêÌï¥Ïïº ÌïúÎã§.</p>

<p>Let $S_i$ be the set of positive labels and negative labels in a shortlist $N_i$. The standard training loss for classification problem is cross entropy loss (CE). Cross entropy loss is defined as follows:</p>

\[\mathcal{L}_{CE} = \sum_i^N  \sum_{\ell \in S_i} y_{i\ell} \log \frac{\exp{(W_{\ell} z_i)}}{\sum_{\ell' } \exp{ (W_{\ell'} z_i) }}\]

<p>With the PG19 datasets with 100 samples per label each, we evaluate the basic ReLU based encoder for the classification problem.</p>

<d-code language="python" style="border:2px dashed gray;border-radius:10px;padding-left:5px;">  
# Model Spec
linear_hidden_size=1024
linear_activation=relu
linear_n_layers=2

</d-code>

<p>We observe that the training is not scalable with the increasing number of labels. We conclude two things in this experiment</p>

<ul>
  <li>(Suitable Loss) CE loss architecture is not suitable for source identification problem with increased number of sources.</li>
  <li>(Model Scalability) As the model size increases, the classification training is better on average.</li>
  <li>(Generalization) Generalization over source identification may not be feasible.</li>
</ul>

<h4 id="training">Training</h4>

<p><img src="https://drive.google.com/uc?export=view&amp;id=1KJ7ra-D_OGfkuZ5R6iW0PD_5TzC28-at"></p>

<h4 id="evaluation">Evaluation</h4>

<p><img src="https://drive.google.com/uc?export=view&amp;id=1Q-jg7hHsSxeFeSRV40TCXoJBgufddOUV"></p>

<h3 id="Î≥ÄÍ≤ΩÎêú-loss--ce--bce">Î≥ÄÍ≤ΩÎêú Loss : CE ‚Äì&gt; BCE</h3>

<p>Binary cross entropy loss is defined as follows:</p>

\[\mathcal{L}_{BCE} = \sum_i^N \sum_{\ell \in S_i} y_{i\ell} \log (\sigma (W_{\ell} z_i )) + (1-y_{i\ell}) \log (1- \sigma (W_{\ell} z_i ))\]

<p>where  $\sigma$ is sigmoid function $\sigma(x) = \frac{1}{1+\exp{(-x)}}$.</p>

<h3 id="ÏûÖÎ†•Í∞í-gpt-hiddensÏóê-ÎåÄÌïú-Í≥†ÎØº">ÏûÖÎ†•Í∞í (GPT Hiddens)Ïóê ÎåÄÌïú Í≥†ÎØº</h3>

<p><strong>Ï§ëÏöîÏßàÎ¨∏</strong> : ÌïôÏäµÏù¥ Í∞ÄÎä•Ìïú Í≤ÉÏùÄ Îã®Ïñ¥Ïóê ÎåÄÌïú Ïú†ÏùòÎØ∏Ìïú Feature Ïù¥Í∏∞ ÎïåÎ¨∏Ïù∏Í∞Ä? ÏïÑÎãàÎ©¥, Class Ïóê ÎåÄÌï¥ÏÑú Íµ¨Î∂ÑÎêú ÌëúÌòÑ Í≥µÍ∞ÑÏùÑ ÏßÄÎãàÍ∏∞ ÎïåÎ¨∏Ïù∏Í∞Ä? Ï¶â, Î¨∏Ïû•ÏùÑ Í∏∞ÏñµÌïòÏßÄ ÏïäÎçîÎùºÎèÑ, Îã®ÏàúÌûà ÌëúÌòÑÏù¥ Îã§Î•¥Í∏∞ ÎïåÎ¨∏Ïóê ÏõêÏ≤ú ÏÜåÏä§Î•º Ï∞æÎäî Í≤å Í∞ÄÎä•ÌïúÍ∞Ä?</p>

<p>GPT Î™®Îç∏Ïù¥ ÏõêÏ≤ú ÏÜåÏä§Ïóê ÎåÄÌï¥ÏÑú ÌïôÏäµÏùÑ Ìï¥ÏÑú, Ïù¥ÌõÑÏóê ÎÇòÏò§Îäî Îã®Ïñ¥Ïóê ÎåÄÌï¥ÏÑú ÌäπÏßïÏùÑ Í∞ÄÏßÄÎ©¥ÏÑú, ÌÅ¥ÎûòÏä§Î≥ÑÎ°ú Íµ¨Î∂ÑÎê† Ïàò ÏûàÎã§Î©¥, ÏõêÏ≤úÏÜåÏä§Î•º Ï∞æÎäîÎç∞ ÏÇ¨Ïö©Îê† Ïàò ÏûàÎã§. 
ÎßåÏùº, GPT Î™®Îç∏Ïù¥ Ìï¥Îãπ Î¨∏Ïû•Ïóê ÎåÄÌï¥ÏÑú ÏõêÏ≤úÏÜåÏä§Î•º Í∏∞ÏñµÌï†ÎßåÌÅº Ï†ïÎ≥¥ÎüâÏùÑ ÏßÄÎãàÍ≥† ÏûàÎã§Î©¥ GPT Î™®Îç∏Ïù¥ ÏõêÏ≤úÏÜåÏä§Ïóê ÎåÄÌï¥ÏÑú ÏõêÏ≤úÏùÑ ÎßµÌïëÌï†ÎßåÌÅº Ïú†ÏùòÎØ∏Ìïú ÌëúÌòÑ Í≥µÍ∞ÑÏùÑ ÏßÄÎãàÎäîÍ∞Ä?</p>

<ol>
  <li>ÏõêÏ≤úÏùÑ ÎÇòÎà†ÏÑú Í∏∞ÏñµÌï†ÎßåÌÅº ÏÑúÎ°ú Îã§Î•∏ ÌëúÌòÑ Í≥µÍ∞ÑÏù¥Îã§.</li>
  <li>ÏõêÏ≤úÏùÑ Í∏∞ÏñµÌï†ÎßåÌÅº Ïú†ÏùòÎØ∏Ìïú ÌëúÌòÑÏù¥Îã§.</li>
</ol>

<hr>

<h3 id="20231013-ÎÖºÎ¨∏-ÌïôÏäµ-Íµ¨Ï°∞-ÌôïÏ†ï">2023.10.13 ÎÖºÎ¨∏ ÌïôÏäµ Íµ¨Ï°∞ ÌôïÏ†ï</h3>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>1. ÌïôÏäµ ÏÑ∏ÌåÖÏóê Îî∞Î•∏ ÏÑ±Îä• ÎπÑÍµê 

1.1 Îã®Ïàú ÌïôÏäµ

BCE / CEÏóê ÎåÄÌï¥ÏÑú ÎÖºÏùòÌïòÎ©¥ÏÑú Îëê ÌïôÏäµ Í≤∞Í≥ºÎ•º ÎπÑÍµêÌïúÎã§. 

Increasing Number of Labels 
Increasing Number of Words 
Increasing Model Size 

1.2 DeepXML ÏÑ∏ÌåÖ 

BCEÏóê ÎåÄÌï¥ÏÑú Ï∂îÍ∞ÄÏ†ÅÏù∏ Ï∂îÍ∞ÄÏ†ÅÏù∏ ÌïôÏäµÏùÑ ÏßÑÌñâÌï† Í≤ΩÏö∞ ÏÑ±Îä•ÏùÑ Ï≤¥ÌÅ¨ÌïúÎã§. 

Ï∂îÍ∞ÄÏ†ÅÏù∏ ÌïôÏäµÏùÄ 
1. ÌÅ¥Îü¨Ïä§ÌÑ∞ÎßÅÏùÑ ÌÜµÌïú Í∞ÄÏßú Î†àÏù¥Î∏î ÌïôÏäµ 
2. Hard Negative Sampling 
3. Training (DeepXML Framework)

Increasing Number of Labels 
Increasing Number of Words 
Increasing Model Size 

* Note 1 : Î™®Îì† ÏõêÏ≤ú Îç∞Ïù¥ÌÑ∞Ïóê ÎåÄÌï¥ÏÑú 1Ï∞®Ï†ÅÏù∏ ÌïôÏäµÏùÑ ÏßÑÌñâÌï† Ïàò ÏûàÎã§. 
* Note 2 : Ïù¥ÌõÑ, Ï∂îÍ∞ÄÏ†ÅÏù∏ ÏõêÏ≤ú Îç∞Ïù¥ÌÑ∞Ïóê ÎåÄÌï¥ÏÑú Finetuning Ìï† Ïàò ÏûàÎã§. 

2. Encoder Ablation

* Simplest 
* Deep MLP 
* RNN
* Transformer Encoder 
</code></pre></div></div>

<hr>

<h3 id="20231013--xml-literature-review">2023.10.13 : XML Literature Review</h3>

<p><strong>ÏµúÍ∑º Ïó∞Íµ¨Îêú XMLÏóê ÎåÄÌï¥ÏÑú ÎÇ¥Ïö©ÏùÑ Ï†ïÎ¶¨ÌïòÍ≥†, SIP ÎÖºÎ¨∏Ïóê Ï†ÅÏùÑ ÎÇ¥Ïö©ÏùÑ ÏÜåÍ∞úÌïúÎã§.</strong></p>

<p>As the training of source identification is not scalable with CE loss, the alternative loss is binary cross entropy loss (BCE) which is widely used in one-versus-all training framework [1,2,3] and in the field of extreme classification (EC) or  extreme multi-label classification (XML). Deep learning-based XML methods have three properties in common : feature learning, negative label shortlisting, and classifier training [3]. We follow the XML training framework to train the source identification problem with integer labels. We do not utilize the label features, for example, the book name is not used. <br>
Although recent works utilizes the label features to handle the lack of training samples [5,6,7], source identification could not directly use label features. As the input embedding GPT hidden representations are not sentence embeddings and the similarity between the labels and inputs are not guaranteed.  Therefore, SIP problem is EC problem without label features.</p>

<p>AttentionXML forms a tree for label features [6]. SiameseXML enhanced zero-shot Siamese architecture to few-shot classification problem [7].
Renee propose optimization loss to stablize the training and increase the size of training to the fart more extreme number of labels (1B) [2]. 
DEXA [4] use additional parameters to complement the gap of pure label feature embeddings.</p>

<p>[1] Rifkin, Ryan, and Aldebaro Klautau. ‚ÄúIn defense of one-vs-all classification.‚Äù The Journal of Machine Learning Research 5 (2004): 101-141.</p>

<p>[2] Jain, Vidit, et al. ‚ÄúRenee: END-TO-END TRAINING OF EXTREME CLASSIFICATION MODELS.‚Äù Proceedings of Machine Learning and Systems 5 (2023). (Renee)</p>

<p>[3] Dahiya, Kunal, et al. ‚ÄúDeepxml: A deep extreme multi-label learning framework applied to short text documents.‚Äù Proceedings of the 14th ACM International Conference on Web Search and Data Mining. 2021. (DeepXML)</p>

<p>[4] Dahiya, Kunal, et al. ‚ÄúDeep Encoders with Auxiliary Parameters for Extreme Classification.‚Äù Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. 2023. DEXA</p>

<p>[5] Jiang, Ting, et al. ‚ÄúLightxml: Transformer with dynamic negative sampling for high-performance extreme multi-label text classification.‚Äù Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 35. No. 9. 2021. (LightXML)</p>

<p>[6] You, Ronghui, et al. ‚ÄúAttentionxml: Label tree-based attention-aware deep model for high-performance extreme multi-label text classification.‚Äù Advances in Neural Information Processing Systems 32 (2019). (AttentionXML)</p>

<p>[7] Dahiya, Kunal, et al. ‚ÄúSiamesexml: Siamese networks meet extreme classifiers with 100m labels.‚Äù International Conference on Machine Learning. PMLR, 2021. (SiameseXML)</p>

<p>[8] @Misc{Bhatia16,
          author    = {Bhatia, K. and Dahiya, K. and Jain, H. and Kar, P. and Mittal, A. and Prabhu, Y. and Varma, M.},
          title     = {The extreme classification repository: Multi-label datasets and code},
          url       = {http://manikvarma.org/downloads/XC/XMLRepository.html},
          year      = {2016}
        }</p>

<hr>

<h3 id="20231016--gathering-hiddens">2023.10.16 : Gathering Hiddens</h3>

<p>GPT HiddenÏùò Î∂ÑÌè¨Ïóê ÎåÄÌïú Î∂ÑÏÑù/ÏõêÏ≤úÏÜåÏä§ ÎßµÌïëÏùÑ ÌïôÏäµÌïòÎØÄÎ°ú, <strong>ÏùºÎã® GPT Hiddens (Vocab Ï†Ñ ÎßàÏßÄÎßâ ÌëúÌòÑ)Î•º Î™®ÏïÑÎëêÍ≥†</strong>, prediction task Îßå Îî∞Î°ú ÌïôÏäµÌïúÎã§. 
XMlÍ≥º GPT Hiddens Î•º Í≤∞Ìï©ÌïòÎäî Í≤ΩÏö∞ Î©îÎ™®Î¶¨Í∞Ä ÎÑàÎ¨¥ ÎßéÏù¥ ÏÇ¨Ïö©ÎêúÎã§. ÏòàÎ°ú PG19 Îç∞Ïù¥ÌÑ∞Ïùò Í≤ΩÏö∞, 20000Í∞úÍ∞Ä ÎÑòÎäî Î†àÏù¥Î∏îÏù¥ ÏûàÎäîÎç∞, Î†àÏù¥Î∏î Îãπ 100Í∞úÏî© Îç∞Ïù¥ÌÑ∞Î•º ÏñªÎäî Í≤ΩÏö∞,  <code class="language-plaintext highlighter-rouge">(2,000,000, Tokens, Dims)</code> ÏÇ¨Ïù¥Ï¶àÏùò Î©îÎ™®Î¶¨Î•º Ï†ÄÏû•Ìï¥Ïïº ÌïúÎã§. Îî∞ÎùºÏÑú Î†àÏù¥Î∏î 1000Í∞úÏóê ÎåÄÌï¥ÏÑú 100Í∞úÏùò ÏÉòÌîåÏùÑ Î™®ÏúºÍ≥†, CE LossÎ•º Binary CE Î°ú Î≥ÄÍ≤ΩÌïú Í≤ÉÍ≥º, Renee ÏóêÏÑú Ï†úÏïàÌïú LossÍπåÏßÄ Ïç®Î≥∏Îã§.</p>

<table>
  <thead>
    <tr>
      <th>Models</th>
      <th>Amazon131K</th>
      <th>PG19</th>
      <th>Wikipedia500</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">Pythia-70m</code></td>
      <td>‚úÖ</td>
      <td>‚úÖ</td>
      <td>¬†</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">Pythia-160m</code></td>
      <td>‚úÖ</td>
      <td>-</td>
      <td>¬†</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">Pythia-410m</code></td>
      <td>‚úÖ</td>
      <td>-</td>
      <td>¬†</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">Pythia-1b</code></td>
      <td>‚úÖ</td>
      <td>¬†</td>
      <td>¬†</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">Pythia-1.4b</code></td>
      <td>‚úÖ</td>
      <td>¬†</td>
      <td>¬†</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">Pythia-2.8b</code></td>
      <td>‚úÖ</td>
      <td>¬†</td>
      <td>¬†</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">Pythia-6.9b</code></td>
      <td>‚úÖ</td>
      <td>¬†</td>
      <td>¬†</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">Pythia-12b</code></td>
      <td>‚úÖ</td>
      <td>¬†</td>
      <td>¬†</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">LLama-chat-7b</code></td>
      <td>-</td>
      <td>¬†</td>
      <td>¬†</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">LLama-7b</code></td>
      <td>-</td>
      <td>¬†</td>
      <td>¬†</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">LLama-chat-13b</code></td>
      <td>-</td>
      <td>¬†</td>
      <td>¬†</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">LLama-13b</code></td>
      <td>-</td>
      <td>¬†</td>
      <td>¬†</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">GPT2-XL</code></td>
      <td>-</td>
      <td>¬†</td>
      <td>¬†</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">GPT2-XL</code></td>
      <td>-</td>
      <td>¬†</td>
      <td>¬†</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">OPT-350m</code></td>
      <td>-</td>
      <td>¬†</td>
      <td>¬†</td>
    </tr>
  </tbody>
</table>

<h3 id="data-stats">Data stats</h3>

<ul>
  <li>
<strong>PG19</strong> (pythia pretraining data)
    <ul>
      <li>labels: 28,602 (raw data)</li>
      <li>inputs : each book</li>
    </ul>
  </li>
  <li>
<strong>Amazon131K</strong> (classification dataset)
    <ul>
      <li>labels: 131,073</li>
      <li>inputs:  294,805</li>
    </ul>
  </li>
  <li>
<strong>Wikipedia500</strong> (classification dataset)
    <ul>
      <li>labels: 501,070</li>
      <li>inputs: 1,813,391</li>
    </ul>
  </li>
</ul>

<hr>

      </d-article>

      <d-appendix>
        <d-footnote-list></d-footnote-list>
        <d-citation-list></d-citation-list>
      </d-appendix>

<!--<div id="disqus_thread" style="max-width: 1000px; margin: 0 auto;">
    <script type="text/javascript">
        var disqus_shortname  = 'al-folio';
        var disqus_identifier = '/Rsip/source_identification';
        var disqus_title      = "Source Identification for Generative Models";
        (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        })();
    </script>
    <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</div>
    </div>
 -->
      <hr>
<div id="giscus_thread" style="max-width: 1000px; margin: 0 auto;">
    <script>
      let giscusTheme = localStorage.getItem("theme");
      let giscusAttributes = {
          "src": "https://giscus.app/client.js",
          "data-repo": "fxnnxc/fxnnxc",
          "data-repo-id": "MDEwOlJlcG9zaXRvcnkzMjQ2NjUxMTU=",
          "data-category": "Q&A",
          "data-category-id": "DIC_kwDOE1n_G84CYOk_",
          "data-mapping": "title",
          "data-strict": "0",
          "data-reactions-enabled": "1",
          "data-emit-metadata": "0",
          "data-input-position": "top",
          "data-theme": giscusTheme,
          "data-lang": "en",
          "crossorigin": "anonymous",
          "async": "",
      };
  
  
      let giscusScript = document.createElement("script");
      Object.entries(giscusAttributes).forEach(([key, value]) => giscusScript.setAttribute(key, value));
      document.getElementById("giscus_thread").appendChild(giscusScript);
    </script>
    <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" target="_blank" rel="noopener noreferrer">comments powered by giscus.</a>
</noscript>
  </div>
<!-- Footer -->    <footer class="sticky-bottom mt-5" style="border: none;border-top:0px">
      <div class="container" style="text-align: center; ">
        ¬© Copyright 2024 Bumjini  . Powered by Jekyll with al-folio theme. Hosted by GitHub Pages.

      </div>
    </footer>
    
    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.2/dist/umd/popper.min.js" integrity="sha256-l/1pMF/+J4TThfgARS6KwWrk/egwuVvhRzfLAMQ6Ds4=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.min.js" integrity="sha256-SyTu6CwrfOhaznYZPoolVw2rxoY7lKYKQvqbtqN93HI=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    
    

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
  /*
   * This JavaScript code has been adapted from the article 
   * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar, 
   * published on the website https://css-tricks.com on the 7th of May, 2014.
   * Couple of changes were made to the original code to make it compatible 
   * with the `al-foio` theme.
   */
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    let navbarHeight = $("#navbar").outerHeight(true);
    $("body").css({ "padding-top": 0 });
    $("progress-container").css({ "padding-top": 0 });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  /*
   * The offset between the html document height and the browser viewport
   * height will be greater than zero if vertical scroll is possible.
   * This is the distance the user can scroll
   */
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>
  </div></body>
  
  <d-bibliography src="/assets/bibliography/all.bib">
  </d-bibliography>
  <script src="/assets/js/distillpub/overrides.js"></script>

  <center>
    <a  href="">
    <img src="/assets/common/KAIST-hi.gif" width="40px" style="margin-right:0px; padding-bottom: 3px;">
    </a>
  </center>
  <hr> 

  </html>
